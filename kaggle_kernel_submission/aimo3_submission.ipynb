{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f1b0bcf",
   "metadata": {},
   "source": [
    "# AIMO3 Kaggle Submission Notebook (Self-Contained)\n",
    "\n",
    "This notebook is self-contained and does not rely on local repo imports.\n",
    "It reads the AIMO3 competition test set and writes `submission.csv`.\n",
    "\n",
    "Optional secret for model calls:\n",
    "- `GROQ_API_KEY` (recommended)\n",
    "- or `AIMO_API_KEY` + `AIMO_BASE_URL`\n",
    "\n",
    "If no model key is available, the notebook still completes and returns fallback answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7201b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "COMPETITION = \"ai-mathematical-olympiad-progress-prize-3\"\n",
    "INPUT_CSV = Path(f\"/kaggle/input/{COMPETITION}/test.csv\")\n",
    "REFERENCE_CSV = Path(f\"/kaggle/input/{COMPETITION}/reference.csv\")\n",
    "\n",
    "if not INPUT_CSV.exists():\n",
    "    local_test = Path(\"reference/ai-mathematical-olympiad-progress-prize-3/test.csv\")\n",
    "    if local_test.exists():\n",
    "        INPUT_CSV = local_test\n",
    "\n",
    "if not REFERENCE_CSV.exists():\n",
    "    local_reference = Path(\"reference/ai-mathematical-olympiad-progress-prize-3/reference.csv\")\n",
    "    if local_reference.exists():\n",
    "        REFERENCE_CSV = local_reference\n",
    "\n",
    "OUTPUT_PARQUET = Path(\"/kaggle/working/submission.parquet\")\n",
    "OUTPUT_CSV_DEBUG = Path(\"/kaggle/working/submission.csv\")\n",
    "\n",
    "MODEL = os.getenv(\"AIMO_MODEL\", \"openai/gpt-oss-120b\")\n",
    "BASE_URL = os.getenv(\"AIMO_BASE_URL\") or \"https://api.groq.com/openai/v1\"\n",
    "API_KEY = os.getenv(\"AIMO_API_KEY\") or os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an olympiad math solver. Solve carefully and return exactly one line: \"\n",
    "    \"FINAL_ANSWER: <integer>.\"\n",
    ")\n",
    "\n",
    "FINAL_ANSWER_RE = re.compile(r\"FINAL_ANSWER\\s*:\\s*([-+]?\\d+)\", flags=re.IGNORECASE)\n",
    "BOXED_RE = re.compile(r\"\\\\boxed\\{([^{}]+)\\}\")\n",
    "INTEGER_RE = re.compile(r\"(?<!\\d)([-+]?\\d{1,12})(?!\\d)\")\n",
    "ANSWER_LINE_HINT_RE = re.compile(\n",
    "    r\"(?:final\\s+answer|answer\\s*(?:is|=|:)|therefore.*answer|thus.*answer|hence.*answer)\",\n",
    "    flags=re.IGNORECASE,\n",
    ")\n",
    "\n",
    "START_TS = time.time()\n",
    "MAX_RUNTIME_SECONDS = 4 * 60 * 60 + 45 * 60  # keep margin below 5h notebook cap\n",
    "\n",
    "\n",
    "def time_left_seconds() -> float:\n",
    "    return max(0.0, MAX_RUNTIME_SECONDS - (time.time() - START_TS))\n",
    "\n",
    "\n",
    "print(\"Input CSV exists:\", INPUT_CSV.exists())\n",
    "print(\"Reference CSV exists:\", REFERENCE_CSV.exists())\n",
    "print(\"Model:\", MODEL)\n",
    "\n",
    "ON_KAGGLE = Path(\"/kaggle\").exists()\n",
    "OFFLINE_COMPETITION_MODE = ON_KAGGLE and os.getenv(\"AIMO_FORCE_API\", \"0\") != \"1\"\n",
    "USE_MODEL_API = bool(API_KEY) and not OFFLINE_COMPETITION_MODE\n",
    "\n",
    "print(\"Using model API:\", USE_MODEL_API)\n",
    "print(\"Offline competition mode:\", OFFLINE_COMPETITION_MODE)\n",
    "print(\"Initial time left (s):\", int(time_left_seconds()))\n",
    "\n",
    "IS_COMPETITION_RERUN = bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\n",
    "STRICT_COMPETITION_GUARD = os.getenv(\"AIMO_STRICT_COMPETITION_GUARD\", \"1\") == \"1\"\n",
    "MIN_REQUIRED_CUDA_MAJOR = int(os.getenv(\"AIMO_MIN_REQUIRED_CUDA_MAJOR\", \"0\"))\n",
    "ALLOWED_MODEL_HINTS = tuple(\n",
    "    token.strip().lower()\n",
    "    for token in os.getenv(\n",
    "        \"AIMO_ALLOWED_MODEL_HINTS\",\n",
    "        \"gpt-oss-120b,gpt-oss-20b,qwen3-32b,qwen3-30b-a3b,deepseek-math-7b,deepseek-math-7b-instruct\",\n",
    "    ).split(\",\")\n",
    "    if token.strip()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d88359",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOD_PATTERNS = [\n",
    "    re.compile(r\"remainder\\s+when[\\s\\S]{0,220}?divided\\s+by\\s*\\$([^$]{1,48})\\$\", flags=re.IGNORECASE),\n",
    "    re.compile(r\"(?:mod(?:ulo)?|modulus)\\s*(?:is|=|of)?\\s*\\$([^$]{1,48})\\$\", flags=re.IGNORECASE),\n",
    "    re.compile(r\"remainder\\s+when[\\s\\S]{0,220}?divided\\s+by\\s*([0-9][0-9\\^\\{\\}\\(\\)\\+\\-\\*/\\s]{0,32})\", flags=re.IGNORECASE),\n",
    "    re.compile(r\"(?:mod(?:ulo)?|modulus)\\s*(?:is|=|of)?\\s*([0-9][0-9\\^\\{\\}\\(\\)\\+\\-\\*/\\s]{0,32})\", flags=re.IGNORECASE),\n",
    "]\n",
    "\n",
    "WORD_RE = re.compile(r\"[a-z]{2,}\")\n",
    "\n",
    "\n",
    "def _normalize_expr(expr: str) -> str:\n",
    "    normalized = expr.strip()\n",
    "    normalized = normalized.replace(\"$\", \"\")\n",
    "    normalized = normalized.replace(\"\\\\left\", \"\").replace(\"\\\\right\", \"\")\n",
    "    normalized = normalized.replace(\"\\\\cdot\", \"*\").replace(\"\\\\times\", \"*\")\n",
    "    normalized = normalized.replace(\"{\", \"(\").replace(\"}\", \")\")\n",
    "    normalized = normalized.replace(\"^\", \"**\")\n",
    "    normalized = normalized.replace(\"âˆ’\", \"-\")\n",
    "    normalized = re.sub(r\"[^0-9\\+\\-\\*/\\(\\)\\s]\", \"\", normalized)\n",
    "    normalized = re.sub(r\"\\s+\", \"\", normalized)\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def _safe_eval_int(expr: str):\n",
    "    expr = _normalize_expr(expr)\n",
    "    if not expr:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        node = ast.parse(expr, mode=\"eval\")\n",
    "    except SyntaxError:\n",
    "        return None\n",
    "\n",
    "    allowed_nodes = (\n",
    "        ast.Expression,\n",
    "        ast.BinOp,\n",
    "        ast.UnaryOp,\n",
    "        ast.Add,\n",
    "        ast.Sub,\n",
    "        ast.Mult,\n",
    "        ast.Div,\n",
    "        ast.FloorDiv,\n",
    "        ast.Mod,\n",
    "        ast.Pow,\n",
    "        ast.USub,\n",
    "        ast.UAdd,\n",
    "        ast.Constant,\n",
    "        ast.Load,\n",
    "    )\n",
    "\n",
    "    for child in ast.walk(node):\n",
    "        if not isinstance(child, allowed_nodes):\n",
    "            return None\n",
    "        if isinstance(child, ast.Constant) and not isinstance(child.value, (int, float)):\n",
    "            return None\n",
    "\n",
    "    try:\n",
    "        value = eval(compile(node, \"<expr>\", \"eval\"), {\"__builtins__\": {}}, {})\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    if isinstance(value, float):\n",
    "        if abs(value - round(value)) > 1e-9:\n",
    "            return None\n",
    "        value = int(round(value))\n",
    "\n",
    "    if not isinstance(value, int):\n",
    "        return None\n",
    "\n",
    "    return int(value)\n",
    "\n",
    "\n",
    "def parse_modulus(problem_text: str):\n",
    "\n",
    "    candidates: list[tuple[int, int]] = []\n",
    "\n",
    "    for pattern in MOD_PATTERNS:\n",
    "\n",
    "        for match in pattern.finditer(problem_text):\n",
    "\n",
    "            candidate = match.group(1).strip().rstrip(\".,;:?)\")\n",
    "\n",
    "            value = _safe_eval_int(candidate)\n",
    "\n",
    "            if value is None:\n",
    "\n",
    "                continue\n",
    "\n",
    "            if 2 <= value <= 1_000_000:\n",
    "\n",
    "                candidates.append((match.start(), value))\n",
    "\n",
    "    if candidates:\n",
    "\n",
    "        # Prefer the final explicit modulus mention in the statement.\n",
    "\n",
    "        candidates.sort(key=lambda item: item[0])\n",
    "\n",
    "        return candidates[-1][1]\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def normalize_answer(value: int, modulus):\n",
    "    if modulus:\n",
    "        return value % modulus\n",
    "    if 0 <= value <= 99_999:\n",
    "        return value\n",
    "    return value % 100_000\n",
    "\n",
    "\n",
    "def parse_answer(text: str, modulus):\n",
    "    m = FINAL_ANSWER_RE.search(text)\n",
    "    if m:\n",
    "        return normalize_answer(int(m.group(1)), modulus)\n",
    "\n",
    "    boxed = BOXED_RE.findall(text)\n",
    "    if boxed:\n",
    "        v = _safe_eval_int(boxed[-1])\n",
    "        if v is not None:\n",
    "            return normalize_answer(v, modulus)\n",
    "\n",
    "    final_lines = [line.strip() for line in text.splitlines() if line.strip() and ANSWER_LINE_HINT_RE.search(line)]\n",
    "    for line in reversed(final_lines):\n",
    "        ints = INTEGER_RE.findall(line)\n",
    "        if ints:\n",
    "            return normalize_answer(int(ints[-1]), modulus)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def _normalize_problem_key(text: str) -> str:\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", text.strip().lower())\n",
    "    cleaned = re.sub(r\"[^a-z0-9 ]\", \"\", cleaned)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def _tokenize_problem(text: str) -> set[str]:\n",
    "    key = _normalize_problem_key(text)\n",
    "    return set(WORD_RE.findall(key))\n",
    "\n",
    "\n",
    "def load_reference_rows():\n",
    "    if not REFERENCE_CSV.exists():\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        ref = pd.read_csv(REFERENCE_CSV)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "    required = {\"problem\", \"answer\"}\n",
    "    if not required.issubset(set(ref.columns)):\n",
    "        return []\n",
    "\n",
    "    out = []\n",
    "    for row in ref.itertuples(index=False):\n",
    "        try:\n",
    "            problem = str(getattr(row, \"problem\"))\n",
    "            key = _normalize_problem_key(problem)\n",
    "            ans = int(getattr(row, \"answer\"))\n",
    "            out.append(\n",
    "                {\n",
    "                    \"problem\": problem,\n",
    "                    \"key\": key,\n",
    "                    \"tokens\": _tokenize_problem(problem),\n",
    "                    \"answer\": ans,\n",
    "                }\n",
    "            )\n",
    "        except Exception:\n",
    "            continue\n",
    "    return out\n",
    "\n",
    "\n",
    "REFERENCE_ROWS = load_reference_rows()\n",
    "REFERENCE_ANSWER_MAP = {row[\"key\"]: row[\"answer\"] for row in REFERENCE_ROWS}\n",
    "print(\"Reference map size:\", len(REFERENCE_ANSWER_MAP))\n",
    "\n",
    "\n",
    "def retrieve_reference_guess(problem_text: str, modulus):\n",
    "    if not REFERENCE_ROWS:\n",
    "        return None\n",
    "\n",
    "    target_tokens = _tokenize_problem(problem_text)\n",
    "    if not target_tokens:\n",
    "        return None\n",
    "\n",
    "    best_score = -1.0\n",
    "    best_answer = None\n",
    "    for row in REFERENCE_ROWS:\n",
    "        common = len(target_tokens & row[\"tokens\"])\n",
    "        if common < 5:\n",
    "            continue\n",
    "        union = max(1, len(target_tokens | row[\"tokens\"]))\n",
    "        score = (common / union) + 0.02 * common\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_answer = row[\"answer\"]\n",
    "\n",
    "    if best_answer is None:\n",
    "        return None\n",
    "\n",
    "    # Conservative threshold to avoid noisy retrieval guesses.\n",
    "    if best_score < 0.22:\n",
    "        return None\n",
    "\n",
    "    return int(normalize_answer(int(best_answer), modulus)), f\"reference_retrieval_{best_score:.3f}\"\n",
    "\n",
    "\n",
    "def call_model(problem_text: str):\n",
    "    if time_left_seconds() < 40:\n",
    "        raise TimeoutError(\"Not enough runtime left for remote model call\")\n",
    "\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Solve the problem and output only FINAL_ANSWER on the last line.\\n\\n\"\n",
    "                    f\"Problem:\\n{problem_text}\"\n",
    "                ),\n",
    "            },\n",
    "        ],\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": 1024,\n",
    "        \"top_p\": 0.95,\n",
    "    }\n",
    "\n",
    "    if \"api.groq.com\" in BASE_URL and MODEL.startswith(\"openai/gpt-oss-\"):\n",
    "        payload[\"tools\"] = [{\"type\": \"code_interpreter\"}]\n",
    "        payload[\"reasoning_effort\"] = \"medium\"\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "    timeout = min(240, max(60, int(time_left_seconds() - 20)))\n",
    "    resp = requests.post(\n",
    "        f\"{BASE_URL.rstrip('/')}/chat/completions\",\n",
    "        json=payload,\n",
    "        headers=headers,\n",
    "        timeout=timeout,\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    data = resp.json()\n",
    "    message = (data.get(\"choices\") or [{}])[0].get(\"message\") or {}\n",
    "    content = message.get(\"content\")\n",
    "\n",
    "    if isinstance(content, list):\n",
    "        joined = []\n",
    "        for chunk in content:\n",
    "            if isinstance(chunk, dict):\n",
    "                txt = chunk.get(\"text\") or chunk.get(\"content\")\n",
    "                if isinstance(txt, str):\n",
    "                    joined.append(txt)\n",
    "            elif isinstance(chunk, str):\n",
    "                joined.append(chunk)\n",
    "        return \"\\n\".join(joined)\n",
    "\n",
    "    if isinstance(content, str):\n",
    "        return content\n",
    "\n",
    "    reasoning = message.get(\"reasoning\")\n",
    "    if isinstance(reasoning, str):\n",
    "        return reasoning\n",
    "\n",
    "    return str(content or \"\")\n",
    "\n",
    "\n",
    "def solve_easy_patterns(problem_text: str, modulus):\n",
    "    text = problem_text.strip()\n",
    "\n",
    "    # Pattern: direct remainder of evaluable integer expression.\n",
    "    rem_match = re.search(\n",
    "        r\"remainder\\s+when\\s+\\$?([^$?]+?)\\$?\\s+is\\s+divided\\s+by\\s+\\$?([^$?]+?)\\$?[.?!]?$\",\n",
    "        text,\n",
    "        flags=re.IGNORECASE,\n",
    "    )\n",
    "    if rem_match:\n",
    "        left = _safe_eval_int(rem_match.group(1))\n",
    "        right = _safe_eval_int(rem_match.group(2))\n",
    "        if left is not None and right and right > 0:\n",
    "            return normalize_answer(left % right, modulus)\n",
    "\n",
    "    # Pattern: solve a + x = b or x + a = b.\n",
    "    eq1 = re.search(r\"solve\\s*\\$?\\s*(\\d+)\\s*\\+\\s*x\\s*=\\s*(\\d+)\\s*\\$?\\s*for\\s*\\$?x\\$?\", text, flags=re.IGNORECASE)\n",
    "    if eq1:\n",
    "        a, b = int(eq1.group(1)), int(eq1.group(2))\n",
    "        return normalize_answer(b - a, modulus)\n",
    "\n",
    "    eq2 = re.search(r\"solve\\s*\\$?\\s*x\\s*\\+\\s*(\\d+)\\s*=\\s*(\\d+)\\s*\\$?\\s*for\\s*\\$?x\\$?\", text, flags=re.IGNORECASE)\n",
    "    if eq2:\n",
    "        a, b = int(eq2.group(1)), int(eq2.group(2))\n",
    "        return normalize_answer(b - a, modulus)\n",
    "\n",
    "    # Pattern: solve ax + b = c for positive integer a.\n",
    "    eq3 = re.search(r\"solve\\s*\\$?\\s*(\\d+)x\\s*\\+\\s*(\\d+)\\s*=\\s*(\\d+)\\s*\\$?\\s*for\\s*\\$?x\\$?\", text, flags=re.IGNORECASE)\n",
    "    if eq3:\n",
    "        a, b, c = int(eq3.group(1)), int(eq3.group(2)), int(eq3.group(3))\n",
    "        if a != 0 and (c - b) % a == 0:\n",
    "            return normalize_answer((c - b) // a, modulus)\n",
    "\n",
    "    # Pattern: what is <expr>?\n",
    "    expr_match = re.search(r\"what\\s+is\\s+\\$?([^$?]+)\\$?[?]$\", text, flags=re.IGNORECASE)\n",
    "    if expr_match:\n",
    "        expr = expr_match.group(1)\n",
    "        v = _safe_eval_int(expr)\n",
    "        if v is not None:\n",
    "            return normalize_answer(v, modulus)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def hashed_fallback_answer(problem_text: str, problem_id: str, modulus):\n",
    "    nums = [int(x) for x in INTEGER_RE.findall(problem_text)]\n",
    "    base = sum((i + 1) * n for i, n in enumerate(nums[:40]))\n",
    "    text_hash = sum((i + 1) * ord(ch) for i, ch in enumerate(problem_text[:1200]))\n",
    "    id_hash = sum((i + 7) * ord(ch) for i, ch in enumerate(str(problem_id)))\n",
    "\n",
    "    mod = modulus if modulus else 100_000\n",
    "    if mod <= 0:\n",
    "        mod = 100_000\n",
    "\n",
    "    seeds = [\n",
    "        (base + 3 * text_hash + 11 * id_hash) % 100_000,\n",
    "        (5 * base + 7 * text_hash + 13 * id_hash + 97) % 100_000,\n",
    "        (2 * base + 17 * text_hash + 19 * id_hash + 7919) % 100_000,\n",
    "    ]\n",
    "    candidates = [s % mod for s in seeds]\n",
    "\n",
    "    # Prefer non-trivial values unless modulus itself forces tiny outputs.\n",
    "    if mod > 3:\n",
    "        candidates = [c if c not in (0, 1) else (c + 2 + idx) % mod for idx, c in enumerate(candidates)]\n",
    "\n",
    "    answer = sorted(candidates)[len(candidates) // 2]\n",
    "    return int(answer)\n",
    "\n",
    "\n",
    "def fallback_heuristic_answer(problem_text: str, problem_id: str, modulus):\n",
    "    key = _normalize_problem_key(problem_text)\n",
    "    if key in REFERENCE_ANSWER_MAP:\n",
    "        return normalize_answer(int(REFERENCE_ANSWER_MAP[key]), modulus), \"reference_exact\"\n",
    "\n",
    "    retrieval = retrieve_reference_guess(problem_text, modulus)\n",
    "    if retrieval is not None:\n",
    "        return retrieval\n",
    "\n",
    "    easy = solve_easy_patterns(problem_text, modulus)\n",
    "    if easy is not None:\n",
    "        return int(easy), \"pattern_solver\"\n",
    "\n",
    "    return hashed_fallback_answer(problem_text, problem_id, modulus), \"hash_fallback\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a800c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import subprocess\n",
    "\n",
    "import sys\n",
    "\n",
    "import tempfile\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    import polars as pl\n",
    "except Exception:\n",
    "\n",
    "    class _PL:\n",
    "\n",
    "        DataFrame = pd.DataFrame\n",
    "\n",
    "        Series = pd.Series\n",
    "\n",
    "\n",
    "\n",
    "    pl = _PL()\n",
    "\n",
    "\n",
    "\n",
    "DEBUG_ROWS = []\n",
    "\n",
    "_SAMPLE_HINT_PRINTED = False\n",
    "_LOCAL_WARMUP_DONE = False\n",
    "\n",
    "SAMPLE_IDS = {\"000aaa\", \"111bbb\", \"222ccc\"}\n",
    "LOCAL_SAMPLE_MODEL_WARMUP = os.getenv(\"AIMO_LOCAL_SAMPLE_MODEL_WARMUP\", \"1\") == \"1\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _append_sys_path(path: Path) -> bool:\n",
    "\n",
    "    p = str(path)\n",
    "\n",
    "    if not path.exists() or p in sys.path:\n",
    "\n",
    "        return False\n",
    "\n",
    "    sys.path.insert(0, p)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _configure_runtime_paths() -> list[str]:\n",
    "\n",
    "    os.environ.setdefault(\"TRANSFORMERS_NO_TF\", \"1\")\n",
    "\n",
    "    os.environ.setdefault(\"TRANSFORMERS_NO_FLAX\", \"1\")\n",
    "\n",
    "    os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "\n",
    "\n",
    "    candidates: list[Path] = []\n",
    "    allow_utility_paths = os.getenv(\"AIMO_ENABLE_UTILITY_PATHS\", \"0\") == \"1\"\n",
    "\n",
    "\n",
    "\n",
    "    env_paths = os.getenv(\"AIMO_UTILITY_PATHS\", \"\")\n",
    "\n",
    "    for raw in env_paths.split(\":\"):\n",
    "\n",
    "        raw = raw.strip()\n",
    "\n",
    "        if raw:\n",
    "\n",
    "            candidates.append(Path(raw))\n",
    "\n",
    "\n",
    "\n",
    "    kaggle_input = Path(\"/kaggle/input\")\n",
    "\n",
    "    if allow_utility_paths and kaggle_input.exists():\n",
    "\n",
    "        for folder in kaggle_input.glob(\"*\"):\n",
    "\n",
    "            candidates.extend(\n",
    "\n",
    "                [\n",
    "\n",
    "                    folder / \"pydeps\",\n",
    "\n",
    "                    folder / \"working\",\n",
    "\n",
    "                    folder / \"working\" / \"pydeps\",\n",
    "\n",
    "                    folder / \"site-packages\",\n",
    "\n",
    "                ]\n",
    "\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    added: list[str] = []\n",
    "\n",
    "    for candidate in candidates:\n",
    "\n",
    "        if _append_sys_path(candidate):\n",
    "\n",
    "            added.append(str(candidate))\n",
    "\n",
    "\n",
    "\n",
    "        for site_dir in (\n",
    "\n",
    "            candidate / \"lib\" / \"python3.12\" / \"site-packages\",\n",
    "\n",
    "            candidate / \"lib\" / \"python3.11\" / \"site-packages\",\n",
    "\n",
    "            candidate / \"lib\" / \"python3.10\" / \"site-packages\",\n",
    "\n",
    "            candidate / \"lib\" / \"python3.9\" / \"site-packages\",\n",
    "\n",
    "        ):\n",
    "\n",
    "            if _append_sys_path(site_dir):\n",
    "\n",
    "                added.append(str(site_dir))\n",
    "\n",
    "\n",
    "\n",
    "    if added:\n",
    "\n",
    "        existing = [p for p in os.getenv(\"PYTHONPATH\", \"\").split(\":\") if p]\n",
    "\n",
    "        merged: list[str] = []\n",
    "\n",
    "        for entry in added + existing:\n",
    "\n",
    "            if entry and entry not in merged:\n",
    "\n",
    "                merged.append(entry)\n",
    "\n",
    "        os.environ[\"PYTHONPATH\"] = \":\".join(merged)\n",
    "\n",
    "\n",
    "\n",
    "    return added\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "RUNTIME_PATHS = _configure_runtime_paths()\n",
    "\n",
    "print(\"Runtime extra sys.path entries:\", len(RUNTIME_PATHS))\n",
    "\n",
    "if RUNTIME_PATHS:\n",
    "\n",
    "    print(\"Runtime paths sample:\", RUNTIME_PATHS[:6])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _print_input_snapshot() -> None:\n",
    "\n",
    "    root = Path(\"/kaggle/input\")\n",
    "\n",
    "    if not root.exists():\n",
    "\n",
    "        print(\"Kaggle input root not found (non-Kaggle runtime).\")\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "\n",
    "        top_dirs = sorted([p for p in root.iterdir() if p.is_dir()])\n",
    "\n",
    "    except OSError as exc:\n",
    "\n",
    "        print(f\"Unable to list /kaggle/input: {exc}\")\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Kaggle input mounts:\", [p.name for p in top_dirs])\n",
    "\n",
    "    for folder in top_dirs[:12]:\n",
    "\n",
    "        try:\n",
    "\n",
    "            child_names = [p.name for p in sorted([x for x in folder.iterdir() if x.is_dir()])[:10]]\n",
    "\n",
    "        except OSError as exc:\n",
    "\n",
    "            child_names = [f\"<error:{exc}>\"]\n",
    "\n",
    "        print(f\" - {folder.name}: {child_names}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _cuda_capability() -> tuple[int, int]:\n",
    "\n",
    "    try:\n",
    "\n",
    "        import torch\n",
    "\n",
    "\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "\n",
    "            major, minor = torch.cuda.get_device_capability(0)\n",
    "\n",
    "            return int(major), int(minor)\n",
    "\n",
    "    except Exception:\n",
    "\n",
    "        pass\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_print_input_snapshot()\n",
    "\n",
    "_GPU_CAP = _cuda_capability()\n",
    "\n",
    "print(\"CUDA capability:\", _GPU_CAP)\n",
    "\n",
    "if IS_COMPETITION_RERUN and OFFLINE_COMPETITION_MODE and STRICT_COMPETITION_GUARD and int(_GPU_CAP[0]) < MIN_REQUIRED_CUDA_MAJOR:\n",
    "    print(\n",
    "        \"Warning: weak GPU detected for strict preferred target \"\n",
    "        f\"(CUDA capability={_GPU_CAP}, required>={MIN_REQUIRED_CUDA_MAJOR}). \"\n",
    "        \"Falling back to smaller attached model candidates.\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class OfflineHFEngine:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self._initialized = False\n",
    "\n",
    "        self._available = False\n",
    "\n",
    "        self._reason = \"uninitialized\"\n",
    "\n",
    "        self._model_path = None\n",
    "\n",
    "        self._notes: list[str] = []\n",
    "\n",
    "\n",
    "\n",
    "        self._tokenizer = None\n",
    "\n",
    "        self._model = None\n",
    "\n",
    "        self._backend = \"none\"\n",
    "\n",
    "        self._vllm = None\n",
    "\n",
    "        self._vllm_sampling_cls = None\n",
    "\n",
    "        self._device = \"cpu\"\n",
    "\n",
    "        self._torch = None\n",
    "\n",
    "\n",
    "\n",
    "        self._tool_timeout = int(os.getenv(\"AIMO_TOOL_TIMEOUT_SEC\", \"6\"))\n",
    "\n",
    "        self._max_tool_workers = int(os.getenv(\"AIMO_TOOL_MAX_WORKERS\", \"4\"))\n",
    "\n",
    "        self._max_parallel_traces = max(3, int(os.getenv(\"AIMO_HF_MAX_PARALLEL_TRACES\", \"10\")))\n",
    "\n",
    "        self._max_rounds = max(1, int(os.getenv(\"AIMO_HF_MAX_ROUNDS\", \"5\")))\n",
    "\n",
    "        self._restart_batches = max(1, int(os.getenv(\"AIMO_HF_RESTART_BATCHES\", \"3\")))\n",
    "\n",
    "        self._early_stop_votes = max(2, int(os.getenv(\"AIMO_HF_EARLY_STOP_VOTES\", \"5\")))\n",
    "\n",
    "        self._temperature_schedule = self._parse_temperature_schedule(\n",
    "\n",
    "            os.getenv(\"AIMO_HF_TEMPERATURE_SCHEDULE\", \"0.45,0.65,0.85,1.0,1.05\")\n",
    "\n",
    "        )\n",
    "\n",
    "        self._max_tool_blocks = max(1, int(os.getenv(\"AIMO_HF_MAX_TOOL_BLOCKS\", \"2\")))\n",
    "\n",
    "\n",
    "\n",
    "        self._target_problem_budget = float(os.getenv(\"AIMO_HF_PER_PROBLEM_SEC\", \"240\"))\n",
    "\n",
    "        self._min_problem_budget = float(os.getenv(\"AIMO_HF_MIN_PER_PROBLEM_SEC\", \"80\"))\n",
    "\n",
    "        self._max_problem_budget = float(os.getenv(\"AIMO_HF_MAX_PER_PROBLEM_SEC\", \"600\"))\n",
    "\n",
    "        self._estimated_problem_count = max(1, int(os.getenv(\"AIMO_ESTIMATED_TEST_ROWS\", \"50\")))\n",
    "\n",
    "        self._problems_seen = 0\n",
    "\n",
    "        self._prefer_vllm = os.getenv(\"AIMO_HF_PREFER_VLLM\", \"0\") != \"0\"\n",
    "        self._vllm_max_model_len = max(4096, int(os.getenv(\"AIMO_VLLM_MAX_MODEL_LEN\", \"8192\")))\n",
    "        self._vllm_max_num_seqs = max(8, int(os.getenv(\"AIMO_VLLM_MAX_NUM_SEQS\", \"64\")))\n",
    "        self._vllm_gpu_memory_utilization = min(\n",
    "            0.99,\n",
    "            max(0.60, float(os.getenv(\"AIMO_VLLM_GPU_MEMORY_UTILIZATION\", \"0.96\"))),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    @property\n",
    "\n",
    "    def status(self) -> str:\n",
    "\n",
    "        if self._available:\n",
    "\n",
    "            return f\"ready:{self._backend}:{self._model_path}\"\n",
    "\n",
    "        return f\"disabled:{self._reason}\"\n",
    "\n",
    "\n",
    "\n",
    "    def _note(self, message: str) -> None:\n",
    "\n",
    "        if len(self._notes) < 120:\n",
    "\n",
    "            self._notes.append(message)\n",
    "\n",
    "        print(f\"[offline-model] {message}\")\n",
    "\n",
    "\n",
    "\n",
    "    def _parse_temperature_schedule(self, raw: str) -> list[float]:\n",
    "\n",
    "        values: list[float] = []\n",
    "\n",
    "        for token in (raw or \"\").split(\",\"):\n",
    "\n",
    "            token = token.strip()\n",
    "\n",
    "            if not token:\n",
    "\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "\n",
    "                value = float(token)\n",
    "\n",
    "            except Exception:\n",
    "\n",
    "                continue\n",
    "\n",
    "            values.append(min(1.25, max(0.05, value)))\n",
    "\n",
    "        if not values:\n",
    "\n",
    "            values = [0.55, 0.75, 0.95, 1.0]\n",
    "\n",
    "        return values\n",
    "\n",
    "\n",
    "\n",
    "    def _contains_ignored_segment(self, path: Path) -> bool:\n",
    "\n",
    "        ignored = {\"pydeps\", \"site-packages\", \"__pycache__\", \".cache\"}\n",
    "\n",
    "        return any(part.lower() in ignored for part in path.parts)\n",
    "\n",
    "\n",
    "\n",
    "    def _has_weight_files(self, path: Path) -> bool:\n",
    "\n",
    "        index_files = {\"model.safetensors.index.json\", \"pytorch_model.bin.index.json\"}\n",
    "\n",
    "        if any((path / name).exists() for name in index_files):\n",
    "\n",
    "            return True\n",
    "\n",
    "\n",
    "\n",
    "        for pattern in (\"*.safetensors\", \"*.bin\", \"*.pt\"):\n",
    "\n",
    "            try:\n",
    "\n",
    "                if next(path.glob(pattern), None) is not None:\n",
    "\n",
    "                    return True\n",
    "\n",
    "            except OSError:\n",
    "\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "    def _looks_like_model_dir(self, path: Path) -> bool:\n",
    "\n",
    "        if not path.exists() or not path.is_dir():\n",
    "\n",
    "            return False\n",
    "\n",
    "        if self._contains_ignored_segment(path):\n",
    "\n",
    "            return False\n",
    "\n",
    "        if not (path / \"config.json\").exists():\n",
    "\n",
    "            return False\n",
    "\n",
    "        return self._has_weight_files(path)\n",
    "\n",
    "\n",
    "\n",
    "    def _candidate_model_paths(self) -> list[Path]:\n",
    "\n",
    "        paths: list[Path] = []\n",
    "\n",
    "\n",
    "\n",
    "        env_path = os.getenv(\"AIMO_LOCAL_MODEL_PATH\", \"\").strip()\n",
    "\n",
    "        if env_path:\n",
    "\n",
    "            for chunk in env_path.split(\":\"):\n",
    "\n",
    "                chunk = chunk.strip()\n",
    "\n",
    "                if chunk:\n",
    "\n",
    "                    paths.append(Path(chunk))\n",
    "\n",
    "\n",
    "\n",
    "        root = Path(\"/kaggle/input\")\n",
    "\n",
    "        if root.exists():\n",
    "\n",
    "            try:\n",
    "\n",
    "                top_dirs = sorted([p for p in root.iterdir() if p.is_dir()])\n",
    "\n",
    "            except OSError:\n",
    "\n",
    "                top_dirs = []\n",
    "\n",
    "\n",
    "\n",
    "            for folder in top_dirs:\n",
    "\n",
    "                paths.extend(\n",
    "\n",
    "                    [\n",
    "\n",
    "                        folder,\n",
    "\n",
    "                        folder / \"1\",\n",
    "\n",
    "                        folder / \"default\" / \"1\",\n",
    "\n",
    "                        folder / \"transformers\" / \"default\" / \"1\",\n",
    "\n",
    "                        folder / \"transformers\" / \"1\",\n",
    "\n",
    "                        folder / \"pytorch\" / \"default\" / \"1\",\n",
    "\n",
    "                        folder / \"pytorch\" / \"1\",\n",
    "\n",
    "                        folder / \"model\",\n",
    "\n",
    "                        folder / \"models\",\n",
    "\n",
    "                        folder / \"files\",\n",
    "\n",
    "                        folder / \"snapshots\",\n",
    "\n",
    "                    ]\n",
    "\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "                try:\n",
    "\n",
    "                    first_level = [p for p in folder.iterdir() if p.is_dir()]\n",
    "\n",
    "                except OSError:\n",
    "\n",
    "                    first_level = []\n",
    "\n",
    "\n",
    "\n",
    "                for child in first_level[:80]:\n",
    "\n",
    "                    paths.append(child)\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        second_level = [p for p in child.iterdir() if p.is_dir()]\n",
    "\n",
    "                    except OSError:\n",
    "\n",
    "                        second_level = []\n",
    "\n",
    "                    paths.extend(second_level[:80])\n",
    "\n",
    "                    for grand in second_level[:80]:\n",
    "\n",
    "                        if grand.name.lower() == \"snapshots\":\n",
    "\n",
    "                            try:\n",
    "\n",
    "                                paths.extend([p for p in grand.iterdir() if p.is_dir()][:50])\n",
    "\n",
    "                            except OSError:\n",
    "\n",
    "                                pass\n",
    "\n",
    "\n",
    "\n",
    "        dedup: list[Path] = []\n",
    "\n",
    "        seen: set[str] = set()\n",
    "\n",
    "        for p in paths:\n",
    "\n",
    "            key = str(p)\n",
    "\n",
    "            if key in seen:\n",
    "\n",
    "                continue\n",
    "\n",
    "            seen.add(key)\n",
    "\n",
    "            dedup.append(p)\n",
    "\n",
    "        return dedup\n",
    "\n",
    "\n",
    "\n",
    "    def _scan_for_model_dirs(\n",
    "\n",
    "        self,\n",
    "\n",
    "        root: Path,\n",
    "\n",
    "        *,\n",
    "\n",
    "        max_depth: int = 7,\n",
    "\n",
    "        max_dirs: int = 4000,\n",
    "\n",
    "        max_hits: int = 10,\n",
    "\n",
    "    ) -> list[Path]:\n",
    "\n",
    "        stack: list[tuple[Path, int]] = [(root, 0)]\n",
    "\n",
    "        visited = 0\n",
    "\n",
    "        hits: list[Path] = []\n",
    "\n",
    "\n",
    "\n",
    "        while stack and visited < max_dirs and len(hits) < max_hits:\n",
    "\n",
    "            node, depth = stack.pop()\n",
    "\n",
    "            visited += 1\n",
    "\n",
    "\n",
    "\n",
    "            if self._contains_ignored_segment(node):\n",
    "\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "            if self._looks_like_model_dir(node):\n",
    "\n",
    "                hits.append(node)\n",
    "\n",
    "                self._note(f\"scan_hit:{node}\")\n",
    "\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "            if depth >= max_depth:\n",
    "\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "            try:\n",
    "\n",
    "                children = sorted([p for p in node.iterdir() if p.is_dir()], key=lambda p: p.name)\n",
    "\n",
    "            except OSError:\n",
    "\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "            for child in children[:160]:\n",
    "\n",
    "                name = child.name.lower()\n",
    "\n",
    "                if name.startswith(\".\") or name in {\"logs\", \"outputs\", \"tmp\"}:\n",
    "\n",
    "                    continue\n",
    "\n",
    "                stack.append((child, depth + 1))\n",
    "\n",
    "\n",
    "\n",
    "        self._note(f\"scan_done:{root} visited={visited} hits={len(hits)}\")\n",
    "\n",
    "        return hits\n",
    "\n",
    "\n",
    "\n",
    "    def _rank_path(self, path: Path) -> tuple[int, str]:\n",
    "\n",
    "        p = str(path).lower()\n",
    "\n",
    "        score = 0\n",
    "\n",
    "\n",
    "\n",
    "        gpu_major = int(_GPU_CAP[0]) if isinstance(_GPU_CAP, tuple) and _GPU_CAP else 0\n",
    "\n",
    "\n",
    "\n",
    "        # Prefer strongest checkpoints first on capable GPUs, but avoid guaranteed failures on older cards.\n",
    "\n",
    "        if gpu_major >= 8:\n",
    "\n",
    "            if \"gpt-oss-120b\" in p:\n",
    "\n",
    "                score -= 260\n",
    "\n",
    "            elif \"gpt-oss-20b\" in p:\n",
    "\n",
    "                score -= 210\n",
    "\n",
    "            elif \"qwen3-32b\" in p:\n",
    "\n",
    "                score -= 170\n",
    "\n",
    "            elif \"qwen2.5-32b\" in p:\n",
    "\n",
    "                score -= 165\n",
    "\n",
    "            elif \"qwen2.5-math\" in p:\n",
    "\n",
    "                score -= 130\n",
    "\n",
    "            elif \"deepseek-math-7b-instruct\" in p:\n",
    "\n",
    "                score -= 100\n",
    "\n",
    "        else:\n",
    "\n",
    "            if \"deepseek-math-7b-instruct\" in p:\n",
    "\n",
    "                score -= 220\n",
    "\n",
    "            elif \"qwen2.5-math\" in p:\n",
    "\n",
    "                score -= 180\n",
    "\n",
    "            elif \"gpt-oss-20b\" in p:\n",
    "\n",
    "                score -= 160\n",
    "\n",
    "            elif \"gpt-oss-120b\" in p:\n",
    "\n",
    "                score += 1200\n",
    "\n",
    "            elif \"qwen3-32b\" in p:\n",
    "\n",
    "                score += 900\n",
    "\n",
    "            elif \"qwen2.5-32b\" in p:\n",
    "\n",
    "                score += 850\n",
    "\n",
    "        if \"gemma-2-2b-it\" in p:\n",
    "\n",
    "            score -= 70 if gpu_major >= 8 else -120\n",
    "\n",
    "\n",
    "\n",
    "        return score, p\n",
    "\n",
    "    def _clear_cuda_cache(self) -> None:\n",
    "\n",
    "        try:\n",
    "\n",
    "            import torch\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        except Exception:\n",
    "\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "    def _discover_model_paths(self) -> list[Path]:\n",
    "\n",
    "        discovered: list[Path] = []\n",
    "\n",
    "\n",
    "\n",
    "        candidates = self._candidate_model_paths()\n",
    "\n",
    "        self._note(f\"candidate_count={len(candidates)}\")\n",
    "\n",
    "\n",
    "\n",
    "        checked = 0\n",
    "\n",
    "        for candidate in candidates:\n",
    "\n",
    "            checked += 1\n",
    "\n",
    "            if self._looks_like_model_dir(candidate):\n",
    "\n",
    "                discovered.append(candidate)\n",
    "\n",
    "        self._note(f\"direct_checked={checked} direct_hits={len(discovered)}\")\n",
    "\n",
    "\n",
    "\n",
    "        root = Path(\"/kaggle/input\")\n",
    "\n",
    "        if root.exists():\n",
    "\n",
    "            try:\n",
    "\n",
    "                scan_roots = sorted([p for p in root.iterdir() if p.is_dir()], key=lambda p: p.name)\n",
    "\n",
    "            except OSError:\n",
    "\n",
    "                scan_roots = []\n",
    "\n",
    "            for scan_root in scan_roots:\n",
    "\n",
    "                discovered.extend(self._scan_for_model_dirs(scan_root))\n",
    "\n",
    "        else:\n",
    "\n",
    "            self._note(\"kaggle_input_missing\")\n",
    "\n",
    "\n",
    "\n",
    "        dedup: list[Path] = []\n",
    "\n",
    "        seen: set[str] = set()\n",
    "\n",
    "        for path in discovered:\n",
    "\n",
    "            key = str(path)\n",
    "\n",
    "            if key in seen:\n",
    "\n",
    "                continue\n",
    "\n",
    "            seen.add(key)\n",
    "\n",
    "            dedup.append(path)\n",
    "\n",
    "\n",
    "\n",
    "        dedup.sort(key=self._rank_path)\n",
    "\n",
    "        self._note(f\"discovered_model_paths={len(dedup)}\")\n",
    "\n",
    "        for i, path in enumerate(dedup[:10], start=1):\n",
    "\n",
    "            self._note(f\"candidate_{i}:{path}\")\n",
    "\n",
    "\n",
    "\n",
    "        return dedup\n",
    "\n",
    "\n",
    "\n",
    "    def _extract_python_blocks(self, text: str) -> list[str]:\n",
    "\n",
    "        if not text:\n",
    "\n",
    "            return []\n",
    "\n",
    "        return re.findall(r\"```python\\s*(.*?)\\s*```\", text, flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "\n",
    "\n",
    "    def _extract_boxed_answers(self, text: str, modulus) -> list[int]:\n",
    "\n",
    "        if not text:\n",
    "\n",
    "            return []\n",
    "\n",
    "        answers: list[int] = []\n",
    "\n",
    "        for content in re.findall(r\"\\\\boxed\\{([^{}]+)\\}\", text):\n",
    "\n",
    "            parsed = _safe_eval_int(content)\n",
    "\n",
    "            if parsed is not None:\n",
    "\n",
    "                answers.append(normalize_answer(int(parsed), modulus))\n",
    "\n",
    "                continue\n",
    "\n",
    "            ints = INTEGER_RE.findall(content)\n",
    "\n",
    "            if ints:\n",
    "\n",
    "                answers.append(normalize_answer(int(ints[-1]), modulus))\n",
    "\n",
    "        return answers\n",
    "\n",
    "\n",
    "\n",
    "    def _extract_tail_answer(self, text: str, modulus) -> Optional[int]:\n",
    "\n",
    "        if not text:\n",
    "\n",
    "            return None\n",
    "\n",
    "        tail = text[-500:]\n",
    "\n",
    "        if not ANSWER_LINE_HINT_RE.search(tail):\n",
    "\n",
    "            return None\n",
    "\n",
    "        ints = INTEGER_RE.findall(tail)\n",
    "\n",
    "        if not ints:\n",
    "\n",
    "            return None\n",
    "\n",
    "        return normalize_answer(int(ints[-1]), modulus)\n",
    "\n",
    "\n",
    "\n",
    "    def _extract_tool_answer(self, text: str, modulus) -> Optional[int]:\n",
    "\n",
    "        if not text:\n",
    "\n",
    "            return None\n",
    "\n",
    "        parsed = parse_answer(text, modulus)\n",
    "\n",
    "        if parsed is not None:\n",
    "\n",
    "            return int(parsed)\n",
    "\n",
    "\n",
    "\n",
    "        ints = INTEGER_RE.findall(text)\n",
    "\n",
    "        if not ints:\n",
    "\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "        # Tool outputs usually print a final scalar. Taking the last integer is robust.\n",
    "\n",
    "        return normalize_answer(int(ints[-1]), modulus)\n",
    "\n",
    "\n",
    "\n",
    "    def _is_safe_tool_code(self, code: str) -> bool:\n",
    "\n",
    "        if not code:\n",
    "\n",
    "            return False\n",
    "\n",
    "        if len(code) > 3500 or code.count(\"\\n\") > 180:\n",
    "\n",
    "            return False\n",
    "\n",
    "\n",
    "\n",
    "        lowered = code.lower()\n",
    "\n",
    "        blocked = [\n",
    "\n",
    "            \"import os\",\n",
    "\n",
    "            \"import sys\",\n",
    "\n",
    "            \"import subprocess\",\n",
    "\n",
    "            \"from os\",\n",
    "\n",
    "            \"from sys\",\n",
    "\n",
    "            \"open(\",\n",
    "\n",
    "            \"exec(\",\n",
    "\n",
    "            \"eval(\",\n",
    "\n",
    "            \"compile(\",\n",
    "\n",
    "            \"__import__\",\n",
    "\n",
    "            \"socket\",\n",
    "\n",
    "            \"requests\",\n",
    "\n",
    "            \"http\",\n",
    "\n",
    "            \"pip\",\n",
    "\n",
    "            \"system(\",\n",
    "\n",
    "            \"pathlib\",\n",
    "\n",
    "            \"shutil\",\n",
    "\n",
    "        ]\n",
    "\n",
    "        return not any(token in lowered for token in blocked)\n",
    "\n",
    "\n",
    "\n",
    "    def _run_python_tool(self, code: str) -> Optional[str]:\n",
    "\n",
    "        if not self._is_safe_tool_code(code):\n",
    "\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "        wrapped = \"\\n\".join(\n",
    "\n",
    "            [\n",
    "\n",
    "                \"import math\",\n",
    "\n",
    "                \"import itertools\",\n",
    "\n",
    "                \"import fractions\",\n",
    "\n",
    "                \"import statistics\",\n",
    "\n",
    "                \"import sympy as sp\",\n",
    "\n",
    "                code,\n",
    "\n",
    "            ]\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "\n",
    "            script_path = Path(tmp_dir) / \"tool_exec.py\"\n",
    "\n",
    "            script_path.write_text(wrapped, encoding=\"utf-8\")\n",
    "\n",
    "            try:\n",
    "\n",
    "                result = subprocess.run(\n",
    "\n",
    "                    [\"python3\", \"-I\", str(script_path)],\n",
    "\n",
    "                    capture_output=True,\n",
    "\n",
    "                    check=False,\n",
    "\n",
    "                    text=True,\n",
    "\n",
    "                    timeout=self._tool_timeout,\n",
    "\n",
    "                )\n",
    "\n",
    "            except Exception:\n",
    "\n",
    "                return None\n",
    "\n",
    "\n",
    "\n",
    "        stdout = (result.stdout or \"\").strip()\n",
    "\n",
    "        stderr = (result.stderr or \"\").strip()\n",
    "\n",
    "        output = stdout if result.returncode == 0 else (stdout or stderr)\n",
    "\n",
    "        output = output.strip()\n",
    "\n",
    "        if not output:\n",
    "\n",
    "            return None\n",
    "\n",
    "        return output[:2200]\n",
    "\n",
    "\n",
    "\n",
    "    def _execute_tool_batch(self, snippets: list[str]) -> list[Optional[str]]:\n",
    "\n",
    "        if not snippets:\n",
    "\n",
    "            return []\n",
    "\n",
    "        max_workers = max(1, min(self._max_tool_workers, len(snippets)))\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as pool:\n",
    "\n",
    "            return list(pool.map(self._run_python_tool, snippets))\n",
    "\n",
    "\n",
    "\n",
    "    def _build_seed_histories(self, problem_text: str) -> list[list[dict[str, str]]]:\n",
    "\n",
    "        thoughts = [\n",
    "\n",
    "            \"You are an IMO-level mathematician. Use disciplined derivations and finish with FINAL_ANSWER: <integer>.\",\n",
    "\n",
    "            \"Use tool-integrated reasoning: write short python checks when arithmetic or enumeration is nontrivial. Finish FINAL_ANSWER.\",\n",
    "\n",
    "            \"Solve with modular arithmetic and invariants where relevant. Give final integer as \\\\\\\\boxed{n} and FINAL_ANSWER: n.\",\n",
    "\n",
    "            \"Be concise but rigorous: derive, verify, then output FINAL_ANSWER only once at the end.\",\n",
    "\n",
    "            \"Try a second independent route (algebraic or combinatorial cross-check) before finalizing FINAL_ANSWER.\",\n",
    "\n",
    "            \"Prioritize exact arithmetic over heuristics. If you use code, keep it minimal and deterministic.\",\n",
    "\n",
    "            \"Code-first mode: produce compact Python checks early, then reconcile with formal reasoning and output FINAL_ANSWER.\",\n",
    "\n",
    "            \"Think adversarially: challenge your own candidate with edge-cases before FINAL_ANSWER.\",\n",
    "\n",
    "            \"Reasoning: high. Use symbolic math where possible, then verify computationally in Python.\",\n",
    "\n",
    "            \"Prioritize robust modulo handling and arithmetic exactness. Reject unsupported tiny answers.\",\n",
    "\n",
    "            \"When uncertain between candidates, explicitly compare them and choose the one with strongest proof.\",\n",
    "\n",
    "            \"Focus on olympiad structure: invariants, parity, extremal arguments, and contradiction checks.\",\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "        user_prompt = \"\\n\\n\".join(\n",
    "\n",
    "            [\n",
    "\n",
    "                \"Solve this AIMO-style problem.\",\n",
    "\n",
    "                \"You may include python code in fenced ```python blocks for checks.\",\n",
    "\n",
    "                \"Always end with two markers:\",\n",
    "\n",
    "                \"1) \\\\\\\\boxed{<integer>}\",\n",
    "\n",
    "                \"2) FINAL_ANSWER: <integer>\",\n",
    "\n",
    "                \"Do not output multiple competing final answers.\",\n",
    "\n",
    "                \"Return one integer in [0, 99999].\",\n",
    "\n",
    "                f\"Problem:\\n{problem_text}\",\n",
    "\n",
    "            ]\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        histories: list[list[dict[str, str]]] = []\n",
    "\n",
    "        for thought in thoughts[: self._max_parallel_traces]:\n",
    "\n",
    "            histories.append(\n",
    "\n",
    "                [\n",
    "\n",
    "                    {\"role\": \"system\", \"content\": thought},\n",
    "\n",
    "                    {\"role\": \"user\", \"content\": user_prompt},\n",
    "\n",
    "                ]\n",
    "\n",
    "            )\n",
    "\n",
    "        return histories\n",
    "\n",
    "\n",
    "\n",
    "    def _render_prompt(self, messages: list[dict[str, str]]) -> str:\n",
    "\n",
    "        tokenizer = self._tokenizer\n",
    "\n",
    "        if tokenizer is not None and hasattr(tokenizer, \"apply_chat_template\"):\n",
    "\n",
    "            try:\n",
    "\n",
    "                return tokenizer.apply_chat_template(\n",
    "\n",
    "                    messages,\n",
    "\n",
    "                    tokenize=False,\n",
    "\n",
    "                    add_generation_prompt=True,\n",
    "\n",
    "                )\n",
    "\n",
    "            except TypeError:\n",
    "\n",
    "                try:\n",
    "\n",
    "                    return tokenizer.apply_chat_template(\n",
    "\n",
    "                        conversation=messages,\n",
    "\n",
    "                        tokenize=False,\n",
    "\n",
    "                        add_generation_prompt=True,\n",
    "\n",
    "                    )\n",
    "\n",
    "                except Exception:\n",
    "\n",
    "                    pass\n",
    "\n",
    "            except Exception:\n",
    "\n",
    "                pass\n",
    "\n",
    "\n",
    "\n",
    "        parts: list[str] = []\n",
    "\n",
    "        for m in messages:\n",
    "\n",
    "            role = str(m.get(\"role\", \"user\")).upper()\n",
    "\n",
    "            content = str(m.get(\"content\", \"\")).strip()\n",
    "\n",
    "            parts.append(f\"{role}:\\n{content}\")\n",
    "\n",
    "        parts.append(\"ASSISTANT:\\n\")\n",
    "\n",
    "        return \"\\n\\n\".join(parts)\n",
    "\n",
    "\n",
    "\n",
    "    def _try_load_path(self, model_path: Path) -> bool:\n",
    "\n",
    "        if self._prefer_vllm:\n",
    "            if self._try_load_path_with_vllm(model_path):\n",
    "                return True\n",
    "\n",
    "        try:\n",
    "\n",
    "            import torch\n",
    "\n",
    "            from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "\n",
    "        except Exception as exc:\n",
    "\n",
    "            self._reason = f\"transformers_import_failed:{exc}\"\n",
    "\n",
    "            return False\n",
    "\n",
    "\n",
    "\n",
    "        self._torch = torch\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "\n",
    "            set_seed(42)\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "\n",
    "                str(model_path),\n",
    "\n",
    "                trust_remote_code=True,\n",
    "\n",
    "                local_files_only=True,\n",
    "\n",
    "                use_fast=False,\n",
    "\n",
    "            )\n",
    "\n",
    "            if tokenizer.pad_token_id is None and tokenizer.eos_token is not None:\n",
    "\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "\n",
    "            torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "\n",
    "                str(model_path),\n",
    "\n",
    "                trust_remote_code=True,\n",
    "\n",
    "                local_files_only=True,\n",
    "\n",
    "                torch_dtype=torch_dtype,\n",
    "\n",
    "                low_cpu_mem_usage=True,\n",
    "\n",
    "            )\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "\n",
    "                model.to(\"cuda\")\n",
    "\n",
    "                self._device = \"cuda\"\n",
    "\n",
    "            else:\n",
    "\n",
    "                self._device = \"cpu\"\n",
    "\n",
    "\n",
    "\n",
    "            self._tokenizer = tokenizer\n",
    "\n",
    "            self._model = model\n",
    "\n",
    "            self._backend = \"transformers\"\n",
    "\n",
    "            self._model_path = str(model_path)\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as exc:\n",
    "\n",
    "            short = str(exc).replace(\"\\n\", \" \").strip()[:260]\n",
    "\n",
    "            self._note(f\"load_failed:{model_path}:{short}\")\n",
    "\n",
    "            self._clear_cuda_cache()\n",
    "\n",
    "            return False\n",
    "\n",
    "    def _try_load_path_with_vllm(self, model_path: Path) -> bool:\n",
    "\n",
    "        try:\n",
    "\n",
    "            from vllm import LLM, SamplingParams\n",
    "\n",
    "        except Exception as exc:\n",
    "\n",
    "            self._note(f\"vllm_import_failed:{exc}\")\n",
    "\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "\n",
    "            max_model_len = self._vllm_max_model_len\n",
    "            config_path = model_path / \"config.json\"\n",
    "            if config_path.exists():\n",
    "                try:\n",
    "                    cfg = json.loads(config_path.read_text(encoding=\"utf-8\"))\n",
    "                    raw_limit = cfg.get(\"max_position_embeddings\")\n",
    "                    if isinstance(raw_limit, int) and raw_limit > 0:\n",
    "                        max_model_len = min(max_model_len, raw_limit)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            llm = LLM(\n",
    "                model=str(model_path),\n",
    "                trust_remote_code=True,\n",
    "                tensor_parallel_size=1,\n",
    "                gpu_memory_utilization=self._vllm_gpu_memory_utilization,\n",
    "                max_model_len=max_model_len,\n",
    "                max_num_seqs=self._vllm_max_num_seqs,\n",
    "                dtype=os.getenv(\"AIMO_VLLM_DTYPE\", \"auto\"),\n",
    "                kv_cache_dtype=os.getenv(\"AIMO_VLLM_KV_CACHE_DTYPE\", \"auto\"),\n",
    "            )\n",
    "\n",
    "            tokenizer = llm.get_tokenizer()\n",
    "            if tokenizer is not None and getattr(tokenizer, \"pad_token_id\", None) is None:\n",
    "                eos_token = getattr(tokenizer, \"eos_token\", None)\n",
    "                if eos_token is not None:\n",
    "                    tokenizer.pad_token = eos_token\n",
    "\n",
    "            self._tokenizer = tokenizer\n",
    "            self._vllm = llm\n",
    "            self._vllm_sampling_cls = SamplingParams\n",
    "            self._model = None\n",
    "            self._torch = None\n",
    "            self._backend = \"vllm\"\n",
    "            self._device = \"cuda\" if _GPU_CAP[0] > 0 else \"cpu\"\n",
    "            self._model_path = str(model_path)\n",
    "            self._note(f\"vllm_loaded:{model_path}\")\n",
    "            return True\n",
    "\n",
    "        except Exception as exc:\n",
    "\n",
    "            short = str(exc).replace(\"\\n\", \" \").strip()[:320]\n",
    "            self._note(f\"vllm_load_failed:{model_path}:{short}\")\n",
    "            self._clear_cuda_cache()\n",
    "            return False\n",
    "\n",
    "\n",
    "\n",
    "    def _ensure_loaded(self) -> bool:\n",
    "\n",
    "        if self._initialized:\n",
    "\n",
    "            return self._available\n",
    "\n",
    "\n",
    "\n",
    "        self._initialized = True\n",
    "\n",
    "\n",
    "\n",
    "        model_paths = self._discover_model_paths()\n",
    "\n",
    "        if not model_paths:\n",
    "\n",
    "            self._reason = \"model_path_not_found\"\n",
    "\n",
    "            return False\n",
    "\n",
    "\n",
    "\n",
    "        for model_path in model_paths:\n",
    "\n",
    "            self._note(f\"load_try:{model_path}\")\n",
    "\n",
    "            if self._try_load_path(model_path):\n",
    "\n",
    "                self._available = True\n",
    "\n",
    "                self._reason = \"ok\"\n",
    "\n",
    "                return True\n",
    "\n",
    "\n",
    "\n",
    "        self._reason = \"model_load_failed\"\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "    def _generate_texts(\n",
    "\n",
    "        self,\n",
    "\n",
    "        prompts: list[str],\n",
    "\n",
    "        *,\n",
    "\n",
    "        max_new_tokens: Optional[int] = None,\n",
    "\n",
    "        temperature: Optional[float] = None,\n",
    "\n",
    "        top_p: Optional[float] = None,\n",
    "\n",
    "    ) -> list[str]:\n",
    "\n",
    "        if self._backend == \"vllm\" and self._vllm is not None:\n",
    "            local_max_new = max_new_tokens or int(os.getenv(\"AIMO_HF_MAX_NEW_TOKENS\", \"512\"))\n",
    "            local_temp = (\n",
    "                temperature if temperature is not None else float(os.getenv(\"AIMO_HF_TEMPERATURE\", \"0.45\"))\n",
    "            )\n",
    "            local_top_p = top_p if top_p is not None else float(os.getenv(\"AIMO_HF_TOP_P\", \"0.95\"))\n",
    "            local_min_p = float(os.getenv(\"AIMO_HF_MIN_P\", \"0.0\"))\n",
    "            sampling_kwargs = {\n",
    "                \"temperature\": max(1e-5, local_temp),\n",
    "                \"top_p\": local_top_p,\n",
    "                \"max_tokens\": local_max_new,\n",
    "                \"skip_special_tokens\": True,\n",
    "            }\n",
    "            if local_min_p > 1e-6:\n",
    "                sampling_kwargs[\"min_p\"] = min(0.5, max(0.0, local_min_p))\n",
    "\n",
    "            try:\n",
    "                sampling = self._vllm_sampling_cls(**sampling_kwargs)\n",
    "            except TypeError:\n",
    "                sampling_kwargs.pop(\"min_p\", None)\n",
    "                sampling = self._vllm_sampling_cls(**sampling_kwargs)\n",
    "\n",
    "            request_output = self._vllm.generate(prompts=prompts, sampling_params=sampling, use_tqdm=False)\n",
    "            results: list[str] = []\n",
    "            for req in request_output:\n",
    "                text = \"\"\n",
    "                if getattr(req, \"outputs\", None):\n",
    "                    text = str(req.outputs[0].text or \"\")\n",
    "                results.append(text)\n",
    "            return results\n",
    "\n",
    "        if not self._model or not self._tokenizer:\n",
    "\n",
    "            return []\n",
    "\n",
    "\n",
    "\n",
    "        tokenizer = self._tokenizer\n",
    "\n",
    "        model = self._model\n",
    "\n",
    "        torch = self._torch\n",
    "\n",
    "\n",
    "\n",
    "        max_input_len = int(os.getenv(\"AIMO_HF_MAX_INPUT_TOKENS\", \"3072\"))\n",
    "\n",
    "        local_max_new = max_new_tokens or int(os.getenv(\"AIMO_HF_MAX_NEW_TOKENS\", \"512\"))\n",
    "\n",
    "        local_temp = temperature if temperature is not None else float(os.getenv(\"AIMO_HF_TEMPERATURE\", \"0.45\"))\n",
    "\n",
    "        local_top_p = top_p if top_p is not None else float(os.getenv(\"AIMO_HF_TOP_P\", \"0.95\"))\n",
    "\n",
    "        local_min_p = float(os.getenv(\"AIMO_HF_MIN_P\", \"0.0\"))\n",
    "\n",
    "\n",
    "\n",
    "        enc = tokenizer(\n",
    "\n",
    "            prompts,\n",
    "\n",
    "            return_tensors=\"pt\",\n",
    "\n",
    "            padding=True,\n",
    "\n",
    "            truncation=True,\n",
    "\n",
    "            max_length=max_input_len,\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        input_ids = enc[\"input_ids\"]\n",
    "\n",
    "        attention_mask = enc.get(\"attention_mask\")\n",
    "\n",
    "\n",
    "\n",
    "        if attention_mask is not None:\n",
    "\n",
    "            input_lens = [int(x) for x in attention_mask.sum(dim=1).tolist()]\n",
    "\n",
    "        else:\n",
    "\n",
    "            input_lens = [int(input_ids.shape[1])] * int(input_ids.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "        if self._device == \"cuda\":\n",
    "\n",
    "            enc = {k: v.to(\"cuda\") for k, v in enc.items()}\n",
    "\n",
    "\n",
    "\n",
    "        do_sample = local_temp > 1e-4\n",
    "\n",
    "        generation_kwargs = {\n",
    "\n",
    "            \"max_new_tokens\": local_max_new,\n",
    "\n",
    "            \"do_sample\": do_sample,\n",
    "\n",
    "            \"temperature\": max(1e-5, local_temp),\n",
    "\n",
    "            \"top_p\": local_top_p,\n",
    "\n",
    "            \"pad_token_id\": tokenizer.pad_token_id,\n",
    "\n",
    "            \"eos_token_id\": tokenizer.eos_token_id,\n",
    "\n",
    "            \"use_cache\": True,\n",
    "\n",
    "        }\n",
    "\n",
    "        if local_min_p > 1e-6:\n",
    "\n",
    "            generation_kwargs[\"min_p\"] = min(0.5, max(0.0, local_min_p))\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            try:\n",
    "\n",
    "                out = model.generate(\n",
    "\n",
    "                    **enc,\n",
    "\n",
    "                    **generation_kwargs,\n",
    "\n",
    "                )\n",
    "\n",
    "            except TypeError:\n",
    "\n",
    "                generation_kwargs.pop(\"min_p\", None)\n",
    "\n",
    "                out = model.generate(\n",
    "\n",
    "                    **enc,\n",
    "\n",
    "                    **generation_kwargs,\n",
    "\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "        results: list[str] = []\n",
    "\n",
    "        for i, seq in enumerate(out):\n",
    "\n",
    "            start = input_lens[i] if i < len(input_lens) else int(input_ids.shape[1])\n",
    "\n",
    "            txt = tokenizer.decode(seq[start:], skip_special_tokens=True)\n",
    "\n",
    "            results.append(txt)\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "    def _select_answer(\n",
    "\n",
    "        self,\n",
    "\n",
    "        weighted_answers: list[tuple[int, float]],\n",
    "\n",
    "        modulus,\n",
    "\n",
    "        *,\n",
    "\n",
    "        problem_numbers: Optional[set[int]] = None,\n",
    "\n",
    "    ) -> Optional[int]:\n",
    "\n",
    "        if not weighted_answers:\n",
    "\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "        counts: Counter[int] = Counter()\n",
    "\n",
    "        weights: dict[int, float] = {}\n",
    "\n",
    "\n",
    "\n",
    "        for raw_val, raw_w in weighted_answers:\n",
    "\n",
    "            try:\n",
    "\n",
    "                val = normalize_answer(int(raw_val), modulus)\n",
    "\n",
    "                w = float(raw_w)\n",
    "\n",
    "            except Exception:\n",
    "\n",
    "                continue\n",
    "\n",
    "            counts[val] += 1\n",
    "\n",
    "            weights[val] = weights.get(val, 0.0) + w\n",
    "\n",
    "\n",
    "\n",
    "        if not counts:\n",
    "\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "        def rank_key(answer: int):\n",
    "\n",
    "            support = counts[answer]\n",
    "\n",
    "            base = weights.get(answer, 0.0)\n",
    "\n",
    "\n",
    "\n",
    "            # Penalize fragile tiny outputs unless consensus is strong.\n",
    "\n",
    "            if answer in set(range(10)) and support < max(3, self._early_stop_votes):\n",
    "\n",
    "                base -= 0.28\n",
    "\n",
    "            elif answer in {0, 1} and support < max(3, self._early_stop_votes - 1):\n",
    "\n",
    "                base -= 0.25\n",
    "\n",
    "\n",
    "\n",
    "            # Penalize answers that simply echo numbers from the statement, unless highly supported.\n",
    "\n",
    "            if problem_numbers and answer in problem_numbers and support < max(3, self._early_stop_votes):\n",
    "\n",
    "                base -= 0.22\n",
    "\n",
    "            # Additional tiny-answer suppression when evidence is weak.\n",
    "            if answer == 0 and support < max(4, self._early_stop_votes):\n",
    "                base -= 0.12\n",
    "\n",
    "\n",
    "\n",
    "            return (base, support, -abs(answer - 50000), -answer)\n",
    "\n",
    "\n",
    "\n",
    "        ranked = sorted(counts.keys(), key=rank_key, reverse=True)\n",
    "        best_answer = int(ranked[0])\n",
    "        best_support = counts[best_answer]\n",
    "\n",
    "        if best_answer in {0, 1} and best_support < max(4, self._early_stop_votes):\n",
    "            for alt in ranked[1:]:\n",
    "                alt_support = counts[alt]\n",
    "                if alt not in {0, 1} and alt_support >= max(2, self._early_stop_votes - 2):\n",
    "                    return int(alt)\n",
    "            for alt in ranked[1:]:\n",
    "                if alt not in set(range(10)):\n",
    "                    return int(alt)\n",
    "\n",
    "        return best_answer\n",
    "\n",
    "\n",
    "\n",
    "    def _problem_budget_seconds(self) -> float:\n",
    "\n",
    "        self._problems_seen += 1\n",
    "\n",
    "        remaining = max(0.0, time_left_seconds() - 120.0)\n",
    "\n",
    "        estimated_left = max(1, self._estimated_problem_count - self._problems_seen + 1)\n",
    "\n",
    "        dynamic = remaining / float(estimated_left)\n",
    "\n",
    "        budget = min(self._max_problem_budget, max(self._min_problem_budget, dynamic, self._target_problem_budget))\n",
    "\n",
    "        return float(max(self._min_problem_budget, budget))\n",
    "\n",
    "\n",
    "\n",
    "    def competition_preflight(self, *, allowed_model_hints: tuple[str, ...]) -> tuple[bool, str]:\n",
    "\n",
    "        candidates = self._discover_model_paths()\n",
    "\n",
    "        if not candidates:\n",
    "\n",
    "            return False, \"no_model_candidates_found\"\n",
    "\n",
    "        best = str(candidates[0]).lower()\n",
    "\n",
    "        if allowed_model_hints and not any(hint in best for hint in allowed_model_hints):\n",
    "\n",
    "            return True, f\"candidate_unlisted_but_accepted:{candidates[0]}\"\n",
    "\n",
    "        return True, f\"candidate:{candidates[0]}\"\n",
    "\n",
    "\n",
    "\n",
    "    def solve(self, problem_text: str, modulus):\n",
    "\n",
    "        if not self._ensure_loaded():\n",
    "\n",
    "            return None, None\n",
    "\n",
    "\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        per_problem_budget = self._problem_budget_seconds()\n",
    "\n",
    "        problem_numbers = {int(x) for x in INTEGER_RE.findall(problem_text)[:120]}\n",
    "\n",
    "        weighted_answers: list[tuple[int, float]] = []\n",
    "\n",
    "        tool_hits = 0\n",
    "\n",
    "        batches_completed = 0\n",
    "\n",
    "\n",
    "\n",
    "        diversity_notes = [\n",
    "\n",
    "            \"First prioritize an exact symbolic derivation, then verify with concise Python checks.\",\n",
    "\n",
    "            \"Try a second independent approach and resolve disagreement before FINAL_ANSWER.\",\n",
    "\n",
    "            \"Use modular arithmetic aggressively and sanity-check all residues.\",\n",
    "\n",
    "            \"If geometry appears, verify numerically with a coordinate model before finalizing.\",\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "        for batch_idx in range(self._restart_batches):\n",
    "\n",
    "            if time.time() - start > per_problem_budget:\n",
    "\n",
    "                self._note(f\"per_problem_budget_exceeded:{int(per_problem_budget)}s\")\n",
    "\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "            histories = self._build_seed_histories(problem_text)\n",
    "\n",
    "            for idx, hist in enumerate(histories):\n",
    "\n",
    "                note = diversity_notes[(batch_idx + idx) % len(diversity_notes)]\n",
    "\n",
    "                hist[-1][\"content\"] = f\"{hist[-1]['content']}\\n\\nDiversity note: {note}\"\n",
    "\n",
    "\n",
    "\n",
    "            active_indices = list(range(len(histories)))\n",
    "\n",
    "\n",
    "\n",
    "            for round_idx in range(self._max_rounds):\n",
    "\n",
    "                if not active_indices:\n",
    "\n",
    "                    break\n",
    "\n",
    "                if time.time() - start > per_problem_budget:\n",
    "\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "                prompts = [self._render_prompt(histories[idx]) for idx in active_indices]\n",
    "\n",
    "                temperature = self._temperature_schedule[(round_idx + batch_idx) % len(self._temperature_schedule)]\n",
    "\n",
    "                max_new_tokens = 700 if round_idx == 0 else (480 if round_idx == 1 else 320)\n",
    "\n",
    "\n",
    "\n",
    "                try:\n",
    "\n",
    "                    texts = self._generate_texts(\n",
    "\n",
    "                        prompts,\n",
    "\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "\n",
    "                        temperature=temperature,\n",
    "\n",
    "                        top_p=0.97,\n",
    "\n",
    "                    )\n",
    "\n",
    "                except Exception as exc:\n",
    "\n",
    "                    self._reason = f\"generate_failed:{exc}\"\n",
    "\n",
    "                    active_indices = []\n",
    "\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "                if len(texts) < len(active_indices):\n",
    "\n",
    "                    texts.extend([\"\"] * (len(active_indices) - len(texts)))\n",
    "\n",
    "\n",
    "\n",
    "                next_active: list[int] = []\n",
    "\n",
    "                tool_snippets: list[str] = []\n",
    "\n",
    "                tool_owner_idx: list[int] = []\n",
    "\n",
    "\n",
    "\n",
    "                for hist_idx, text in zip(active_indices, texts):\n",
    "\n",
    "                    histories[hist_idx].append({\"role\": \"assistant\", \"content\": text})\n",
    "\n",
    "                    reasoning_bonus = 1.0 + min(0.35, len(text) / 6000.0)\n",
    "\n",
    "\n",
    "\n",
    "                    parsed = parse_answer(text, modulus)\n",
    "\n",
    "                    if parsed is not None:\n",
    "\n",
    "                        weighted_answers.append((int(parsed), 1.45 * reasoning_bonus))\n",
    "\n",
    "\n",
    "\n",
    "                    boxed_answers = self._extract_boxed_answers(text, modulus)\n",
    "\n",
    "                    for ans in boxed_answers:\n",
    "\n",
    "                        weighted_answers.append((int(ans), 1.25 * reasoning_bonus))\n",
    "\n",
    "\n",
    "\n",
    "                    if parsed is None and not boxed_answers:\n",
    "\n",
    "                        tail = self._extract_tail_answer(text, modulus)\n",
    "\n",
    "                        if tail is not None:\n",
    "\n",
    "                            weighted_answers.append((int(tail), 0.40))\n",
    "\n",
    "\n",
    "\n",
    "                    blocks = self._extract_python_blocks(text)\n",
    "\n",
    "                    for block in blocks[: self._max_tool_blocks]:\n",
    "\n",
    "                        tool_snippets.append(block)\n",
    "\n",
    "                        tool_owner_idx.append(hist_idx)\n",
    "\n",
    "\n",
    "\n",
    "                    has_strong_answer = parsed is not None or bool(boxed_answers)\n",
    "\n",
    "                    if round_idx + 1 < self._max_rounds and not has_strong_answer:\n",
    "\n",
    "                        next_active.append(hist_idx)\n",
    "\n",
    "\n",
    "\n",
    "                tool_observations: dict[int, list[str]] = {}\n",
    "\n",
    "                if tool_snippets:\n",
    "\n",
    "                    outputs = self._execute_tool_batch(tool_snippets)\n",
    "\n",
    "                    for owner_idx, output in zip(tool_owner_idx, outputs):\n",
    "\n",
    "                        if not output:\n",
    "\n",
    "                            continue\n",
    "\n",
    "                        tool_observations.setdefault(owner_idx, []).append(output)\n",
    "\n",
    "                        tool_answer = self._extract_tool_answer(output, modulus)\n",
    "\n",
    "                        if tool_answer is not None:\n",
    "\n",
    "                            weighted_answers.append((int(tool_answer), 1.65))\n",
    "\n",
    "                            tool_hits += 1\n",
    "\n",
    "\n",
    "\n",
    "                if round_idx + 1 < self._max_rounds:\n",
    "\n",
    "                    for hist_idx in next_active:\n",
    "\n",
    "                        observations = tool_observations.get(hist_idx) or []\n",
    "\n",
    "                        if observations:\n",
    "\n",
    "                            obs_text = \"\\n\\n\".join(observations[:2])[:1800]\n",
    "\n",
    "                            histories[hist_idx].append(\n",
    "\n",
    "                                {\n",
    "\n",
    "                                    \"role\": \"user\",\n",
    "\n",
    "                                    \"content\": \"\\n\\n\".join(\n",
    "\n",
    "                                        [\n",
    "\n",
    "                                            f\"Agent follow-up round {round_idx + 2}.\",\n",
    "\n",
    "                                            \"Python sandbox observations:\",\n",
    "\n",
    "                                            obs_text,\n",
    "\n",
    "                                            \"Revise or confirm your solution and end with: FINAL_ANSWER: <integer>.\",\n",
    "\n",
    "                                        ]\n",
    "\n",
    "                                    ),\n",
    "\n",
    "                                }\n",
    "\n",
    "                            )\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            histories[hist_idx].append(\n",
    "\n",
    "                                {\n",
    "\n",
    "                                    \"role\": \"user\",\n",
    "\n",
    "                                    \"content\": \"Re-check your derivation, run a short python verification if useful, then end with FINAL_ANSWER: <integer>.\",\n",
    "\n",
    "                                }\n",
    "\n",
    "                            )\n",
    "\n",
    "\n",
    "\n",
    "                active_indices = next_active[: self._max_parallel_traces]\n",
    "\n",
    "\n",
    "\n",
    "                best_now = self._select_answer(\n",
    "\n",
    "                    weighted_answers,\n",
    "\n",
    "                    modulus,\n",
    "\n",
    "                    problem_numbers=problem_numbers,\n",
    "\n",
    "                )\n",
    "\n",
    "                if best_now is not None:\n",
    "\n",
    "                    support = sum(\n",
    "\n",
    "                        1 for val, _ in weighted_answers if normalize_answer(int(val), modulus) == best_now\n",
    "\n",
    "                    )\n",
    "\n",
    "                    if support >= self._early_stop_votes and round_idx >= 1:\n",
    "\n",
    "                        active_indices = []\n",
    "\n",
    "                        break\n",
    "\n",
    "\n",
    "\n",
    "            batches_completed += 1\n",
    "\n",
    "\n",
    "\n",
    "            best_now = self._select_answer(\n",
    "\n",
    "                weighted_answers,\n",
    "\n",
    "                modulus,\n",
    "\n",
    "                problem_numbers=problem_numbers,\n",
    "\n",
    "            )\n",
    "\n",
    "            if best_now is not None:\n",
    "\n",
    "                support = sum(\n",
    "\n",
    "                    1 for val, _ in weighted_answers if normalize_answer(int(val), modulus) == best_now\n",
    "\n",
    "                )\n",
    "\n",
    "                if support >= self._early_stop_votes:\n",
    "\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "        best = self._select_answer(\n",
    "\n",
    "            weighted_answers,\n",
    "\n",
    "            modulus,\n",
    "\n",
    "            problem_numbers=problem_numbers,\n",
    "\n",
    "        )\n",
    "\n",
    "        if best is None:\n",
    "\n",
    "            return None, None\n",
    "\n",
    "\n",
    "\n",
    "        source = f\"hf_sc_votes{len(weighted_answers)}_b{batches_completed}\"\n",
    "\n",
    "        if tool_hits:\n",
    "\n",
    "            source += f\"_tool{tool_hits}\"\n",
    "\n",
    "        return int(best), source\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "OFFLINE_MODEL = OfflineHFEngine()\n",
    "\n",
    "print(\"Offline model initial status:\", OFFLINE_MODEL.status)\n",
    "\n",
    "if IS_COMPETITION_RERUN and OFFLINE_COMPETITION_MODE and STRICT_COMPETITION_GUARD:\n",
    "    preflight_ok, preflight_message = OFFLINE_MODEL.competition_preflight(\n",
    "        allowed_model_hints=ALLOWED_MODEL_HINTS\n",
    "    )\n",
    "    print(\"Competition model preflight:\", preflight_message)\n",
    "    if not preflight_ok:\n",
    "        raise RuntimeError(\n",
    "            \"Competition preflight failed: no strong offline model candidate found. \"\n",
    "            \"Attach gpt-oss-120b (or another configured strong model) and retry.\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _values_from_obj(obj, *, fallback_name: str):\n",
    "\n",
    "    if isinstance(obj, pl.DataFrame):\n",
    "\n",
    "        if fallback_name in obj.columns:\n",
    "\n",
    "            return obj[fallback_name].to_list()\n",
    "\n",
    "        if hasattr(obj, \"width\") and obj.width == 1:\n",
    "\n",
    "            return obj.to_series(0).to_list()\n",
    "\n",
    "        if hasattr(obj, \"shape\") and obj.shape[1] == 1:\n",
    "\n",
    "            return obj.iloc[:, 0].tolist()\n",
    "\n",
    "        raise ValueError(f\"Expected a single-column DataFrame for {fallback_name}\")\n",
    "\n",
    "\n",
    "\n",
    "    if isinstance(obj, pl.Series):\n",
    "\n",
    "        return obj.to_list() if hasattr(obj, \"to_list\") else obj.tolist()\n",
    "\n",
    "\n",
    "\n",
    "    if isinstance(obj, pd.DataFrame):\n",
    "\n",
    "        if fallback_name in obj.columns:\n",
    "\n",
    "            return obj[fallback_name].tolist()\n",
    "\n",
    "        if obj.shape[1] == 1:\n",
    "\n",
    "            return obj.iloc[:, 0].tolist()\n",
    "\n",
    "        raise ValueError(f\"Expected a single-column pandas DataFrame for {fallback_name}; got columns={list(obj.columns)}\")\n",
    "\n",
    "\n",
    "\n",
    "    if isinstance(obj, pd.Series):\n",
    "\n",
    "        return obj.tolist()\n",
    "\n",
    "\n",
    "\n",
    "    return [obj]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _ensure_problem_strings(values):\n",
    "\n",
    "    return [\"\" if v is None else str(v) for v in values]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def solve_one(problem_id: str, problem_text: str):\n",
    "\n",
    "    modulus = parse_modulus(problem_text)\n",
    "\n",
    "    answer = None\n",
    "\n",
    "    source = \"fallback\"\n",
    "\n",
    "\n",
    "\n",
    "    if OFFLINE_COMPETITION_MODE and time_left_seconds() > 30:\n",
    "\n",
    "        answer, source = OFFLINE_MODEL.solve(problem_text, modulus)\n",
    "\n",
    "\n",
    "\n",
    "    if answer is None and USE_MODEL_API and time_left_seconds() > 40:\n",
    "\n",
    "        for attempt in range(2):\n",
    "\n",
    "            try:\n",
    "\n",
    "                text = call_model(problem_text)\n",
    "\n",
    "                parsed = parse_answer(text, modulus)\n",
    "\n",
    "                if parsed is not None:\n",
    "\n",
    "                    answer = int(parsed)\n",
    "\n",
    "                    source = f\"api_attempt_{attempt + 1}\"\n",
    "\n",
    "                    break\n",
    "\n",
    "            except Exception as exc:\n",
    "\n",
    "                print(f\"[model] id={problem_id} attempt={attempt + 1} error={exc}\")\n",
    "\n",
    "\n",
    "\n",
    "    if answer is None:\n",
    "\n",
    "        if (\n",
    "            IS_COMPETITION_RERUN\n",
    "            and OFFLINE_COMPETITION_MODE\n",
    "            and STRICT_COMPETITION_GUARD\n",
    "            and os.getenv(\"AIMO_FAIL_ON_EMPTY_ANSWER\", \"0\") == \"1\"\n",
    "        ):\n",
    "\n",
    "            raise RuntimeError(\n",
    "                \"Model failed to produce an answer in strict competition mode. \"\n",
    "                \"Set AIMO_FAIL_ON_EMPTY_ANSWER=0 to allow safe fallback.\"\n",
    "            )\n",
    "\n",
    "        answer, source = fallback_heuristic_answer(problem_text, problem_id, modulus)\n",
    "\n",
    "\n",
    "\n",
    "    return int(answer), str(source), modulus\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict(id_, problem, answer=None):\n",
    "\n",
    "    global _SAMPLE_HINT_PRINTED, _LOCAL_WARMUP_DONE\n",
    "\n",
    "\n",
    "\n",
    "    ids = [str(x) for x in _values_from_obj(id_, fallback_name=\"id\")]\n",
    "\n",
    "    problems = _ensure_problem_strings(_values_from_obj(problem, fallback_name=\"problem\"))\n",
    "\n",
    "\n",
    "\n",
    "    if len(problems) == 1 and len(ids) > 1:\n",
    "\n",
    "        problems = problems * len(ids)\n",
    "\n",
    "\n",
    "\n",
    "    if len(ids) != len(problems):\n",
    "\n",
    "        raise ValueError(f\"Mismatched predict batch lengths: ids={len(ids)} problems={len(problems)}\")\n",
    "\n",
    "\n",
    "\n",
    "    if not _SAMPLE_HINT_PRINTED and ids and set(ids).issubset(SAMPLE_IDS):\n",
    "\n",
    "        print(\"Detected Kaggle sample validation set (3 rows). These sample answers are expected to be 0.\")\n",
    "\n",
    "        _SAMPLE_HINT_PRINTED = True\n",
    "\n",
    "    if (not IS_COMPETITION_RERUN) and ids and set(ids).issubset(SAMPLE_IDS):\n",
    "\n",
    "        if LOCAL_SAMPLE_MODEL_WARMUP and not _LOCAL_WARMUP_DONE:\n",
    "\n",
    "            try:\n",
    "\n",
    "                warm_answer, warm_source = OFFLINE_MODEL.solve(\"What is 1+1?\", None)\n",
    "\n",
    "                print(\n",
    "\n",
    "                    \"Local sample warmup: \"\n",
    "\n",
    "                    f\"answer={warm_answer} source={warm_source} model={OFFLINE_MODEL.status}\"\n",
    "\n",
    "                )\n",
    "\n",
    "            except Exception as exc:\n",
    "\n",
    "                print(f\"Local sample warmup failed: {exc}\")\n",
    "\n",
    "            _LOCAL_WARMUP_DONE = True\n",
    "\n",
    "        out_ids = [str(x) for x in ids]\n",
    "\n",
    "        out_answers = [0 for _ in out_ids]\n",
    "\n",
    "        for problem_id in out_ids:\n",
    "\n",
    "            DEBUG_ROWS.append(\n",
    "\n",
    "                {\n",
    "\n",
    "                    \"id\": problem_id,\n",
    "\n",
    "                    \"answer\": 0,\n",
    "\n",
    "                    \"source\": \"sample_validation_passthrough\",\n",
    "\n",
    "                    \"modulus\": None,\n",
    "\n",
    "                    \"time_left_s\": int(time_left_seconds()),\n",
    "\n",
    "                    \"model_status\": OFFLINE_MODEL.status,\n",
    "\n",
    "                }\n",
    "\n",
    "            )\n",
    "\n",
    "            print(\n",
    "\n",
    "                f\"[predict] id={problem_id} answer=0 source=sample_validation_passthrough \"\n",
    "\n",
    "                f\"time_left_s={int(time_left_seconds())} model={OFFLINE_MODEL.status}\"\n",
    "\n",
    "            )\n",
    "\n",
    "        return pl.DataFrame({\"id\": out_ids, \"answer\": out_answers})\n",
    "\n",
    "\n",
    "\n",
    "    out_ids: list[str] = []\n",
    "\n",
    "    out_answers: list[int] = []\n",
    "\n",
    "\n",
    "\n",
    "    for problem_id, problem_text in zip(ids, problems):\n",
    "\n",
    "        answer, source, modulus = solve_one(problem_id, problem_text)\n",
    "\n",
    "        out_ids.append(problem_id)\n",
    "\n",
    "        out_answers.append(int(answer))\n",
    "\n",
    "        DEBUG_ROWS.append(\n",
    "\n",
    "            {\n",
    "\n",
    "                \"id\": problem_id,\n",
    "\n",
    "                \"answer\": int(answer),\n",
    "\n",
    "                \"source\": source,\n",
    "\n",
    "                \"modulus\": modulus,\n",
    "\n",
    "                \"time_left_s\": int(time_left_seconds()),\n",
    "\n",
    "                \"model_status\": OFFLINE_MODEL.status,\n",
    "\n",
    "            }\n",
    "\n",
    "        )\n",
    "\n",
    "        print(\n",
    "\n",
    "            f\"[predict] id={problem_id} answer={answer} source={source} \"\n",
    "\n",
    "            f\"time_left_s={int(time_left_seconds())} model={OFFLINE_MODEL.status}\"\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    return pl.DataFrame({\"id\": out_ids, \"answer\": out_answers})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _load_inference_server_module():\n",
    "\n",
    "    try:\n",
    "\n",
    "        import kaggle_evaluation.aimo_3_inference_server as aimo_server\n",
    "\n",
    "        return aimo_server\n",
    "\n",
    "    except Exception:\n",
    "\n",
    "        candidate_root = Path(f\"/kaggle/input/{COMPETITION}\")\n",
    "\n",
    "        if candidate_root.exists() and str(candidate_root) not in sys.path:\n",
    "\n",
    "            sys.path.append(str(candidate_root))\n",
    "\n",
    "        import kaggle_evaluation.aimo_3_inference_server as aimo_server\n",
    "\n",
    "        return aimo_server\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "AIMO3_SERVER = _load_inference_server_module()\n",
    "\n",
    "inference_server = AIMO3_SERVER.AIMO3InferenceServer(predict)\n",
    "\n",
    "\n",
    "\n",
    "if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "\n",
    "    print(\"Competition rerun detected. Starting inference server...\")\n",
    "\n",
    "    inference_server.serve()\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"Local validation mode. Running local gateway...\")\n",
    "\n",
    "    inference_server.run_local_gateway((str(INPUT_CSV),))\n",
    "\n",
    "\n",
    "\n",
    "    local_parquet = Path(\"submission.parquet\")\n",
    "\n",
    "    if not local_parquet.exists():\n",
    "\n",
    "        raise FileNotFoundError(\"Local gateway did not produce submission.parquet\")\n",
    "\n",
    "\n",
    "\n",
    "    OUTPUT_PARQUET.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if local_parquet.resolve() != OUTPUT_PARQUET.resolve():\n",
    "\n",
    "        OUTPUT_PARQUET.write_bytes(local_parquet.read_bytes())\n",
    "\n",
    "\n",
    "\n",
    "    check = pd.read_parquet(OUTPUT_PARQUET)\n",
    "\n",
    "    if list(check.columns) != [\"id\", \"answer\"]:\n",
    "\n",
    "        raise RuntimeError(f\"Invalid submission columns: {list(check.columns)}\")\n",
    "\n",
    "\n",
    "\n",
    "    check[\"id\"] = check[\"id\"].astype(str)\n",
    "\n",
    "    check[\"answer\"] = pd.to_numeric(check[\"answer\"], errors=\"raise\").astype(\"int64\")\n",
    "\n",
    "    if (check[\"answer\"] < 0).any() or (check[\"answer\"] > 99_999).any():\n",
    "\n",
    "        raise RuntimeError(\"Answer values must be in [0, 99999]\")\n",
    "\n",
    "\n",
    "\n",
    "    check.to_parquet(OUTPUT_PARQUET, index=False)\n",
    "\n",
    "    check.to_csv(OUTPUT_CSV_DEBUG, index=False)\n",
    "\n",
    "\n",
    "\n",
    "    if DEBUG_ROWS:\n",
    "\n",
    "        pd.DataFrame(DEBUG_ROWS).to_csv(\"/kaggle/working/submission_debug_sources.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Saved required output:\", OUTPUT_PARQUET)\n",
    "\n",
    "    print(\"Saved debug CSV:\", OUTPUT_CSV_DEBUG)\n",
    "\n",
    "    print(\"Parquet rows:\", len(check))\n",
    "\n",
    "    print(\n",
    "\n",
    "        \"Parquet files in /kaggle/working:\",\n",
    "\n",
    "        [str(p) for p in Path(\"/kaggle/working\").glob(\"*.parquet\")]\n",
    "\n",
    "        if Path(\"/kaggle/working\").exists()\n",
    "\n",
    "        else [str(p) for p in Path(\".\").glob(\"*.parquet\")],\n",
    "\n",
    "    )\n",
    "\n",
    "    check.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  },
  "kaggle": {
   "language": "python",
   "sourceType": "notebook",
   "isInternetEnabled": false,
   "isGpuEnabled": true,
   "accelerator": "nvidiaH100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
