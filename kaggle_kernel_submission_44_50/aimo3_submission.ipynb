{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9069d307",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2026-01-12T17:23:56.406793Z",
     "iopub.status.busy": "2026-01-12T17:23:56.406216Z",
     "iopub.status.idle": "2026-01-12T17:24:20.349163Z",
     "shell.execute_reply": "2026-01-12T17:24:20.348681Z"
    },
    "papermill": {
     "duration": 23.948239,
     "end_time": "2026-01-12T17:24:20.350211",
     "exception": false,
     "start_time": "2026-01-12T17:23:56.401972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: keras 3.10.0\r\n",
      "Uninstalling keras-3.10.0:\r\n",
      "  Successfully uninstalled keras-3.10.0\r\n",
      "Found existing installation: matplotlib 3.10.0\r\n",
      "Uninstalling matplotlib-3.10.0:\r\n",
      "  Successfully uninstalled matplotlib-3.10.0\r\n",
      "Found existing installation: scikit-learn 1.6.1\r\n",
      "Uninstalling scikit-learn-1.6.1:\r\n",
      "  Successfully uninstalled scikit-learn-1.6.1\r\n",
      "Found existing installation: tensorflow 2.19.0\r\n",
      "Uninstalling tensorflow-2.19.0:\r\n",
      "  Successfully uninstalled tensorflow-2.19.0\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Safety default:\n",
    "# avoid mutating the base Kaggle image unless explicitly requested.\n",
    "# This prevents accidental environment breakage before inference starts.\n",
    "if os.getenv('AIMO_FORCE_PACKAGE_CLEANUP', '0') == '1':\n",
    "    subprocess.run(\n",
    "        [\n",
    "            sys.executable,\n",
    "            '-m',\n",
    "            'pip',\n",
    "            'uninstall',\n",
    "            '--yes',\n",
    "            'keras',\n",
    "            'matplotlib',\n",
    "            'scikit-learn',\n",
    "            'tensorflow',\n",
    "        ],\n",
    "        check=False,\n",
    "    )\n",
    "else:\n",
    "    print('Skipping optional package cleanup (set AIMO_FORCE_PACKAGE_CLEANUP=1 to enable).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3f338ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T17:24:20.355826Z",
     "iopub.status.busy": "2026-01-12T17:24:20.355657Z",
     "iopub.status.idle": "2026-01-12T17:24:20.358294Z",
     "shell.execute_reply": "2026-01-12T17:24:20.357935Z"
    },
    "papermill": {
     "duration": 0.006461,
     "end_time": "2026-01-12T17:24:20.359095",
     "exception": false,
     "start_time": "2026-01-12T17:24:20.352634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e203fa71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T17:24:20.364351Z",
     "iopub.status.busy": "2026-01-12T17:24:20.364203Z",
     "iopub.status.idle": "2026-01-12T17:24:20.366273Z",
     "shell.execute_reply": "2026-01-12T17:24:20.365947Z"
    },
    "papermill": {
     "duration": 0.005606,
     "end_time": "2026-01-12T17:24:20.367006",
     "exception": false,
     "start_time": "2026-01-12T17:24:20.361400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "379a964e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T17:24:20.371851Z",
     "iopub.status.busy": "2026-01-12T17:24:20.371721Z",
     "iopub.status.idle": "2026-01-12T17:24:20.374517Z",
     "shell.execute_reply": "2026-01-12T17:24:20.374134Z"
    },
    "papermill": {
     "duration": 0.006183,
     "end_time": "2026-01-12T17:24:20.375287",
     "exception": false,
     "start_time": "2026-01-12T17:24:20.369104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BOOTSTRAP_DIAGNOSTICS = {\n",
    "    'bootstrap_mode': 'not_started',\n",
    "    'wheel_source': 'unknown',\n",
    "    'install_status': 'not_started',\n",
    "    'details': '',\n",
    "}\n",
    "_BOOTSTRAP_DONE = False\n",
    "_BOOTSTRAP_LOCK_PATH = '/kaggle/working/.aimo_bootstrap.lock'\n",
    "\n",
    "\n",
    "def _deps_ready(required: list[str] | None = None) -> bool:\n",
    "\n",
    "    required_packages = required or []\n",
    "    if not required_packages:\n",
    "        return True\n",
    "\n",
    "    for pkg in required_packages:\n",
    "        try:\n",
    "            __import__(pkg)\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def _running_competition_rerun() -> bool:\n",
    "\n",
    "    raw = os.getenv('KAGGLE_IS_COMPETITION_RERUN')\n",
    "    if raw is None:\n",
    "        return False\n",
    "\n",
    "    return str(raw).strip().lower() in {'1', 'true', 'yes', 'y', 'on'}\n",
    "\n",
    "\n",
    "def _bootstrap_base_packages() -> list[str]:\n",
    "\n",
    "    # Bootstrap policy:\n",
    "    # - minimal (default): no dependency mutations.\n",
    "    # - deepseek: ensure transformers/openai runtime only.\n",
    "    # - full / vllm: install vLLM + harmony runtime.\n",
    "    # - explicit override via AIMO_BOOTSTRAP_PACKAGES.\n",
    "    explicit = os.getenv('AIMO_BOOTSTRAP_PACKAGES', '').strip()\n",
    "    if explicit:\n",
    "        return [p.strip() for p in explicit.split(',') if p.strip()]\n",
    "\n",
    "    mode = os.getenv('AIMO_BOOTSTRAP_MODE', 'minimal').strip().lower()\n",
    "    if mode in {'off', 'none', 'disabled', '0', 'false'}:\n",
    "        return []\n",
    "    if mode in {'deepseek', 'hf', 'transformers'}:\n",
    "        return ['transformers', 'openai']\n",
    "    if mode in {'full', 'vllm'}:\n",
    "        return ['vllm', 'openai_harmony', 'openai']\n",
    "\n",
    "    # minimal / safe / light\n",
    "    return []\n",
    "\n",
    "def _bootstrap_lock(lock_path: str, timeout_sec: int = 120):\n",
    "\n",
    "    import time as _time\n",
    "\n",
    "    try:\n",
    "        import fcntl  # type: ignore\n",
    "    except Exception:\n",
    "        fcntl = None\n",
    "\n",
    "    os.makedirs(os.path.dirname(lock_path), exist_ok=True)\n",
    "    handle = open(lock_path, 'a+', encoding='utf-8')\n",
    "\n",
    "    if fcntl is None:\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            handle.close()\n",
    "        return\n",
    "\n",
    "    start = _time.time()\n",
    "    while True:\n",
    "        try:\n",
    "            fcntl.flock(handle.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
    "            break\n",
    "        except BlockingIOError:\n",
    "            if (_time.time() - start) > float(timeout_sec):\n",
    "                handle.close()\n",
    "                raise TimeoutError(f'Timed out waiting for bootstrap lock: {lock_path}')\n",
    "            _time.sleep(0.2)\n",
    "\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        try:\n",
    "            fcntl.flock(handle.fileno(), fcntl.LOCK_UN)\n",
    "        except Exception:\n",
    "            pass\n",
    "        handle.close()\n",
    "\n",
    "\n",
    "def set_env(input_archive, temp_dir):\n",
    "    global _BOOTSTRAP_DONE\n",
    "\n",
    "    base_packages = _bootstrap_base_packages()\n",
    "    BOOTSTRAP_DIAGNOSTICS['wheel_source'] = str(input_archive)\n",
    "\n",
    "    is_kaggle_runtime = os.path.exists('/kaggle/input') or os.path.exists('/kaggle/working')\n",
    "\n",
    "    if (\n",
    "        (not _running_competition_rerun())\n",
    "        and (not is_kaggle_runtime)\n",
    "        and os.getenv('AIMO_FORCE_BOOTSTRAP_LOCAL', '0') != '1'\n",
    "    ):\n",
    "        BOOTSTRAP_DIAGNOSTICS.update(\n",
    "            {\n",
    "                'bootstrap_mode': 'skipped_local_dev',\n",
    "                'install_status': 'skipped',\n",
    "                'details': 'Non-Kaggle local runtime detected; heavy bootstrap disabled by default.',\n",
    "            }\n",
    "        )\n",
    "        _BOOTSTRAP_DONE = True\n",
    "        print(\n",
    "            'Dependency bootstrap skipped in non-Kaggle local mode '\n",
    "            '(set AIMO_FORCE_BOOTSTRAP_LOCAL=1 to override).'\n",
    "        )\n",
    "        return\n",
    "\n",
    "    if os.getenv('AIMO_SKIP_DEP_BOOTSTRAP', '0') == '1':\n",
    "        BOOTSTRAP_DIAGNOSTICS.update(\n",
    "            {\n",
    "                'bootstrap_mode': 'skipped_env',\n",
    "                'install_status': 'skipped',\n",
    "                'details': 'AIMO_SKIP_DEP_BOOTSTRAP=1',\n",
    "            }\n",
    "        )\n",
    "        _BOOTSTRAP_DONE = True\n",
    "        print('Skipping dependency bootstrap via AIMO_SKIP_DEP_BOOTSTRAP=1')\n",
    "        return\n",
    "\n",
    "    extra_packages = []\n",
    "    extra_raw = os.getenv('AIMO_BOOTSTRAP_EXTRA_PACKAGES', '').strip()\n",
    "    if extra_raw:\n",
    "        for pkg in [p.strip() for p in extra_raw.split(',') if p.strip()]:\n",
    "            if pkg not in extra_packages:\n",
    "                extra_packages.append(pkg)\n",
    "\n",
    "    required_packages = list(base_packages)\n",
    "    for pkg in extra_packages:\n",
    "        if pkg not in required_packages:\n",
    "            required_packages.append(pkg)\n",
    "\n",
    "    if not required_packages:\n",
    "        BOOTSTRAP_DIAGNOSTICS.update(\n",
    "            {\n",
    "                'bootstrap_mode': 'minimal_noop',\n",
    "                'install_status': 'skipped',\n",
    "                'details': (\n",
    "                    'No bootstrap packages requested (AIMO_BOOTSTRAP_MODE=minimal). '\n",
    "                    'Set AIMO_BOOTSTRAP_MODE=full to install vllm/openai_harmony.'\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        _BOOTSTRAP_DONE = True\n",
    "        print('Dependency bootstrap running in minimal mode: no package mutations performed.')\n",
    "        return\n",
    "\n",
    "    if _BOOTSTRAP_DONE:\n",
    "        print('Dependency bootstrap already completed in this process.')\n",
    "        return\n",
    "\n",
    "    if _deps_ready(required_packages):\n",
    "        BOOTSTRAP_DIAGNOSTICS.update(\n",
    "            {\n",
    "                'bootstrap_mode': 'already_ready',\n",
    "                'install_status': 'ready',\n",
    "                'details': f'Required runtime packages already importable: {required_packages}',\n",
    "            }\n",
    "        )\n",
    "        _BOOTSTRAP_DONE = True\n",
    "        print('Dependency bootstrap skipped: required packages already importable.')\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(input_archive):\n",
    "        BOOTSTRAP_DIAGNOSTICS.update(\n",
    "            {\n",
    "                'bootstrap_mode': 'missing_wheels_archive',\n",
    "                'install_status': 'failed',\n",
    "                'details': f'missing_archive:{input_archive}',\n",
    "            }\n",
    "        )\n",
    "        raise FileNotFoundError(f'Missing wheels archive: {input_archive}')\n",
    "\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    wheels_dir = os.path.join(temp_dir, 'wheels')\n",
    "\n",
    "    with _bootstrap_lock(_BOOTSTRAP_LOCK_PATH):\n",
    "        if _deps_ready(required_packages):\n",
    "            BOOTSTRAP_DIAGNOSTICS.update(\n",
    "                {\n",
    "                    'bootstrap_mode': 'already_ready',\n",
    "                    'install_status': 'ready',\n",
    "                    'details': 'Runtime became ready while waiting on lock.',\n",
    "                }\n",
    "            )\n",
    "            _BOOTSTRAP_DONE = True\n",
    "            print('Dependency bootstrap skipped after lock: required packages now importable.')\n",
    "            return\n",
    "\n",
    "        if not os.path.exists(wheels_dir):\n",
    "            subprocess.run(['tar', '-xzf', input_archive, '-C', temp_dir], check=True)\n",
    "\n",
    "        BOOTSTRAP_DIAGNOSTICS.update(\n",
    "            {\n",
    "                'bootstrap_mode': 'installing',\n",
    "                'install_status': 'running',\n",
    "                'details': f'packages={required_packages}',\n",
    "            }\n",
    "        )\n",
    "\n",
    "        pip_cmd = [\n",
    "            sys.executable,\n",
    "            '-m',\n",
    "            'pip',\n",
    "            'install',\n",
    "            '--no-index',\n",
    "            '--find-links',\n",
    "            wheels_dir,\n",
    "            '--upgrade-strategy',\n",
    "            'only-if-needed',\n",
    "        ]\n",
    "\n",
    "        if os.getenv('AIMO_BOOTSTRAP_NO_DEPS', '0') == '1':\n",
    "            pip_cmd.append('--no-deps')\n",
    "\n",
    "        pip_cmd.extend(required_packages)\n",
    "        subprocess.run(pip_cmd, check=True)\n",
    "\n",
    "    if not _deps_ready(required_packages):\n",
    "        BOOTSTRAP_DIAGNOSTICS.update(\n",
    "            {\n",
    "                'bootstrap_mode': 'post_install_check_failed',\n",
    "                'install_status': 'failed',\n",
    "                'details': 'Required imports still missing after installation.',\n",
    "            }\n",
    "        )\n",
    "        raise RuntimeError('Dependency bootstrap completed but runtime imports are still missing.')\n",
    "\n",
    "    BOOTSTRAP_DIAGNOSTICS.update(\n",
    "        {\n",
    "            'bootstrap_mode': 'installed',\n",
    "            'install_status': 'ready',\n",
    "            'details': 'Offline wheel installation completed successfully.',\n",
    "        }\n",
    "    )\n",
    "    _BOOTSTRAP_DONE = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "772fcbee",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2026-01-12T17:24:20.380059Z",
     "iopub.status.busy": "2026-01-12T17:24:20.379925Z",
     "iopub.status.idle": "2026-01-12T17:27:09.665632Z",
     "shell.execute_reply": "2026-01-12T17:27:09.665185Z"
    },
    "papermill": {
     "duration": 169.289557,
     "end_time": "2026-01-12T17:27:09.666969",
     "exception": false,
     "start_time": "2026-01-12T17:24:20.377412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/tmp/setup/wheels\n",
      "Processing /kaggle/tmp/setup/wheels/unsloth-2025.12.9-py3-none-any.whl\n",
      "Processing /kaggle/tmp/setup/wheels/trl-0.24.0-py3-none-any.whl\n",
      "Processing /kaggle/tmp/setup/wheels/vllm-0.11.2-cp38-abi3-manylinux1_x86_64.whl\n",
      "Processing /kaggle/tmp/setup/wheels/openai_harmony-0.0.8-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Processing /kaggle/tmp/setup/wheels/unsloth_zoo-2025.12.7-py3-none-any.whl (from unsloth)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.45.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth) (25.0)\n",
      "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.8.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.23.0+cu126)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.0.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.9.5)\n",
      "Processing /kaggle/tmp/setup/wheels/tyro-1.0.3-py3-none-any.whl (from unsloth)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.29.5)\n",
      "Processing /kaggle/tmp/setup/wheels/xformers-0.0.33.post1-cp39-abi3-manylinux_2_28_x86_64.whl (from unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (from unsloth)\n",
      "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (3.4.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.2.1)\n",
      "Processing /kaggle/tmp/setup/wheels/datasets-4.3.0-py3-none-any.whl (from unsloth)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (1.11.0)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.17.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.36.0)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.1.9)\n",
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.35.2)\n",
      "Requirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.3,>=4.51.3 in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.57.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from vllm) (2025.11.3)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from vllm) (5.5.2)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.32.5)\n",
      "Requirement already satisfied: blake3 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.0.8)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from vllm) (9.0.0)\n",
      "Requirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.22.1)\n",
      "Requirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.119.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from vllm) (3.13.2)\n",
      "Requirement already satisfied: openai>=1.99.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.109.1)\n",
      "Requirement already satisfied: pydantic>=2.12.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (2.12.5)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.23.1)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from vllm) (11.3.0)\n",
      "Processing /kaggle/tmp/setup/wheels/prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (from vllm)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (0.12.0)\n",
      "Processing /kaggle/tmp/setup/wheels/lm_format_enforcer-0.11.3-py3-none-any.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/llguidance-1.3.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/outlines_core-0.2.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/diskcache-5.6.3-py3-none-any.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/lark-1.2.2-py3-none-any.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/xgrammar-0.1.25-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from vllm) (4.15.0)\n",
      "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.12/dist-packages (from vllm) (3.20.1)\n",
      "Processing /kaggle/tmp/setup/wheels/partial_json_parser-0.2.1.1.post7-py3-none-any.whl (from vllm)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (26.2.1)\n",
      "Processing /kaggle/tmp/setup/wheels/msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/gguf-0.17.1-py3-none-any.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/mistral_common-1.8.8-py3-none-any.whl (from mistral_common[image]>=1.8.5->vllm)\n",
      "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (4.12.0.88)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from vllm) (6.0.3)\n",
      "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from vllm) (1.17.0)\n",
      "Processing /kaggle/tmp/setup/wheels/setuptools-80.9.0-py3-none-any.whl (from vllm)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from vllm) (0.8.1)\n",
      "Processing /kaggle/tmp/setup/wheels/compressed_tensors-0.12.2-py3-none-any.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/depyf-0.20.0-py3-none-any.whl (from vllm)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from vllm) (3.1.1)\n",
      "Processing /kaggle/tmp/setup/wheels/watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\n",
      "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.12/dist-packages (from vllm) (4.0.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from vllm) (1.15.3)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from vllm) (1.13.0)\n",
      "Processing /kaggle/tmp/setup/wheels/pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/cbor2-5.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/anthropic-0.71.0-py3-none-any.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/model_hosting_container_standards-0.1.12-py3-none-any.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/numba-0.61.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from vllm)\n",
      "Requirement already satisfied: ray>=2.48.0 in /usr/local/lib/python3.12/dist-packages (from ray[cgraph]>=2.48.0->vllm) (2.52.1)\n",
      "Processing /kaggle/tmp/setup/wheels/torch-2.9.0+cu128-cp312-cp312-manylinux_2_28_x86_64.whl (from unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/torchaudio-2.9.0+cu128-cp312-cp312-manylinux_2_28_x86_64.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/torchvision-0.24.0+cu128-cp312-cp312-manylinux_2_28_x86_64.whl (from unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/flashinfer_python-0.5.2-py3-none-any.whl (from vllm)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (4.12.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (1.9.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (0.17.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (0.11.1)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic==0.71.0->vllm) (1.3.1)\n",
      "Processing /kaggle/tmp/setup/wheels/loguru-0.7.3-py3-none-any.whl (from compressed-tensors==0.12.2->vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/astor-0.8.1-py2.py3-none-any.whl (from depyf==0.20.0->vllm)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from depyf==0.20.0->vllm) (0.4.0)\n",
      "Processing /kaggle/tmp/setup/wheels/apache_tvm_ffi-0.1.7-cp312-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (from flashinfer-python==0.5.2->vllm)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from flashinfer-python==0.5.2->vllm) (8.3.1)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_cudnn_frontend-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (from flashinfer-python==0.5.2->vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_cutlass_dsl-4.3.4-cp312-cp312-manylinux_2_28_x86_64.whl (from flashinfer-python==0.5.2->vllm)\n",
      "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.12/dist-packages (from flashinfer-python==0.5.2->vllm) (12.575.51)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from flashinfer-python==0.5.2->vllm) (0.9.0)\n",
      "Processing /kaggle/tmp/setup/wheels/interegular-0.3.3-py37-none-any.whl (from lm-format-enforcer==0.11.3->vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from numba==0.61.2->vllm)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (2025.10.0)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (9.10.2.21)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (0.7.1)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch>=2.4.0->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (from unsloth)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (0.6.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (22.0.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.6.0)\n",
      "Processing /kaggle/tmp/setup/wheels/multiprocess-0.70.16-py312-none-any.whl (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/fsspec-2025.9.0-py3-none-any.whl (from torch>=2.4.0->unsloth)\n",
      "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.48.0)\n",
      "Processing /kaggle/tmp/setup/wheels/fastapi_cli-0.0.20-py3-none-any.whl (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.3.0)\n",
      "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.38.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (1.2.1rc0)\n",
      "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (4.25.1)\n",
      "Processing /kaggle/tmp/setup/wheels/pydantic_extra_types-2.10.6-py3-none-any.whl (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm)\n",
      "Requirement already satisfied: jmespath in /usr/local/lib/python3.12/dist-packages (from model-hosting-container-standards<1.0.0->vllm) (1.0.1)\n",
      "INFO: pip is looking at multiple versions of model-hosting-container-standards to determine which version is compatible with other requirements. This could take a while.\n",
      "Processing /kaggle/tmp/setup/wheels/fastapi-0.128.0-py3-none-any.whl (from vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/annotated_doc-0.0.4-py3-none-any.whl (from fastapi[standard]>=0.115.0->vllm)\n",
      "Requirement already satisfied: pydantic-settings>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.11.0)\n",
      "Processing /kaggle/tmp/setup/wheels/starlette-0.50.0-py3-none-any.whl (from fastapi[standard]>=0.115.0->vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/supervisor-4.3.0-py2.py3-none-any.whl (from model-hosting-container-standards<1.0.0->vllm)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.12.0->vllm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.12.0->vllm) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.12.0->vllm) (0.4.2)\n",
      "INFO: pip is looking at multiple versions of ray to determine which version is compatible with other requirements. This could take a while.\n",
      "Processing /kaggle/tmp/setup/wheels/ray-2.53.0-cp312-cp312-manylinux2014_x86_64.whl (from ray[cgraph]>=2.48.0->vllm)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm) (1.1.2)\n",
      "INFO: pip is looking at multiple versions of ray[cgraph] to determine which version is compatible with other requirements. This could take a while.\n",
      "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.12/dist-packages (from ray[cgraph]>=2.48.0->vllm) (13.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm) (2025.11.12)\n",
      "Processing /kaggle/tmp/setup/wheels/torchao-0.15.0+cu128-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (from unsloth_zoo>=2025.12.7->unsloth)\n",
      "Processing /kaggle/tmp/setup/wheels/cut_cross_entropy-25.1.1-py3-none-any.whl (from unsloth_zoo>=2025.12.7->unsloth)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm) (1.22.0)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth) (8.7.0)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (4.4.4)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.8.0)\n",
      "Requirement already satisfied: typer>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.20.0)\n",
      "Processing /kaggle/tmp/setup/wheels/rich_toolkit-0.17.1-py3-none-any.whl (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/fastapi_cloud_cli-0.8.0-py3-none-any.whl (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic==0.71.0->vllm) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic==0.71.0->vllm) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm) (0.27.1)\n",
      "Processing /kaggle/tmp/setup/wheels/cuda_python-13.1.1-py3-none-any.whl (from nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.2->vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/pycountry-24.6.1-py3-none-any.whl (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.5->mistral_common[image]>=1.8.5->vllm)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings>=2.0.0->fastapi[standard]>=0.115.0->vllm) (1.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\n",
      "Processing /kaggle/tmp/setup/wheels/httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.12/dist-packages (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm) (0.8.3)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.3)\n",
      "Processing /kaggle/tmp/setup/wheels/cuda_bindings-13.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (from cuda-python>=12.8->nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.2->vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/cuda_pathfinder-1.3.3-py3-none-any.whl (from cuda-python>=12.8->nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.2->vllm)\n",
      "Processing /kaggle/tmp/setup/wheels/rignore-0.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "Requirement already satisfied: sentry-sdk>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.42.1)\n",
      "Processing /kaggle/tmp/setup/wheels/fastar-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.12/dist-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.2.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n",
      "Installing collected packages: torchao, supervisor, uvloop, triton, setuptools, setproctitle, rignore, pycountry, pybase64, partial-json-parser, outlines_core, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cudnn-frontend, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, multiprocess, msgspec, loguru, llvmlite, llguidance, lark, interegular, httptools, gguf, fsspec, fastar, diskcache, cuda-pathfinder, cbor2, astor, apache-tvm-ffi, annotated-doc, watchfiles, tyro, starlette, nvidia-cusparse-cu12, nvidia-cufft-cu12, numba, depyf, cuda-bindings, rich-toolkit, pydantic-extra-types, prometheus-fastapi-instrumentator, openai_harmony, nvidia-cusolver-cu12, lm-format-enforcer, fastapi, cuda-python, anthropic, torch, ray, nvidia-cutlass-dsl, model-hosting-container-standards, fastapi-cloud-cli, fastapi-cli, datasets, xgrammar, xformers, torchvision, torchaudio, mistral_common, flashinfer-python, cut_cross_entropy, compressed-tensors, bitsandbytes, trl, unsloth_zoo, vllm, unsloth\n",
      "  Attempting uninstall: torchao\n",
      "    Found existing installation: torchao 0.10.0\n",
      "    Uninstalling torchao-0.10.0:\n",
      "      Successfully uninstalled torchao-0.10.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.4.0\n",
      "    Uninstalling triton-3.4.0:\n",
      "      Successfully uninstalled triton-3.4.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 75.2.0\n",
      "    Uninstalling setuptools-75.2.0:\n",
      "      Successfully uninstalled setuptools-75.2.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
      "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-nvshmem-cu12\n",
      "    Found existing installation: nvidia-nvshmem-cu12 3.4.5\n",
      "    Uninstalling nvidia-nvshmem-cu12-3.4.5:\n",
      "      Successfully uninstalled nvidia-nvshmem-cu12-3.4.5\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufile-cu12\n",
      "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
      "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
      "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.18\n",
      "    Uninstalling multiprocess-0.70.18:\n",
      "      Successfully uninstalled multiprocess-0.70.18\n",
      "  Attempting uninstall: llvmlite\n",
      "    Found existing installation: llvmlite 0.43.0\n",
      "    Uninstalling llvmlite-0.43.0:\n",
      "      Successfully uninstalled llvmlite-0.43.0\n",
      "  Attempting uninstall: lark\n",
      "    Found existing installation: lark 1.3.0\n",
      "    Uninstalling lark-1.3.0:\n",
      "      Successfully uninstalled lark-1.3.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.10.0\n",
      "    Uninstalling fsspec-2025.10.0:\n",
      "      Successfully uninstalled fsspec-2025.10.0\n",
      "  Attempting uninstall: starlette\n",
      "    Found existing installation: starlette 0.48.0\n",
      "    Uninstalling starlette-0.48.0:\n",
      "      Successfully uninstalled starlette-0.48.0\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: numba\n",
      "    Found existing installation: numba 0.60.0\n",
      "    Uninstalling numba-0.60.0:\n",
      "      Successfully uninstalled numba-0.60.0\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: fastapi\n",
      "    Found existing installation: fastapi 0.119.1\n",
      "    Uninstalling fastapi-0.119.1:\n",
      "      Successfully uninstalled fastapi-0.119.1\n",
      "  Attempting uninstall: cuda-python\n",
      "    Found existing installation: cuda-python 12.6.2.post1\n",
      "    Uninstalling cuda-python-12.6.2.post1:\n",
      "      Successfully uninstalled cuda-python-12.6.2.post1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.8.0+cu126\n",
      "    Uninstalling torch-2.8.0+cu126:\n",
      "      Successfully uninstalled torch-2.8.0+cu126\n",
      "  Attempting uninstall: ray\n",
      "    Found existing installation: ray 2.52.1\n",
      "    Uninstalling ray-2.52.1:\n",
      "      Successfully uninstalled ray-2.52.1\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.4.1\n",
      "    Uninstalling datasets-4.4.1:\n",
      "      Successfully uninstalled datasets-4.4.1\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.23.0+cu126\n",
      "    Uninstalling torchvision-0.23.0+cu126:\n",
      "      Successfully uninstalled torchvision-0.23.0+cu126\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.8.0+cu126\n",
      "    Uninstalling torchaudio-2.8.0+cu126:\n",
      "      Successfully uninstalled torchaudio-2.8.0+cu126\n",
      "Successfully installed annotated-doc-0.0.4 anthropic-0.71.0 apache-tvm-ffi-0.1.7 astor-0.8.1 bitsandbytes-0.49.0 cbor2-5.7.1 compressed-tensors-0.12.2 cuda-bindings-13.1.1 cuda-pathfinder-1.3.3 cuda-python-13.1.1 cut_cross_entropy-25.1.1 datasets-4.3.0 depyf-0.20.0 diskcache-5.6.3 fastapi-0.128.0 fastapi-cli-0.0.20 fastapi-cloud-cli-0.8.0 fastar-0.8.0 flashinfer-python-0.5.2 fsspec-2025.9.0 gguf-0.17.1 httptools-0.7.1 interegular-0.3.3 lark-1.2.2 llguidance-1.3.0 llvmlite-0.44.0 lm-format-enforcer-0.11.3 loguru-0.7.3 mistral_common-1.8.8 model-hosting-container-standards-0.1.12 msgspec-0.20.0 multiprocess-0.70.16 numba-0.61.2 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-frontend-1.17.0 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cutlass-dsl-4.3.4 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 openai_harmony-0.0.8 outlines_core-0.2.11 partial-json-parser-0.2.1.1.post7 prometheus-fastapi-instrumentator-7.1.0 pybase64-1.4.3 pycountry-24.6.1 pydantic-extra-types-2.10.6 ray-2.53.0 rich-toolkit-0.17.1 rignore-0.7.6 setproctitle-1.3.7 setuptools-80.9.0 starlette-0.50.0 supervisor-4.3.0 torch-2.9.0+cu128 torchao-0.15.0+cu128 torchaudio-2.9.0+cu128 torchvision-0.24.0+cu128 triton-3.5.0 trl-0.24.0 tyro-1.0.3 unsloth-2025.12.9 unsloth_zoo-2025.12.7 uvloop-0.22.1 vllm-0.11.2 watchfiles-1.1.1 xformers-0.0.33.post1 xgrammar-0.1.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kauldron 1.3.0 requires scikit-learn, which is not installed.\n",
      "kauldron 1.3.0 requires tensorflow, which is not installed.\n",
      "ydata-profiling 4.18.0 requires matplotlib<=3.10,>=3.5, which is not installed.\n",
      "pyldavis 3.4.1 requires scikit-learn>=1.0.0, which is not installed.\n",
      "stable-baselines3 2.1.0 requires matplotlib, which is not installed.\n",
      "sentence-transformers 5.1.1 requires scikit-learn, which is not installed.\n",
      "librosa 0.11.0 requires scikit-learn>=1.1.0, which is not installed.\n",
      "cuml-cu12 25.6.0 requires scikit-learn>=1.5, which is not installed.\n",
      "bigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "bigframes 2.26.0 requires matplotlib>=3.7.1, which is not installed.\n",
      "arviz 0.22.0 requires matplotlib>=3.8, which is not installed.\n",
      "pynndescent 0.5.13 requires scikit-learn>=0.18, which is not installed.\n",
      "shap 0.49.1 requires scikit-learn, which is not installed.\n",
      "fastai 2.8.4 requires matplotlib, which is not installed.\n",
      "fastai 2.8.4 requires scikit-learn, which is not installed.\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, which is not installed.\n",
      "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.2.19 which is incompatible.\n",
      "cudf-cu12 25.6.0 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\n",
      "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "gradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\n",
      "cuml-cu12 25.6.0 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\n",
      "bigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "pylibraft-cu12 25.6.0 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\n",
      "cuvs-cu12 25.6.1 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\n",
      "fastai 2.8.4 requires torch<2.9,>=1.10, but you have torch 2.9.0+cu128 which is incompatible.\n",
      "rmm-cu12 25.6.0 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\n",
      "pylibcudf-cu12 25.6.0 requires cuda-python<13.0a0,>=12.6.2, but you have cuda-python 13.1.1 which is incompatible.\n",
      "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "set_env(\n",
    "    input_archive='/kaggle/input/aimo-3-utils/wheels.tar.gz', \n",
    "    temp_dir='/kaggle/tmp/setup'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4182af53",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2026-01-12T17:27:09.676870Z",
     "iopub.status.busy": "2026-01-12T17:27:09.676696Z",
     "iopub.status.idle": "2026-01-12T17:27:09.683890Z",
     "shell.execute_reply": "2026-01-12T17:27:09.683490Z"
    },
    "papermill": {
     "duration": 0.013412,
     "end_time": "2026-01-12T17:27:09.684791",
     "exception": false,
     "start_time": "2026-01-12T17:27:09.671379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cl100k_base.tiktoken\n",
      "o200k_base.tiktoken\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['ls', '/kaggle/tmp/setup/tiktoken_encodings'], returncode=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists('/kaggle/tmp/setup/tiktoken_encodings'):\n",
    "    subprocess.run(['ls', '/kaggle/tmp/setup/tiktoken_encodings'], check=False)\n",
    "else:\n",
    "    print('No tiktoken_encodings directory found under /kaggle/tmp/setup (continuing).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd8cbb74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T17:27:09.694753Z",
     "iopub.status.busy": "2026-01-12T17:27:09.694600Z",
     "iopub.status.idle": "2026-01-12T17:27:09.697146Z",
     "shell.execute_reply": "2026-01-12T17:27:09.696800Z"
    },
    "papermill": {
     "duration": 0.008442,
     "end_time": "2026-01-12T17:27:09.697869",
     "exception": false,
     "start_time": "2026-01-12T17:27:09.689427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['TRANSFORMERS_NO_TF'] = '1'\n",
    "os.environ['TRANSFORMERS_NO_FLAX'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "if os.path.exists('/usr/local/cuda/bin/ptxas'):\n",
    "    os.environ['TRITON_PTXAS_PATH'] = '/usr/local/cuda/bin/ptxas'\n",
    "\n",
    "if os.path.exists('/kaggle/tmp/setup/tiktoken_encodings'):\n",
    "    os.environ['TIKTOKEN_ENCODINGS_BASE'] = '/kaggle/tmp/setup/tiktoken_encodings'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "035e4f22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T17:27:09.707189Z",
     "iopub.status.busy": "2026-01-12T17:27:09.707057Z",
     "iopub.status.idle": "2026-01-12T17:27:14.712253Z",
     "shell.execute_reply": "2026-01-12T17:27:14.711805Z"
    },
    "papermill": {
     "duration": 5.011445,
     "end_time": "2026-01-12T17:27:14.713696",
     "exception": false,
     "start_time": "2026-01-12T17:27:09.702251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import queue\n",
    "import threading\n",
    "import contextlib\n",
    "from typing import Optional\n",
    "from jupyter_client import KernelManager\n",
    "from collections import Counter, defaultdict\n",
    "from concurrent.futures import as_completed, ThreadPoolExecutor\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# Track optional import failures so startup preflight can explain safe-mode causes.\n",
    "OPTIONAL_IMPORT_ERRORS: dict[str, str] = {}\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except Exception as exc:\n",
    "    OpenAI = None\n",
    "    OPTIONAL_IMPORT_ERRORS['openai'] = str(exc)\n",
    "\n",
    "try:\n",
    "    from openai_harmony import (\n",
    "        HarmonyEncodingName,\n",
    "        load_harmony_encoding,\n",
    "        SystemContent,\n",
    "        ReasoningEffort,\n",
    "        ToolNamespaceConfig,\n",
    "        Author,\n",
    "        Message,\n",
    "        Role,\n",
    "        TextContent,\n",
    "        Conversation,\n",
    "    )\n",
    "except Exception as exc:\n",
    "    HarmonyEncodingName = None\n",
    "    load_harmony_encoding = None\n",
    "    SystemContent = None\n",
    "    ReasoningEffort = None\n",
    "    ToolNamespaceConfig = None\n",
    "    Author = None\n",
    "    Message = None\n",
    "    Role = None\n",
    "    TextContent = None\n",
    "    Conversation = None\n",
    "    OPTIONAL_IMPORT_ERRORS['openai_harmony'] = str(exc)\n",
    "\n",
    "try:\n",
    "    from transformers import set_seed\n",
    "except Exception as exc:\n",
    "    set_seed = None\n",
    "    OPTIONAL_IMPORT_ERRORS['transformers'] = str(exc)\n",
    "\n",
    "import kaggle_evaluation.aimo_3_inference_server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00a589bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T17:27:14.724897Z",
     "iopub.status.busy": "2026-01-12T17:27:14.724244Z",
     "iopub.status.idle": "2026-01-12T17:27:14.729375Z",
     "shell.execute_reply": "2026-01-12T17:27:14.728969Z"
    },
    "papermill": {
     "duration": 0.011452,
     "end_time": "2026-01-12T17:27:14.730138",
     "exception": false,
     "start_time": "2026-01-12T17:27:14.718686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _env_int(name: str, default: int) -> int:\n",
    "\n",
    "    raw = os.getenv(name)\n",
    "\n",
    "    if raw is None:\n",
    "        return int(default)\n",
    "\n",
    "    try:\n",
    "        return int(raw)\n",
    "    except Exception:\n",
    "        return int(default)\n",
    "\n",
    "\n",
    "def _env_bool(name: str, default: bool) -> bool:\n",
    "\n",
    "    raw = os.getenv(name)\n",
    "\n",
    "    if raw is None:\n",
    "        return bool(default)\n",
    "\n",
    "    return str(raw).strip().lower() in {'1', 'true', 'yes', 'y', 'on'}\n",
    "\n",
    "\n",
    "class CFG:\n",
    "\n",
    "    system_prompt = (\n",
    "        'You are an elite mathematical problem solver with expertise at the International '\n",
    "        'Mathematical Olympiad (IMO) level. Your goal is to find the correct answer through '\n",
    "        'rigorous mathematical reasoning.\\n\\n'\n",
    "        'Always end with either FINAL_ANSWER: <integer> or \\\\boxed{<integer>}.'\n",
    "    )\n",
    "\n",
    "    problem_type_prompts = {\n",
    "        'number_theory': (\n",
    "            'Focus on modular arithmetic, valuations, CRT, multiplicative order, and residue checks. '\n",
    "            'Always verify modulus handling before finalizing.'\n",
    "        ),\n",
    "        'algebra': (\n",
    "            'Focus on symbolic manipulation, identities, polynomial constraints, and algebraic invariants. '\n",
    "            'Use an independent symbolic/numeric check before final answer.'\n",
    "        ),\n",
    "        'geometry': (\n",
    "            'Translate to coordinates or vectors when useful, verify area/length relations, '\n",
    "            'and run a numeric sanity check for candidate answers.'\n",
    "        ),\n",
    "        'combinatorics': (\n",
    "            'Use counting invariants, generating functions, bijections, and boundary-case checks. '\n",
    "            'Double-check recurrence/base-case logic.'\n",
    "        ),\n",
    "        'misc': (\n",
    "            'Decompose problem into sub-goals, run independent verification, and prefer robust derivations '\n",
    "            'over fragile shortcuts.'\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    tool_prompt = (\n",
    "        'Use this tool to execute Python code for:\\n'\n",
    "        '- complex calculations that are error-prone by hand\\n'\n",
    "        '- symbolic verification (sympy) and numerical sanity checks\\n'\n",
    "        '- brute-force checks on small cases\\n\\n'\n",
    "        'Keep code concise and deterministic. Print intermediate values that justify conclusions.'\n",
    "    )\n",
    "\n",
    "    preference_prompt = (\n",
    "        'You may use `math`, `numpy`, and `sympy`. '\n",
    "        'Prefer exact symbolic reasoning first, then verify numerically.'\n",
    "    )\n",
    "\n",
    "    served_model_name = 'gpt-oss'\n",
    "    model_path_candidates = [\n",
    "        '/kaggle/input/models/danielhanchen/gpt-oss-20b/transformers/default/1',\n",
    "        '/kaggle/input/gpt-oss-120b/transformers/default/1',\n",
    "        '/kaggle/input/models/danielhanchen/gpt-oss-120b/transformers/default/1',\n",
    "        '/kaggle/input/models/deepseek-ai/deepseek-math/pytorch/deepseek-math-7b-instruct/1',\n",
    "    ]\n",
    "    model_path = next((p for p in model_path_candidates if os.path.exists(p)), model_path_candidates[0])\n",
    "\n",
    "    kv_cache_dtype = 'fp8_e4m3'\n",
    "    dtype = 'auto'\n",
    "\n",
    "    # Runtime controls via env (decision-complete and non-breaking).\n",
    "    strict_submission_mode = _env_bool('AIMO_STRICT_SUBMISSION_MODE', True)\n",
    "    local_warmup_solver = _env_bool('AIMO_LOCAL_WARMUP_SOLVER', True)\n",
    "    fail_on_local_warmup_error = _env_bool('AIMO_FAIL_ON_LOCAL_WARMUP_ERROR', True)\n",
    "    force_model_family = os.getenv('AIMO_FORCE_MODEL_FAMILY', 'auto').strip().lower()\n",
    "    disable_gpt_oss_on_sm_lt = _env_int('AIMO_DISABLE_GPT_OSS_ON_SM_LT', 80)\n",
    "    deepseek_attempts_high = _env_int('AIMO_DEEPSEEK_ATTEMPTS_HIGH', 8)\n",
    "    deepseek_attempts_med = _env_int('AIMO_DEEPSEEK_ATTEMPTS_MED', 6)\n",
    "    deepseek_attempts_low = _env_int('AIMO_DEEPSEEK_ATTEMPTS_LOW', 4)\n",
    "    deepseek_verify_top_k = _env_int('AIMO_DEEPSEEK_VERIFY_TOP_K', 2)\n",
    "    prefer_small_model_below_gb = _env_int('AIMO_PREFER_SMALL_MODEL_BELOW_GB', 28)\n",
    "    notebook_limit = _env_int('AIMO_NOTEBOOK_LIMIT_SEC', 17400)\n",
    "    high_problem_timeout = _env_int('AIMO_MAX_PROBLEM_SEC', 300)\n",
    "    base_problem_timeout = _env_int('AIMO_MIN_PROBLEM_SEC', 45)\n",
    "\n",
    "    finalization_reserve_sec = _env_int('AIMO_END_BUFFER_SEC', _env_int('AIMO_FINALIZATION_RESERVE_SEC', 240))\n",
    "    disagreement_extra_attempts = _env_int('AIMO_DISAGREEMENT_EXTRA_ATTEMPTS', 2)\n",
    "    verification_attempts = _env_int('AIMO_VERIFICATION_ATTEMPTS', 2)\n",
    "\n",
    "    server_timeout = 180\n",
    "    session_timeout = 960\n",
    "    jupyter_timeout = 6\n",
    "    sandbox_timeout = 3\n",
    "\n",
    "    stream_interval = 200\n",
    "    context_tokens = 65536\n",
    "    buffer_tokens = 512\n",
    "    search_tokens = 32\n",
    "    top_logprobs = 5\n",
    "    batch_size = 256\n",
    "    early_stop = 4\n",
    "    attempts = 8\n",
    "    workers = 16\n",
    "    turns = 128\n",
    "    seed = 42\n",
    "\n",
    "    # Tool budget controls.\n",
    "    max_tool_calls_per_attempt = _env_int('AIMO_MAX_TOOL_CALLS_PER_ATTEMPT', 6)\n",
    "    max_tool_wall_time_per_attempt = _env_int('AIMO_MAX_TOOL_WALL_TIME_PER_ATTEMPT', 40)\n",
    "    max_total_tool_time_per_attempt = _env_int('AIMO_MAX_TOTAL_TOOL_TIME_PER_ATTEMPT', 75)\n",
    "    tool_failure_reset_threshold = _env_int('AIMO_TOOL_FAILURE_RESET_THRESHOLD', 3)\n",
    "\n",
    "    gpu_memory_utilization = 0.96\n",
    "    temperature = 1.0\n",
    "    min_p = 0.02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8aa1898",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T17:27:14.740336Z",
     "iopub.status.busy": "2026-01-12T17:27:14.740161Z",
     "iopub.status.idle": "2026-01-12T17:27:14.744255Z",
     "shell.execute_reply": "2026-01-12T17:27:14.743848Z"
    },
    "papermill": {
     "duration": 0.010678,
     "end_time": "2026-01-12T17:27:14.745120",
     "exception": false,
     "start_time": "2026-01-12T17:27:14.734442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if callable(set_seed):\n",
    "    set_seed(CFG.seed)\n",
    "else:\n",
    "    print('transformers.set_seed unavailable; continuing without explicit global seed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a48e14f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T17:27:14.754596Z",
     "iopub.status.busy": "2026-01-12T17:27:14.754427Z",
     "iopub.status.idle": "2026-01-12T17:27:14.757824Z",
     "shell.execute_reply": "2026-01-12T17:27:14.757441Z"
    },
    "papermill": {
     "duration": 0.009149,
     "end_time": "2026-01-12T17:27:14.758651",
     "exception": false,
     "start_time": "2026-01-12T17:27:14.749502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AIMO3Template:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        pass\n",
    "\n",
    "    def get_system_content(self, system_prompt: str, tool_config: ToolNamespaceConfig) -> SystemContent:\n",
    "\n",
    "        return (\n",
    "            SystemContent.new()\n",
    "            .with_model_identity(system_prompt)\n",
    "            .with_reasoning_effort(reasoning_effort=ReasoningEffort.HIGH)\n",
    "            .with_tools(tool_config)\n",
    "        )\n",
    "\n",
    "    def apply_chat_template(\n",
    "        self, \n",
    "        system_prompt: str, \n",
    "        user_prompt: str, \n",
    "        tool_config: ToolNamespaceConfig\n",
    "    ) -> list[Message]:\n",
    "\n",
    "        system_content = self.get_system_content(system_prompt, tool_config)        \n",
    "        system_message = Message.from_role_and_content(Role.SYSTEM, system_content)\n",
    "\n",
    "        user_message = Message.from_role_and_content(Role.USER, user_prompt)\n",
    "\n",
    "        return [system_message, user_message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8417c80f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T17:27:14.769150Z",
     "iopub.status.busy": "2026-01-12T17:27:14.768759Z",
     "iopub.status.idle": "2026-01-12T17:27:14.779004Z",
     "shell.execute_reply": "2026-01-12T17:27:14.778611Z"
    },
    "papermill": {
     "duration": 0.016525,
     "end_time": "2026-01-12T17:27:14.779818",
     "exception": false,
     "start_time": "2026-01-12T17:27:14.763293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AIMO3Sandbox:\n",
    "\n",
    "    _port_lock = threading.Lock()\n",
    "    _next_port = 50000\n",
    "\n",
    "    @classmethod\n",
    "    def _get_next_ports(cls, count: int = 5) -> list[int]:\n",
    "\n",
    "        with cls._port_lock:\n",
    "            ports = list(range(cls._next_port, cls._next_port + count))\n",
    "            cls._next_port += count\n",
    "\n",
    "            return ports\n",
    "\n",
    "    def __init__(self, timeout: float):\n",
    "\n",
    "        self._default_timeout = timeout\n",
    "        self._owns_kernel = False\n",
    "        self._client = None\n",
    "        self._km = None\n",
    "        \n",
    "        ports = self._get_next_ports(5)\n",
    "\n",
    "        env = os.environ.copy()\n",
    "        env['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
    "        env['PYDEVD_WARN_EVALUATION_TIMEOUT'] = '0'\n",
    "        env['JUPYTER_PLATFORM_DIRS'] = '1'\n",
    "        env['PYTHONWARNINGS'] = 'ignore'\n",
    "        env['MPLBACKEND'] = 'Agg'\n",
    "\n",
    "        self._km = KernelManager()\n",
    "        self._km.shell_port = ports[0]\n",
    "        self._km.iopub_port = ports[1]\n",
    "        self._km.stdin_port = ports[2]\n",
    "        self._km.hb_port = ports[3]\n",
    "        self._km.control_port = ports[4]\n",
    "\n",
    "        self._km.start_kernel(env=env, extra_arguments=['--Application.log_level=CRITICAL'])\n",
    "\n",
    "        self._client = self._km.blocking_client()\n",
    "        self._client.start_channels()\n",
    "        self._client.wait_for_ready(timeout=self._default_timeout)\n",
    "        self._owns_kernel = True\n",
    "\n",
    "        self.execute(\n",
    "            'import math\\n'\n",
    "            'import numpy\\n'\n",
    "            'import sympy\\n'\n",
    "            'import itertools\\n'\n",
    "            'import collections\\n'\n",
    "            'import mpmath\\n'\n",
    "            'mpmath.mp.dps = 64\\n'\n",
    "        )\n",
    "\n",
    "    def _format_error(self, traceback: list[str]) -> str:\n",
    "\n",
    "        clean_lines = []\n",
    "\n",
    "        for frame in traceback:\n",
    "            clean_frame = re.sub(r'\\x1b\\[[0-9;]*m', '', frame)\n",
    "\n",
    "            if 'File \"' in clean_frame and 'ipython-input' not in clean_frame:\n",
    "                continue\n",
    "\n",
    "            clean_lines.append(clean_frame)\n",
    "\n",
    "        return ''.join(clean_lines)\n",
    "\n",
    "    def execute(self, code: str, timeout: float | None = None) -> str:\n",
    "\n",
    "        client = self._client\n",
    "        effective_timeout = timeout or self._default_timeout\n",
    "        \n",
    "        msg_id = client.execute(\n",
    "            code, \n",
    "            store_history=True, \n",
    "            allow_stdin=False, \n",
    "            stop_on_error=False\n",
    "        )\n",
    "\n",
    "        stdout_parts = []\n",
    "        stderr_parts = []\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            if elapsed > effective_timeout:\n",
    "                self._km.interrupt_kernel()\n",
    "\n",
    "                return f'[ERROR] Execution timed out after {effective_timeout} seconds'\n",
    "\n",
    "            try:\n",
    "                msg = client.get_iopub_msg(timeout=1.0)\n",
    "\n",
    "            except queue.Empty:\n",
    "                continue\n",
    "\n",
    "            if msg.get('parent_header', {}).get('msg_id') != msg_id:\n",
    "                continue\n",
    "\n",
    "            msg_type = msg.get('msg_type')\n",
    "            content = msg.get('content', {})\n",
    "\n",
    "            if msg_type == 'stream':\n",
    "                text = content.get('text', '')\n",
    "\n",
    "                if content.get('name') == 'stdout':\n",
    "                    stdout_parts.append(text)\n",
    "\n",
    "                else:\n",
    "                    stderr_parts.append(text)\n",
    "\n",
    "            elif msg_type == 'error':\n",
    "                traceback_list = content.get('traceback', [])\n",
    "\n",
    "                stderr_parts.append(self._format_error(traceback_list))\n",
    "\n",
    "            elif msg_type in {'execute_result', 'display_data'}:\n",
    "                data = content.get('data', {})\n",
    "                text = data.get('text/plain')\n",
    "\n",
    "                if text:\n",
    "                    stdout_parts.append(text if text.endswith('\\n') else f'{text}\\n')\n",
    "\n",
    "            elif msg_type == 'status':\n",
    "                if content.get('execution_state') == 'idle':\n",
    "                    break\n",
    "\n",
    "        stdout = ''.join(stdout_parts)\n",
    "        stderr = ''.join(stderr_parts)\n",
    "\n",
    "        if stderr:\n",
    "            return f'{stdout.rstrip()}\\n{stderr}' if stdout else stderr\n",
    "\n",
    "        return stdout if stdout.strip() else '[WARN] No output. Use print() to see results.'\n",
    "\n",
    "    def close(self):\n",
    "\n",
    "        with contextlib.suppress(Exception):\n",
    "            if self._client:\n",
    "                self._client.stop_channels()\n",
    "\n",
    "        if self._owns_kernel and self._km is not None:\n",
    "            with contextlib.suppress(Exception):\n",
    "                self._km.shutdown_kernel(now=True)\n",
    "\n",
    "            with contextlib.suppress(Exception):\n",
    "                self._km.cleanup_resources()\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        self.execute(\n",
    "            '%reset -f\\n'\n",
    "            'import math\\n'\n",
    "            'import numpy\\n'\n",
    "            'import sympy\\n'\n",
    "            'import itertools\\n'\n",
    "            'import collections\\n'\n",
    "            'import mpmath\\n'\n",
    "            'mpmath.mp.dps = 64\\n'\n",
    "        )\n",
    "\n",
    "    def __del__(self):\n",
    "\n",
    "        self.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a190e706",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T17:27:14.789263Z",
     "iopub.status.busy": "2026-01-12T17:27:14.789125Z",
     "iopub.status.idle": "2026-01-12T17:27:14.794665Z",
     "shell.execute_reply": "2026-01-12T17:27:14.794285Z"
    },
    "papermill": {
     "duration": 0.010858,
     "end_time": "2026-01-12T17:27:14.795379",
     "exception": false,
     "start_time": "2026-01-12T17:27:14.784521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AIMO3Tool:\n",
    "\n",
    "    def __init__(self, local_jupyter_timeout: float, tool_prompt: str, sandbox=None):\n",
    "\n",
    "        self._local_jupyter_timeout = local_jupyter_timeout\n",
    "        self._tool_prompt = tool_prompt\n",
    "        self._jupyter_session = sandbox\n",
    "\n",
    "        self._owns_session = sandbox is None\n",
    "\n",
    "        self._execution_lock = threading.Lock()\n",
    "        self._init_lock = threading.Lock()\n",
    "\n",
    "    def _ensure_session(self):\n",
    "\n",
    "        if self._jupyter_session is None:\n",
    "            with self._init_lock:\n",
    "                if self._jupyter_session is None:\n",
    "                    self._jupyter_session = AIMO3Sandbox(timeout=self._local_jupyter_timeout)\n",
    "\n",
    "    def _ensure_last_print(self, code: str) -> str:\n",
    "\n",
    "        lines = code.strip().split('\\n')\n",
    "\n",
    "        if not lines:\n",
    "            return code\n",
    "\n",
    "        last_line = lines[-1].strip()\n",
    "\n",
    "        if 'print' in last_line or 'import' in last_line:\n",
    "            return code\n",
    "\n",
    "        if not last_line:\n",
    "            return code\n",
    "\n",
    "        if last_line.startswith('#'):\n",
    "            return code\n",
    "\n",
    "        lines[-1] = 'print(' + last_line + ')'\n",
    "\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "    def _looks_like_compute(self, code: str) -> bool:\n",
    "\n",
    "        text = code.strip().lower()\n",
    "        if not text:\n",
    "            return False\n",
    "\n",
    "        if len(text) < 8:\n",
    "            return False\n",
    "\n",
    "        compute_markers = [\n",
    "            'for ', 'while ', 'sympy', 'numpy', 'math.', 'solve', 'factor', 'matrix',\n",
    "            '+', '-', '*', '/', '%', '**', '=', 'gcd', 'lcm', 'pow(', 'mod', 'prime',\n",
    "        ]\n",
    "\n",
    "        return any(marker in text for marker in compute_markers)\n",
    "\n",
    "    @property\n",
    "    def instruction(self) -> str:\n",
    "\n",
    "        return self._tool_prompt\n",
    "\n",
    "    @property\n",
    "    def tool_config(self) -> ToolNamespaceConfig:\n",
    "\n",
    "        return ToolNamespaceConfig(\n",
    "            name='python',\n",
    "            description=self.instruction,\n",
    "            tools=[]\n",
    "        )\n",
    "\n",
    "    def _make_response(self, output: str, channel: str | None = None) -> Message:\n",
    "\n",
    "        content = TextContent(text=output)\n",
    "        author = Author(role=Role.TOOL, name='python')\n",
    "        message = Message(author=author, content=[content]).with_recipient('assistant')\n",
    "\n",
    "        if channel:\n",
    "            message = message.with_channel(channel)\n",
    "\n",
    "        return message\n",
    "\n",
    "    def execute_script(self, raw_script: str) -> tuple[str, bool, float]:\n",
    "\n",
    "        self._ensure_session()\n",
    "        final_script = self._ensure_last_print(raw_script)\n",
    "\n",
    "        if not self._looks_like_compute(final_script):\n",
    "            return '[SKIP] Non-compute or redundant python request skipped.', False, 0.0\n",
    "\n",
    "        started = time.time()\n",
    "\n",
    "        with self._execution_lock:\n",
    "            try:\n",
    "                output = self._jupyter_session.execute(final_script)\n",
    "            except TimeoutError as exc:\n",
    "                output = f'[ERROR] {exc}'\n",
    "\n",
    "        elapsed = time.time() - started\n",
    "        return output, True, elapsed\n",
    "\n",
    "    def process_sync_plus(self, message: Message) -> list[Message]:\n",
    "\n",
    "        output, _, _ = self.execute_script(message.content[0].text)\n",
    "        return [self._make_response(output, channel=message.channel)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f38fd67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T17:27:14.804967Z",
     "iopub.status.busy": "2026-01-12T17:27:14.804834Z",
     "iopub.status.idle": "2026-01-12T17:27:14.829283Z",
     "shell.execute_reply": "2026-01-12T17:27:14.828927Z"
    },
    "papermill": {
     "duration": 0.030513,
     "end_time": "2026-01-12T17:27:14.830218",
     "exception": false,
     "start_time": "2026-01-12T17:27:14.799705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AIMO3Solver:\n",
    "\n",
    "    def __init__(self, cfg, port: int = 8000):\n",
    "\n",
    "        # Fail fast with actionable diagnostics if critical runtime imports are unavailable.\n",
    "        if load_harmony_encoding is None or HarmonyEncodingName is None:\n",
    "            raise RuntimeError(\n",
    "                'openai_harmony is unavailable; dependency bootstrap failed or mount is missing.'\n",
    "            )\n",
    "        if OpenAI is None:\n",
    "            raise RuntimeError('openai client is unavailable; dependency bootstrap failed.')\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.port = port\n",
    "        self.base_url = f'http://0.0.0.0:{port}/v1'\n",
    "        self.api_key = 'sk-local'\n",
    "        self.template = AIMO3Template()\n",
    "        self.encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n",
    "        self.stop_token_ids = self.encoding.stop_tokens_for_assistant_actions()\n",
    "\n",
    "        self._preload_model_weights()\n",
    "        self.server_process = self._start_server()\n",
    "\n",
    "        self.client = OpenAI(\n",
    "            base_url=self.base_url,\n",
    "            api_key=self.api_key,\n",
    "            timeout=self.cfg.session_timeout,\n",
    "        )\n",
    "\n",
    "        self._wait_for_server()\n",
    "        self._initialize_kernels()\n",
    "\n",
    "        self.notebook_start_time = time.time()\n",
    "        self.problems_remaining = 50\n",
    "\n",
    "    def _preload_model_weights(self) -> None:\n",
    "\n",
    "        if not os.path.exists(self.cfg.model_path):\n",
    "            raise RuntimeError(f'Model path not found: {self.cfg.model_path}')\n",
    "\n",
    "        print(f'Loading model weights from {self.cfg.model_path} into OS Page Cache...')\n",
    "        start_time = time.time()\n",
    "\n",
    "        files_to_load = []\n",
    "        total_size = 0\n",
    "\n",
    "        for root, _, files in os.walk(self.cfg.model_path):\n",
    "            for file_name in files:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    files_to_load.append(file_path)\n",
    "                    total_size += os.path.getsize(file_path)\n",
    "\n",
    "        def _read_file(path: str) -> None:\n",
    "            with open(path, 'rb') as file_object:\n",
    "                while file_object.read(1024 * 1024 * 1024):\n",
    "                    pass\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=min(self.cfg.workers, 8)) as executor:\n",
    "            list(executor.map(_read_file, files_to_load))\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(\n",
    "            f'Processed {len(files_to_load)} files '\n",
    "            f'({total_size / 1e9:.2f} GB) in {elapsed:.2f} seconds.\\n'\n",
    "        )\n",
    "\n",
    "    def _start_server(self) -> subprocess.Popen:\n",
    "\n",
    "        cmd = [\n",
    "            sys.executable,\n",
    "            '-m',\n",
    "            'vllm.entrypoints.openai.api_server',\n",
    "            '--seed',\n",
    "            str(self.cfg.seed),\n",
    "            '--model',\n",
    "            self.cfg.model_path,\n",
    "            '--served-model-name',\n",
    "            self.cfg.served_model_name,\n",
    "            '--tensor-parallel-size',\n",
    "            '1',\n",
    "            '--max-num-seqs',\n",
    "            str(self.cfg.batch_size),\n",
    "            '--gpu-memory-utilization',\n",
    "            str(self.cfg.gpu_memory_utilization),\n",
    "            '--host',\n",
    "            '0.0.0.0',\n",
    "            '--port',\n",
    "            str(self.port),\n",
    "            '--dtype',\n",
    "            self.cfg.dtype,\n",
    "            '--kv-cache-dtype',\n",
    "            self.cfg.kv_cache_dtype,\n",
    "            '--max-model-len',\n",
    "            str(self.cfg.context_tokens),\n",
    "            '--stream-interval',\n",
    "            str(self.cfg.stream_interval),\n",
    "            '--async-scheduling',\n",
    "            '--disable-log-stats',\n",
    "            '--enable-prefix-caching',\n",
    "        ]\n",
    "\n",
    "        self.log_file = open('vllm_server.log', 'w')\n",
    "        print('Starting vLLM server with model:', self.cfg.model_path)\n",
    "\n",
    "        return subprocess.Popen(\n",
    "            cmd,\n",
    "            stdout=self.log_file,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            start_new_session=True,\n",
    "        )\n",
    "\n",
    "    def _wait_for_server(self):\n",
    "\n",
    "        print('Waiting for vLLM server...')\n",
    "        start_time = time.time()\n",
    "\n",
    "        for _ in range(self.cfg.server_timeout):\n",
    "            return_code = self.server_process.poll()\n",
    "\n",
    "            if return_code is not None:\n",
    "                self.log_file.flush()\n",
    "\n",
    "                with open('vllm_server.log', 'r') as log_file:\n",
    "                    logs = log_file.read()\n",
    "\n",
    "                diagnostics = (\n",
    "                    f'model_path={self.cfg.model_path} '\n",
    "                    f'cwd={os.getcwd()} '\n",
    "                    f'cuda_visible={os.getenv(\"CUDA_VISIBLE_DEVICES\", \"\")}'\n",
    "                )\n",
    "                raise RuntimeError(\n",
    "                    f'Server died with code {return_code}. Diagnostics: {diagnostics}. '\n",
    "                    f'Full logs:\\n{logs}\\n'\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                self.client.models.list()\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f'Server is ready (took {elapsed:.2f} seconds).\\n')\n",
    "                return\n",
    "            except Exception:\n",
    "                time.sleep(1)\n",
    "\n",
    "        raise RuntimeError('Server failed to start (timeout).\\n')\n",
    "\n",
    "    def _initialize_kernels(self) -> None:\n",
    "\n",
    "        print(f'Initializing {self.cfg.workers} persistent Jupyter kernels...')\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.sandbox_pool = queue.Queue()\n",
    "\n",
    "        def _create_sandbox():\n",
    "            return AIMO3Sandbox(timeout=self.cfg.jupyter_timeout)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:\n",
    "            futures = [executor.submit(_create_sandbox) for _ in range(self.cfg.workers)]\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                self.sandbox_pool.put(future.result())\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f'Kernels initialized in {elapsed:.2f} seconds.\\n')\n",
    "\n",
    "    def _classify_problem(self, problem: str) -> str:\n",
    "\n",
    "        text = problem.lower()\n",
    "\n",
    "        if any(k in text for k in ['mod', 'modulo', 'gcd', 'lcm', 'prime', 'remainder', 'crt', 'totient']):\n",
    "            return 'number_theory'\n",
    "        if any(k in text for k in ['triangle', 'circle', 'area', 'coordinate', 'distance', 'angle']):\n",
    "            return 'geometry'\n",
    "        if any(k in text for k in ['count', 'ways', 'combin', 'surjective', 'spanning trees', 'tuples']):\n",
    "            return 'combinatorics'\n",
    "        if any(k in text for k in ['polynomial', 'sequence', 'recurrence', 'matrix', 'coefficient', 'equation']):\n",
    "            return 'algebra'\n",
    "\n",
    "        return 'misc'\n",
    "\n",
    "    def _build_system_prompt(self, problem_type: str) -> str:\n",
    "\n",
    "        addendum = self.cfg.problem_type_prompts.get(problem_type, self.cfg.problem_type_prompts['misc'])\n",
    "        return f'{self.cfg.system_prompt}\\n\\nSpecialized guidance ({problem_type}): {addendum}'\n",
    "\n",
    "    def _runtime_parallel_plan(self, time_left: float) -> tuple[int, int]:\n",
    "\n",
    "        # Balanced runtime policy: spend more parallel compute only while global budget is healthy.\n",
    "        if time_left >= 7_200:\n",
    "            target_attempts, target_workers = 8, 16\n",
    "        elif time_left >= 2_400:\n",
    "            target_attempts, target_workers = 6, 12\n",
    "        else:\n",
    "            target_attempts, target_workers = 4, 8\n",
    "\n",
    "        attempts = max(1, min(int(self.cfg.attempts), int(target_attempts)))\n",
    "        workers = max(1, min(int(self.cfg.workers), int(target_workers)))\n",
    "        return attempts, workers\n",
    "\n",
    "    def _has_compute_intent(self, code: str) -> bool:\n",
    "\n",
    "        if not code or not code.strip():\n",
    "            return False\n",
    "\n",
    "        compact = code.lower()\n",
    "        signals = [\n",
    "            'print(',\n",
    "            'for ',\n",
    "            'while ',\n",
    "            'return ',\n",
    "            'sympy',\n",
    "            'numpy',\n",
    "            'math.',\n",
    "            'solve(',\n",
    "            'factor(',\n",
    "            'expand(',\n",
    "            'mod',\n",
    "            'gcd',\n",
    "            'lcm',\n",
    "            'import ',\n",
    "            'def ',\n",
    "            'lambda ',\n",
    "            '=',\n",
    "        ]\n",
    "        return any(token in compact for token in signals)\n",
    "\n",
    "    def _parse_modulus(self, problem: str) -> int | None:\n",
    "\n",
    "        patterns = [\n",
    "            r'mod(?:ulo)?\\s*(\\d+)',\n",
    "            r'remainder\\s+when[\\s\\S]{0,180}?divided\\s+by\\s*(\\d+)',\n",
    "        ]\n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, problem, flags=re.IGNORECASE)\n",
    "            if matches:\n",
    "                try:\n",
    "                    val = int(matches[-1])\n",
    "                    if 2 <= val <= 1_000_000:\n",
    "                        return val\n",
    "                except Exception:\n",
    "                    continue\n",
    "        return None\n",
    "\n",
    "    def _normalize_answer(self, value: int, modulus: int | None) -> int:\n",
    "\n",
    "        if modulus is not None:\n",
    "            return int(value) % int(modulus)\n",
    "\n",
    "        value = int(value)\n",
    "        if 0 <= value <= 99_999:\n",
    "            return value\n",
    "        return value % 100_000\n",
    "\n",
    "    def _scan_for_answer(self, text: str, problem: str, modulus: int | None) -> int | None:\n",
    "\n",
    "        patterns = [\n",
    "            r'\\\\boxed\\s*\\{\\s*([-+]?\\d[\\d,]*)\\s*\\}',\n",
    "            r'final\\s*_?answer\\s*[:=]\\s*([-+]?\\d[\\d,]*)',\n",
    "            r'final\\s+answer\\s+is\\s*([-+]?\\d[\\d,]*)',\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
    "            if matches:\n",
    "                try:\n",
    "                    clean_value = matches[-1].replace(',', '')\n",
    "                    return self._normalize_answer(int(clean_value), modulus)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        tail_ints = re.findall(r'(?<!\\d)([-+]?\\d{1,12})(?!\\d)', text[-700:])\n",
    "        if tail_ints:\n",
    "            try:\n",
    "                return self._normalize_answer(int(tail_ints[-1]), modulus)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _extract_problem_numbers(self, problem: str) -> set[int]:\n",
    "\n",
    "        out: set[int] = set()\n",
    "        for token in re.findall(r'(?<!\\d)(\\d{1,9})(?!\\d)', problem):\n",
    "            try:\n",
    "                out.add(int(token))\n",
    "            except Exception:\n",
    "                continue\n",
    "        return out\n",
    "\n",
    "    def _compute_mean_entropy(self, logprobs_buffer: list) -> float:\n",
    "\n",
    "        if not logprobs_buffer:\n",
    "            return float('inf')\n",
    "\n",
    "        total_entropy = 0.0\n",
    "        token_count = 0\n",
    "\n",
    "        for top_logprobs_dict in logprobs_buffer:\n",
    "\n",
    "            if not isinstance(top_logprobs_dict, dict):\n",
    "                continue\n",
    "\n",
    "            if not top_logprobs_dict:\n",
    "                continue\n",
    "\n",
    "            token_entropy = 0.0\n",
    "\n",
    "            for _, log_prob in top_logprobs_dict.items():\n",
    "                prob = math.exp(log_prob)\n",
    "\n",
    "                if prob > 0:\n",
    "                    token_entropy -= prob * math.log2(prob)\n",
    "\n",
    "            total_entropy += token_entropy\n",
    "            token_count += 1\n",
    "\n",
    "        if token_count == 0:\n",
    "            return float('inf')\n",
    "\n",
    "        return total_entropy / token_count\n",
    "\n",
    "    def _process_attempt(\n",
    "        self,\n",
    "        problem: str,\n",
    "        system_prompt: str,\n",
    "        attempt_index: int,\n",
    "        stop_event: threading.Event,\n",
    "        deadline: float,\n",
    "        shared_tool_cache: dict[str, str] | None = None,\n",
    "        cache_lock=None,\n",
    "    ) -> dict:\n",
    "\n",
    "        # One end-to-end reasoning trajectory:\n",
    "        # model generation + optional tool calls + answer extraction + telemetry.\n",
    "        if stop_event.is_set() or time.time() > deadline:\n",
    "            return {\n",
    "                'Attempt': attempt_index + 1,\n",
    "                'Answer': None,\n",
    "                'Python Calls': 0,\n",
    "                'Python Errors': 0,\n",
    "                'ToolTime': 0.0,\n",
    "                'Response Length': 0,\n",
    "                'Entropy': float('inf'),\n",
    "            }\n",
    "\n",
    "        local_tool = None\n",
    "        sandbox = None\n",
    "        python_calls = 0\n",
    "        python_errors = 0\n",
    "        tool_time = 0.0\n",
    "        total_tokens = 0\n",
    "        final_answer = None\n",
    "\n",
    "        logprobs_buffer = []\n",
    "        modulus = self._parse_modulus(problem)\n",
    "\n",
    "        attempt_seed = int(math.pow(self.cfg.seed + attempt_index, 2))\n",
    "\n",
    "        try:\n",
    "            sandbox = self.sandbox_pool.get(timeout=self.cfg.sandbox_timeout)\n",
    "\n",
    "            local_tool = AIMO3Tool(\n",
    "                local_jupyter_timeout=self.cfg.jupyter_timeout,\n",
    "                tool_prompt=self.cfg.tool_prompt,\n",
    "                sandbox=sandbox,\n",
    "            )\n",
    "\n",
    "            encoding = self.encoding\n",
    "            messages = self.template.apply_chat_template(\n",
    "                system_prompt,\n",
    "                problem,\n",
    "                local_tool.tool_config,\n",
    "            )\n",
    "\n",
    "            conversation = Conversation.from_messages(messages)\n",
    "\n",
    "            for _ in range(self.cfg.turns):\n",
    "                if stop_event.is_set() or time.time() > deadline:\n",
    "                    break\n",
    "\n",
    "                prompt_ids = encoding.render_conversation_for_completion(conversation, Role.ASSISTANT)\n",
    "                max_tokens = self.cfg.context_tokens - len(prompt_ids)\n",
    "\n",
    "                if max_tokens < self.cfg.buffer_tokens:\n",
    "                    break\n",
    "\n",
    "                stream = self.client.completions.create(\n",
    "                    model=self.cfg.served_model_name,\n",
    "                    temperature=self.cfg.temperature,\n",
    "                    logprobs=self.cfg.top_logprobs,\n",
    "                    max_tokens=max_tokens,\n",
    "                    prompt=prompt_ids,\n",
    "                    seed=attempt_seed,\n",
    "                    stream=True,\n",
    "                    extra_body={\n",
    "                        'min_p': self.cfg.min_p,\n",
    "                        'stop_token_ids': self.stop_token_ids,\n",
    "                        'return_token_ids': True,\n",
    "                    },\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    token_buffer = []\n",
    "                    text_chunks = []\n",
    "\n",
    "                    for chunk in stream:\n",
    "                        if stop_event.is_set() or time.time() > deadline:\n",
    "                            break\n",
    "\n",
    "                        new_tokens = chunk.choices[0].token_ids\n",
    "                        new_text = chunk.choices[0].text\n",
    "\n",
    "                        if new_tokens:\n",
    "                            token_buffer.extend(new_tokens)\n",
    "                            total_tokens += len(new_tokens)\n",
    "                            text_chunks.append(new_text)\n",
    "\n",
    "                            chunk_logprobs = chunk.choices[0].logprobs\n",
    "\n",
    "                            if chunk_logprobs is not None and chunk_logprobs.top_logprobs:\n",
    "                                logprobs_buffer.extend(chunk_logprobs.top_logprobs)\n",
    "\n",
    "                        if '}' in new_text or 'FINAL_ANSWER' in new_text:\n",
    "                            search_text = ''.join(text_chunks[-self.cfg.search_tokens:])\n",
    "                            answer = self._scan_for_answer(search_text, problem, modulus)\n",
    "\n",
    "                            if answer is not None:\n",
    "                                final_answer = answer\n",
    "                                break\n",
    "\n",
    "                finally:\n",
    "                    stream.close()\n",
    "\n",
    "                if final_answer is not None:\n",
    "                    break\n",
    "\n",
    "                if not token_buffer:\n",
    "                    break\n",
    "\n",
    "                new_messages = encoding.parse_messages_from_completion_tokens(token_buffer, Role.ASSISTANT)\n",
    "                conversation.messages.extend(new_messages)\n",
    "                last_message = new_messages[-1]\n",
    "\n",
    "                if last_message.channel == 'final':\n",
    "                    answer_text = last_message.content[0].text\n",
    "                    final_answer = self._scan_for_answer(answer_text, problem, modulus)\n",
    "                    break\n",
    "\n",
    "                if last_message.recipient == 'python':\n",
    "\n",
    "                    raw_script = last_message.content[0].text\n",
    "\n",
    "                    # Skip non-computational tool calls to preserve budget.\n",
    "                    if not self._has_compute_intent(raw_script):\n",
    "                        skip_msg = local_tool._make_response(\n",
    "                            '[SKIP] Non-computational tool call skipped by execution gate.',\n",
    "                            channel=last_message.channel,\n",
    "                        )\n",
    "                        conversation.messages.extend([skip_msg])\n",
    "                        continue\n",
    "\n",
    "                    # Enforce tool budgets to avoid long-running degenerate attempts.\n",
    "                    if python_calls >= self.cfg.max_tool_calls_per_attempt:\n",
    "                        skip_msg = local_tool._make_response(\n",
    "                            '[SKIP] Tool-call budget reached for this attempt.',\n",
    "                            channel=last_message.channel,\n",
    "                        )\n",
    "                        conversation.messages.extend([skip_msg])\n",
    "                        continue\n",
    "\n",
    "                    if tool_time >= float(self.cfg.max_total_tool_time_per_attempt):\n",
    "                        skip_msg = local_tool._make_response(\n",
    "                            '[SKIP] Tool wall-time budget reached for this attempt.',\n",
    "                            channel=last_message.channel,\n",
    "                        )\n",
    "                        conversation.messages.extend([skip_msg])\n",
    "                        continue\n",
    "\n",
    "                    cache_key = raw_script.strip()\n",
    "                    cached_output = None\n",
    "\n",
    "                    if shared_tool_cache is not None and cache_key:\n",
    "                        if cache_lock is not None:\n",
    "                            with cache_lock:\n",
    "                                cached_output = shared_tool_cache.get(cache_key)\n",
    "                        else:\n",
    "                            cached_output = shared_tool_cache.get(cache_key)\n",
    "\n",
    "                    if cached_output is not None:\n",
    "                        output = cached_output\n",
    "                        elapsed = 0.0\n",
    "                    else:\n",
    "                        # Execute tool call once, then cache text-identical code blocks.\n",
    "                        output, _, elapsed = local_tool.execute_script(raw_script)\n",
    "                        if shared_tool_cache is not None and cache_key and len(cache_key) < 6000:\n",
    "                            if cache_lock is not None:\n",
    "                                with cache_lock:\n",
    "                                    shared_tool_cache.setdefault(cache_key, output)\n",
    "                            else:\n",
    "                                shared_tool_cache.setdefault(cache_key, output)\n",
    "\n",
    "                    python_calls += 1\n",
    "                    tool_time += elapsed\n",
    "\n",
    "                    if (\n",
    "                        output.startswith('[ERROR]')\n",
    "                        or 'Traceback' in output\n",
    "                        or 'Error:' in output\n",
    "                    ):\n",
    "                        python_errors += 1\n",
    "\n",
    "                    if elapsed > float(self.cfg.max_tool_wall_time_per_attempt):\n",
    "                        output = (\n",
    "                            '[WARN] Tool call exceeded preferred wall-time budget '\n",
    "                            f'({elapsed:.1f}s).\\n{output}'\n",
    "                        )\n",
    "\n",
    "                    tool_responses = [local_tool._make_response(output, channel=last_message.channel)]\n",
    "                    conversation.messages.extend(tool_responses)\n",
    "\n",
    "        except Exception:\n",
    "            python_errors += 1\n",
    "\n",
    "        finally:\n",
    "            if sandbox is not None:\n",
    "                if python_errors >= self.cfg.tool_failure_reset_threshold:\n",
    "                    try:\n",
    "                        sandbox.close()\n",
    "                        sandbox = AIMO3Sandbox(timeout=self.cfg.jupyter_timeout)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "                sandbox.reset()\n",
    "                self.sandbox_pool.put(sandbox)\n",
    "\n",
    "        mean_entropy = self._compute_mean_entropy(logprobs_buffer)\n",
    "\n",
    "        return {\n",
    "            'Attempt': attempt_index + 1,\n",
    "            'Response Length': total_tokens,\n",
    "            'Python Calls': python_calls,\n",
    "            'Python Errors': python_errors,\n",
    "            'ToolTime': tool_time,\n",
    "            'Entropy': mean_entropy,\n",
    "            'Answer': final_answer,\n",
    "        }\n",
    "\n",
    "    def _score_answers(\n",
    "        self,\n",
    "        detailed_results: list,\n",
    "        verification_scores: dict[int, int],\n",
    "        problem_numbers: set[int],\n",
    "    ) -> tuple[list[dict[str, float]], float]:\n",
    "\n",
    "        # Aggregate candidates with a hybrid score:\n",
    "        # vote count + entropy weight + verification evidence + tool consistency.\n",
    "        candidates: dict[int, dict[str, float]] = {}\n",
    "\n",
    "        for result in detailed_results:\n",
    "            answer = result.get('Answer')\n",
    "            if answer is None:\n",
    "                continue\n",
    "\n",
    "            entropy = float(result.get('Entropy', float('inf')))\n",
    "            tool_calls = float(result.get('Python Calls', 0.0))\n",
    "            tool_errors = float(result.get('Python Errors', 0.0))\n",
    "            entropy_weight = 1.0 / max(entropy, 1e-9)\n",
    "            tool_consistency = max(0.0, 1.0 - (tool_errors / max(1.0, tool_calls + 1.0)))\n",
    "\n",
    "            item = candidates.setdefault(\n",
    "                int(answer),\n",
    "                {\n",
    "                    'answer': int(answer),\n",
    "                    'votes': 0.0,\n",
    "                    'entropy_weight': 0.0,\n",
    "                    'tool_consistency': 0.0,\n",
    "                    'verification': 0.0,\n",
    "                    'score': 0.0,\n",
    "                },\n",
    "            )\n",
    "\n",
    "            item['votes'] += 1.0\n",
    "            item['entropy_weight'] += entropy_weight\n",
    "            item['tool_consistency'] += tool_consistency\n",
    "\n",
    "        scored: list[dict[str, float]] = []\n",
    "\n",
    "        for answer, item in candidates.items():\n",
    "            verify = float(verification_scores.get(answer, 0))\n",
    "            avg_tool_consistency = item['tool_consistency'] / max(1.0, item['votes'])\n",
    "\n",
    "            score = (\n",
    "                0.8 * item['votes']\n",
    "                + item['entropy_weight']\n",
    "                + 1.2 * verify\n",
    "                + 0.4 * avg_tool_consistency\n",
    "            )\n",
    "\n",
    "            # Anti-degenerate heuristics.\n",
    "            if answer in {0, 1} and item['votes'] < max(3.0, float(self.cfg.early_stop)):\n",
    "                score -= 1.25\n",
    "\n",
    "            if answer in problem_numbers and item['votes'] < max(3.0, float(self.cfg.early_stop)):\n",
    "                score -= 0.75\n",
    "\n",
    "            item['verification'] = verify\n",
    "            item['score'] = score\n",
    "            item['tool_consistency'] = avg_tool_consistency\n",
    "            scored.append(item)\n",
    "\n",
    "        scored.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "        vote_margin = 0.0\n",
    "        if len(scored) >= 2:\n",
    "            vote_margin = float(scored[0]['score'] - scored[1]['score'])\n",
    "        elif len(scored) == 1:\n",
    "            vote_margin = float(scored[0]['score'])\n",
    "\n",
    "        return scored, vote_margin\n",
    "\n",
    "    def _run_verification_stage(\n",
    "        self,\n",
    "        problem: str,\n",
    "        problem_type: str,\n",
    "        top_candidates: list[int],\n",
    "        deadline: float,\n",
    "        shared_tool_cache: dict[str, str],\n",
    "        cache_lock: threading.Lock,\n",
    "    ) -> dict[int, int]:\n",
    "\n",
    "        verification: dict[int, int] = {}\n",
    "\n",
    "        # Stage-B verifier: explicitly challenge top candidates and reward survivors.\n",
    "        for idx, candidate in enumerate(top_candidates[: self.cfg.verification_attempts]):\n",
    "            if time.time() > deadline - 35:\n",
    "                break\n",
    "\n",
    "            verify_prompt = (\n",
    "                self._build_system_prompt(problem_type)\n",
    "                + '\\n\\nVerification stage: test candidate answer rigorously; '\n",
    "                + 'if wrong, provide corrected FINAL_ANSWER.'\n",
    "            )\n",
    "            verify_problem = (\n",
    "                f'{problem}\\n\\nCandidate answer to verify: {candidate}. '\n",
    "                'Use Python if useful, then output FINAL_ANSWER: <integer>.'\n",
    "            )\n",
    "\n",
    "            stop_event = threading.Event()\n",
    "            result = self._process_attempt(\n",
    "                verify_problem,\n",
    "                verify_prompt,\n",
    "                10_000 + idx,\n",
    "                stop_event,\n",
    "                deadline,\n",
    "                shared_tool_cache=shared_tool_cache,\n",
    "                cache_lock=cache_lock,\n",
    "            )\n",
    "\n",
    "            answer = result.get('Answer')\n",
    "            if answer is None:\n",
    "                continue\n",
    "\n",
    "            verification[int(answer)] = verification.get(int(answer), 0) + 1\n",
    "\n",
    "        return verification\n",
    "\n",
    "    def solve_problem(self, problem_id: str, problem: str) -> dict[str, object]:\n",
    "\n",
    "        # Two-stage solve loop:\n",
    "        # Stage-A candidate generation + Stage-B verification/refutation.\n",
    "        started = time.time()\n",
    "        print(f'\\nProblem[{problem_id}]: {problem}\\n')\n",
    "\n",
    "        problem_type = self._classify_problem(problem)\n",
    "        user_input = f'{problem} {self.cfg.preference_prompt}'\n",
    "\n",
    "        elapsed_global = time.time() - self.notebook_start_time\n",
    "        time_left = max(0.0, self.cfg.notebook_limit - elapsed_global)\n",
    "        problems_left_others = max(0, self.problems_remaining - 1)\n",
    "\n",
    "        reserve_for_others = (\n",
    "            problems_left_others * self.cfg.base_problem_timeout\n",
    "            + self.cfg.finalization_reserve_sec\n",
    "        )\n",
    "\n",
    "        available = max(float(self.cfg.base_problem_timeout), time_left - reserve_for_others)\n",
    "        difficulty_multiplier = {\n",
    "            'number_theory': 1.12,\n",
    "            'algebra': 1.0,\n",
    "            'geometry': 0.95,\n",
    "            'combinatorics': 1.08,\n",
    "            'misc': 1.0,\n",
    "        }.get(problem_type, 1.0)\n",
    "\n",
    "        budget = available * difficulty_multiplier\n",
    "        budget = min(float(self.cfg.high_problem_timeout), budget)\n",
    "        budget = max(float(self.cfg.base_problem_timeout), budget)\n",
    "\n",
    "        deadline = time.time() + budget\n",
    "        attempts_planned, workers_planned = self._runtime_parallel_plan(time_left)\n",
    "\n",
    "        print(\n",
    "            f'Budget: {budget:.2f}s | Time left global: {time_left:.2f}s '\n",
    "            f'| Type: {problem_type} | attempts={attempts_planned} workers={workers_planned}\\n'\n",
    "        )\n",
    "\n",
    "        system_prompt = self._build_system_prompt(problem_type)\n",
    "\n",
    "        detailed_results = []\n",
    "        valid_answers = []\n",
    "        stop_event = threading.Event()\n",
    "\n",
    "        shared_tool_cache: dict[str, str] = {}\n",
    "        cache_lock = threading.Lock()\n",
    "\n",
    "        executor = ThreadPoolExecutor(max_workers=workers_planned)\n",
    "\n",
    "        try:\n",
    "            futures = []\n",
    "\n",
    "            for attempt_index in range(attempts_planned):\n",
    "                futures.append(\n",
    "                    executor.submit(\n",
    "                        self._process_attempt,\n",
    "                        user_input,\n",
    "                        system_prompt,\n",
    "                        attempt_index,\n",
    "                        stop_event,\n",
    "                        deadline,\n",
    "                        shared_tool_cache,\n",
    "                        cache_lock,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                if time.time() > deadline:\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    detailed_results.append(result)\n",
    "\n",
    "                    if result['Answer'] is not None:\n",
    "                        valid_answers.append(int(result['Answer']))\n",
    "\n",
    "                    counts = Counter(valid_answers).most_common(1)\n",
    "                    if counts and counts[0][1] >= self.cfg.early_stop:\n",
    "                        stop_event.set()\n",
    "                        for f in futures:\n",
    "                            f.cancel()\n",
    "                        break\n",
    "\n",
    "                except Exception as exc:\n",
    "                    print(f'Future failed: {exc}')\n",
    "\n",
    "        finally:\n",
    "            stop_event.set()\n",
    "            executor.shutdown(wait=True, cancel_futures=True)\n",
    "\n",
    "        problem_numbers = self._extract_problem_numbers(problem)\n",
    "\n",
    "        verification_scores: dict[int, int] = {}\n",
    "        scored_answers, vote_margin = self._score_answers(detailed_results, verification_scores, problem_numbers)\n",
    "\n",
    "        disagreement = vote_margin < 1.0\n",
    "        if disagreement and time.time() < deadline - 45:\n",
    "            # If consensus is weak, spend extra attempts before final verification.\n",
    "            extra_runs = max(0, int(self.cfg.disagreement_extra_attempts))\n",
    "            for extra_idx in range(extra_runs):\n",
    "                if time.time() > deadline - 35:\n",
    "                    break\n",
    "                result = self._process_attempt(\n",
    "                    user_input,\n",
    "                    system_prompt + '\\n\\nRe-check assumptions and verify candidate consistency.',\n",
    "                    attempts_planned + extra_idx,\n",
    "                    threading.Event(),\n",
    "                    deadline,\n",
    "                    shared_tool_cache,\n",
    "                    cache_lock,\n",
    "                )\n",
    "                detailed_results.append(result)\n",
    "                if result['Answer'] is not None:\n",
    "                    valid_answers.append(int(result['Answer']))\n",
    "\n",
    "            scored_answers, vote_margin = self._score_answers(\n",
    "                detailed_results,\n",
    "                verification_scores,\n",
    "                problem_numbers,\n",
    "            )\n",
    "\n",
    "        top_candidates = [int(row['answer']) for row in scored_answers[:2]]\n",
    "        if top_candidates and time.time() < deadline - 35:\n",
    "            verification_scores = self._run_verification_stage(\n",
    "                problem,\n",
    "                problem_type,\n",
    "                top_candidates,\n",
    "                deadline,\n",
    "                shared_tool_cache,\n",
    "                cache_lock,\n",
    "            )\n",
    "            scored_answers, vote_margin = self._score_answers(\n",
    "                detailed_results,\n",
    "                verification_scores,\n",
    "                problem_numbers,\n",
    "            )\n",
    "\n",
    "        if detailed_results:\n",
    "            results_dataframe = pd.DataFrame(detailed_results)\n",
    "            if 'Entropy' in results_dataframe.columns:\n",
    "                results_dataframe['Entropy'] = results_dataframe['Entropy'].astype(float).round(3)\n",
    "            if 'Answer' in results_dataframe.columns:\n",
    "                results_dataframe['Answer'] = results_dataframe['Answer'].astype('Int64')\n",
    "            display(results_dataframe)\n",
    "\n",
    "        if scored_answers:\n",
    "            score_df = pd.DataFrame(scored_answers)\n",
    "            score_df = score_df[[\n",
    "                'answer',\n",
    "                'votes',\n",
    "                'entropy_weight',\n",
    "                'verification',\n",
    "                'tool_consistency',\n",
    "                'score',\n",
    "            ]]\n",
    "            display(score_df.round(3))\n",
    "\n",
    "        watchdog_forced = bool(time.time() > deadline - 5)\n",
    "        if (not scored_answers) and valid_answers:\n",
    "            fallback_answer, fallback_votes = Counter(valid_answers).most_common(1)[0]\n",
    "            scored_answers = [\n",
    "                {\n",
    "                    'answer': int(fallback_answer),\n",
    "                    'votes': float(fallback_votes),\n",
    "                    'entropy_weight': 0.0,\n",
    "                    'verification': 0.0,\n",
    "                    'tool_consistency': 0.0,\n",
    "                    'score': float(fallback_votes),\n",
    "                }\n",
    "            ]\n",
    "            vote_margin = 0.0\n",
    "\n",
    "        selected_answer = int(scored_answers[0]['answer']) if scored_answers else 0\n",
    "        if scored_answers and watchdog_forced:\n",
    "            selected_source = f'watchdog_sc_tir_{problem_type}'\n",
    "        elif scored_answers:\n",
    "            selected_source = f'sc_tir_{problem_type}'\n",
    "        else:\n",
    "            selected_source = 'safe_zero_fallback'\n",
    "        consensus = int(scored_answers[0]['votes']) if scored_answers else 0\n",
    "\n",
    "        elapsed = time.time() - started\n",
    "        self.problems_remaining = max(0, self.problems_remaining - 1)\n",
    "\n",
    "        tool_calls_total = int(sum(float(r.get('Python Calls', 0.0)) for r in detailed_results))\n",
    "        tool_errors_total = int(sum(float(r.get('Python Errors', 0.0)) for r in detailed_results))\n",
    "        candidate_count = len({int(r['Answer']) for r in detailed_results if r.get('Answer') is not None})\n",
    "\n",
    "        print(\n",
    "            '[solver-summary] '\n",
    "            f'id={problem_id} '\n",
    "            f'budget_s={budget:.2f} '\n",
    "            f'elapsed_s={elapsed:.2f} '\n",
    "            f'attempts_used={len(detailed_results)} '\n",
    "            f'consensus={consensus} '\n",
    "            f'selected_source={selected_source} '\n",
    "            'model_status=active'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'answer': int(selected_answer),\n",
    "            'source': selected_source,\n",
    "            'model_status': 'active',\n",
    "            'tool_calls': tool_calls_total,\n",
    "            'tool_errors': tool_errors_total,\n",
    "            'candidate_count': int(candidate_count),\n",
    "            'vote_margin': float(vote_margin),\n",
    "            'budget_s': float(budget),\n",
    "            'elapsed_s': float(elapsed),\n",
    "            'attempts_used': int(len(detailed_results)),\n",
    "            'consensus': int(consensus),\n",
    "        }\n",
    "\n",
    "    def __del__(self):\n",
    "\n",
    "        if hasattr(self, 'server_process'):\n",
    "            self.server_process.terminate()\n",
    "            self.server_process.wait()\n",
    "\n",
    "        if hasattr(self, 'log_file'):\n",
    "            self.log_file.close()\n",
    "\n",
    "        if hasattr(self, 'sandbox_pool'):\n",
    "            while not self.sandbox_pool.empty():\n",
    "                try:\n",
    "                    sb = self.sandbox_pool.get_nowait()\n",
    "                    sb.close()\n",
    "                except Exception:\n",
    "                    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35a76143",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2026-01-12T17:27:14.840119Z",
     "iopub.status.busy": "2026-01-12T17:27:14.839737Z",
     "iopub.status.idle": "2026-01-12T17:31:03.907265Z",
     "shell.execute_reply": "2026-01-12T17:31:03.906835Z"
    },
    "papermill": {
     "duration": 229.073583,
     "end_time": "2026-01-12T17:31:03.908118",
     "exception": false,
     "start_time": "2026-01-12T17:27:14.834535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model weights from /kaggle/input/gpt-oss-120b/transformers/default/1 into OS Page Cache...\n",
      "Processed 26 files (65.28 GB) in 99.67 seconds.\n",
      "\n",
      "Waiting for vLLM server...\n",
      "Server is ready (took 126.11 seconds).\n",
      "\n",
      "Initializing 16 persistent Jupyter kernels...\n",
      "Kernels initialized in 2.95 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Runtime diagnostics and safe-mode controls.\n",
    "DEBUG_COLUMNS = [\n",
    "    'id',\n",
    "    'answer',\n",
    "    'source',\n",
    "    'model_status',\n",
    "    'time_left_s',\n",
    "    'tool_calls',\n",
    "    'tool_errors',\n",
    "    'candidate_count',\n",
    "    'vote_margin',\n",
    "]\n",
    "DEBUG_ROWS: list[dict[str, object]] = []\n",
    "RUNTIME_HEALTH: dict[str, object] = {}\n",
    "\n",
    "\n",
    "def _env_is_true(name: str) -> bool:\n",
    "\n",
    "    raw = os.getenv(name)\n",
    "    if raw is None:\n",
    "        return False\n",
    "\n",
    "    return str(raw).strip().lower() in {'1', 'true', 'yes', 'y', 'on'}\n",
    "\n",
    "\n",
    "def _env_float(name: str, default: float) -> float:\n",
    "\n",
    "    raw = os.getenv(name)\n",
    "    if raw is None:\n",
    "        return float(default)\n",
    "\n",
    "    try:\n",
    "        return float(raw)\n",
    "    except Exception:\n",
    "        return float(default)\n",
    "\n",
    "\n",
    "solver = None\n",
    "STARTUP_PREFLIGHT: dict[str, object] = {}\n",
    "SAFE_MODE_REASON = ''\n",
    "SAMPLE_IDS = {'000aaa', '111bbb', '222ccc'}\n",
    "IS_COMPETITION_RERUN = _env_is_true('KAGGLE_IS_COMPETITION_RERUN')\n",
    "\n",
    "RUN_SUMMARY = {\n",
    "    'rows': 0,\n",
    "    'small_answers': 0,\n",
    "    'tool_calls': 0,\n",
    "    'tool_errors': 0,\n",
    "    'sources': Counter(),\n",
    "}\n",
    "\n",
    "\n",
    "def _time_left_s() -> int:\n",
    "\n",
    "    if solver is None:\n",
    "        return int(CFG.notebook_limit)\n",
    "\n",
    "    try:\n",
    "        elapsed = time.time() - solver.notebook_start_time\n",
    "        return int(max(0.0, CFG.notebook_limit - elapsed))\n",
    "    except Exception:\n",
    "        return int(CFG.notebook_limit)\n",
    "\n",
    "\n",
    "def _check_mounts() -> tuple[bool, str]:\n",
    "\n",
    "    comp_root = '/kaggle/input/ai-mathematical-olympiad-progress-prize-3'\n",
    "    util_archive = '/kaggle/input/aimo-3-utils/wheels.tar.gz'\n",
    "\n",
    "    if not os.path.exists(comp_root):\n",
    "        return False, f'missing_competition_mount:{comp_root}'\n",
    "\n",
    "    # Utility wheels are optional when the selected backend is transformers-only.\n",
    "    if not os.path.exists(util_archive):\n",
    "        return True, f'utility_wheels_missing_optional:{util_archive}'\n",
    "\n",
    "    return True, 'ok'\n",
    "\n",
    "def _get_gpu_info() -> dict[str, object]:\n",
    "\n",
    "    info = {\n",
    "        'ok': False,\n",
    "        'major': 0,\n",
    "        'minor': 0,\n",
    "        'total_gb': 0.0,\n",
    "        'raw': '',\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        import torch\n",
    "\n",
    "        if not torch.cuda.is_available():\n",
    "            info['raw'] = 'cuda_unavailable'\n",
    "            return info\n",
    "\n",
    "        major, minor = torch.cuda.get_device_capability(0)\n",
    "        total_bytes = int(torch.cuda.get_device_properties(0).total_memory)\n",
    "        total_gb = float(total_bytes) / float(1024 ** 3)\n",
    "\n",
    "        info.update(\n",
    "            {\n",
    "                'ok': True,\n",
    "                'major': int(major),\n",
    "                'minor': int(minor),\n",
    "                'total_gb': round(float(total_gb), 3),\n",
    "                'raw': f'cuda_capability_{major}_{minor}_mem_{total_gb:.2f}gb',\n",
    "            }\n",
    "        )\n",
    "        return info\n",
    "\n",
    "    except Exception as exc:\n",
    "        info['raw'] = f'cuda_check_failed:{exc}'\n",
    "        return info\n",
    "\n",
    "\n",
    "def _estimate_model_size_rank(path: str) -> int:\n",
    "\n",
    "    lower = str(path).lower()\n",
    "\n",
    "    if any(token in lower for token in ['120b', '110b', '100b']):\n",
    "        return 5\n",
    "    if any(token in lower for token in ['72b', '70b']):\n",
    "        return 4\n",
    "    if any(token in lower for token in ['34b', '32b', '30b', '27b', '24b', '22b', '20b']):\n",
    "        return 3\n",
    "    if any(token in lower for token in ['14b', '13b', '12b', '11b', '10b', '9b', '8b', '7b']):\n",
    "        return 2\n",
    "    if any(token in lower for token in ['4b', '3b', '2b']):\n",
    "        return 1\n",
    "    if any(token in lower for token in ['1.5b', '1b']):\n",
    "        return 0\n",
    "\n",
    "    return 2\n",
    "\n",
    "\n",
    "def _scan_model_quantization_method(model_path: str) -> str:\n",
    "\n",
    "    config_path = os.path.join(model_path, 'config.json')\n",
    "    if not os.path.exists(config_path):\n",
    "        return ''\n",
    "\n",
    "    try:\n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        qcfg = config.get('quantization_config')\n",
    "        if isinstance(qcfg, dict):\n",
    "            for key in ['quant_method', 'quantization_method', 'method', 'name']:\n",
    "                value = qcfg.get(key)\n",
    "                if isinstance(value, str) and value.strip():\n",
    "                    return value.strip().lower()\n",
    "\n",
    "        raw = json.dumps(config).lower()\n",
    "        if 'mxfp4' in raw:\n",
    "            return 'mxfp4'\n",
    "\n",
    "    except Exception:\n",
    "        return ''\n",
    "\n",
    "    return ''\n",
    "\n",
    "\n",
    "def _collect_model_candidates() -> list[str]:\n",
    "\n",
    "    seen: set[str] = set()\n",
    "    ordered: list[str] = []\n",
    "\n",
    "    def _add(candidate: str) -> None:\n",
    "        if not candidate:\n",
    "            return\n",
    "        c = str(candidate)\n",
    "        if c in seen:\n",
    "            return\n",
    "        seen.add(c)\n",
    "        ordered.append(c)\n",
    "\n",
    "    for candidate in list(getattr(CFG, 'model_path_candidates', [])):\n",
    "        _add(candidate)\n",
    "\n",
    "    extra_raw = os.getenv('AIMO_EXTRA_MODEL_PATHS', '').strip()\n",
    "    if extra_raw:\n",
    "        for item in [p.strip() for p in extra_raw.split(',') if p.strip()]:\n",
    "            _add(item)\n",
    "\n",
    "    # Discover mounted models dynamically in Kaggle runtime.\n",
    "    search_roots = ['/kaggle/input/models', '/kaggle/input']\n",
    "    max_discovered = int(os.getenv('AIMO_MAX_DISCOVERED_MODEL_PATHS', '512'))\n",
    "\n",
    "    try:\n",
    "        import glob\n",
    "\n",
    "        discovered = 0\n",
    "        for root in search_roots:\n",
    "            if not os.path.exists(root):\n",
    "                continue\n",
    "\n",
    "            pattern = os.path.join(root, '**', 'config.json')\n",
    "            for cfg_path in glob.glob(pattern, recursive=True):\n",
    "                candidate = os.path.dirname(cfg_path)\n",
    "                if not os.path.isdir(candidate):\n",
    "                    continue\n",
    "\n",
    "                # Prefer canonical Kaggle model folders.\n",
    "                lower = candidate.lower()\n",
    "                if '/site-packages/' in lower or '/venv/' in lower:\n",
    "                    continue\n",
    "\n",
    "                _add(candidate)\n",
    "                discovered += 1\n",
    "                if discovered >= max_discovered:\n",
    "                    break\n",
    "            if discovered >= max_discovered:\n",
    "                break\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return ordered\n",
    "\n",
    "\n",
    "def _discover_model_path(gpu_info: dict[str, object] | None = None) -> tuple[str | None, str]:\n",
    "\n",
    "    info = gpu_info or {}\n",
    "    major = int(info.get('major', 0) or 0)\n",
    "    minor = int(info.get('minor', 0) or 0)\n",
    "    sm = (major * 10 + minor) if major > 0 else 0\n",
    "    disable_sm_lt = int(getattr(CFG, 'disable_gpt_oss_on_sm_lt', 80))\n",
    "    total_gb = float(info.get('total_gb', 0.0) or 0.0)\n",
    "    prefer_small_threshold = float(getattr(CFG, 'prefer_small_model_below_gb', 28))\n",
    "\n",
    "    primary_candidates = [\n",
    "        c for c in _collect_model_candidates()\n",
    "        if 'gpt-oss' in str(c).lower()\n",
    "    ]\n",
    "\n",
    "    if not primary_candidates:\n",
    "        return None, 'missing_primary_gpt_oss_path'\n",
    "\n",
    "    compatible: list[tuple[int, int, int, str]] = []\n",
    "    incompatible: list[str] = []\n",
    "\n",
    "    for candidate in primary_candidates:\n",
    "        if not os.path.exists(candidate):\n",
    "            continue\n",
    "\n",
    "        quant_method = _scan_model_quantization_method(candidate)\n",
    "        if sm and sm < disable_sm_lt and quant_method == 'mxfp4':\n",
    "            incompatible.append(f'{candidate}:quant={quant_method}:sm={sm}')\n",
    "            continue\n",
    "\n",
    "        size_rank = _estimate_model_size_rank(candidate)\n",
    "        mem_penalty = size_rank if (total_gb and total_gb < prefer_small_threshold) else 0\n",
    "        compatible.append((mem_penalty, size_rank, len(candidate), candidate))\n",
    "\n",
    "    if compatible:\n",
    "        compatible.sort()\n",
    "        selected = compatible[0][3]\n",
    "        reason = 'ok_primary'\n",
    "        if total_gb and total_gb < prefer_small_threshold:\n",
    "            reason = f'ok_primary_small_gpu_pref<{prefer_small_threshold}gb'\n",
    "        return selected, reason\n",
    "\n",
    "    if incompatible:\n",
    "        return None, f'primary_incompatible:{\"|\".join(incompatible[:4])}'\n",
    "\n",
    "    return None, 'missing_primary_gpt_oss_path'\n",
    "\n",
    "\n",
    "def _discover_fallback_model_path(gpu_info: dict[str, object] | None = None) -> tuple[str | None, str]:\n",
    "\n",
    "    info = gpu_info or {}\n",
    "    total_gb = float(info.get('total_gb', 0.0) or 0.0)\n",
    "    prefer_small_threshold = float(getattr(CFG, 'prefer_small_model_below_gb', 28))\n",
    "\n",
    "    all_candidates = [c for c in _collect_model_candidates() if os.path.exists(c)]\n",
    "    candidates = [c for c in all_candidates if 'gpt-oss' not in str(c).lower()]\n",
    "\n",
    "    if not candidates:\n",
    "        return None, 'missing_deepseek_or_alt_fallback_path'\n",
    "\n",
    "    scored: list[tuple[int, int, int, str]] = []\n",
    "\n",
    "    for candidate in candidates:\n",
    "        lower = str(candidate).lower()\n",
    "\n",
    "        provider_pref = 0\n",
    "        if 'deepseek' in lower:\n",
    "            provider_pref = -3\n",
    "        elif 'qwen' in lower or 'llama' in lower or 'mistral' in lower:\n",
    "            provider_pref = -1\n",
    "\n",
    "        size_rank = _estimate_model_size_rank(candidate)\n",
    "        mem_penalty = size_rank if (total_gb and total_gb < prefer_small_threshold) else 0\n",
    "        scored.append((provider_pref + mem_penalty, size_rank, len(candidate), candidate))\n",
    "\n",
    "    if not scored:\n",
    "        return None, 'missing_deepseek_or_alt_fallback_path'\n",
    "\n",
    "    scored.sort()\n",
    "    selected = scored[0][3]\n",
    "    reason = 'ok_fallback'\n",
    "    if total_gb and total_gb < prefer_small_threshold:\n",
    "        reason = f'ok_fallback_small_gpu_pref<{prefer_small_threshold}gb'\n",
    "    return selected, reason\n",
    "\n",
    "\n",
    "def _check_fallback_runtime_ready() -> tuple[bool, str]:\n",
    "\n",
    "    try:\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer  # noqa: F401\n",
    "    except Exception as exc:\n",
    "        return False, f'transformers_import_failure:{exc}'\n",
    "\n",
    "    return True, 'ok'\n",
    "\n",
    "\n",
    "def _check_gpu_capability(gpu_info: dict[str, object] | None = None) -> tuple[bool, str]:\n",
    "\n",
    "    info = gpu_info or _get_gpu_info()\n",
    "\n",
    "    if not bool(info.get('ok', False)):\n",
    "        return False, str(info.get('raw', 'cuda_unavailable'))\n",
    "\n",
    "    major = int(info.get('major', 0) or 0)\n",
    "    minor = int(info.get('minor', 0) or 0)\n",
    "    min_major = int(os.getenv('AIMO_MIN_REQUIRED_CUDA_MAJOR', '6'))\n",
    "    if major < min_major:\n",
    "        return False, f'weak_gpu_sm_{major}{minor}'\n",
    "\n",
    "    return True, str(info.get('raw', f'cuda_capability_{major}_{minor}'))\n",
    "\n",
    "\n",
    "def _check_tool_runtime_ready(*, require_harmony: bool = True) -> tuple[bool, str]:\n",
    "\n",
    "    if require_harmony:\n",
    "        if OPTIONAL_IMPORT_ERRORS:\n",
    "            details = ';'.join(f'{k}:{v}' for k, v in OPTIONAL_IMPORT_ERRORS.items())\n",
    "            return False, f'optional_import_failure:{details}'\n",
    "\n",
    "        required = [\n",
    "            ('openai_harmony', load_harmony_encoding),\n",
    "            ('openai', OpenAI),\n",
    "            ('harmony_types', Conversation),\n",
    "        ]\n",
    "        for name, obj in required:\n",
    "            if obj is None:\n",
    "                return False, f'missing_runtime_symbol:{name}'\n",
    "\n",
    "    return True, 'ok'\n",
    "\n",
    "\n",
    "def _apply_resource_profile(gpu_info: dict[str, object], selected_model_path: str) -> dict[str, object]:\n",
    "\n",
    "    major = int(gpu_info.get('major', 0) or 0)\n",
    "    total_gb = float(gpu_info.get('total_gb', 0.0) or 0.0)\n",
    "    path_lower = str(selected_model_path).lower()\n",
    "\n",
    "    profile = {\n",
    "        'profile': 'default',\n",
    "        'kv_cache_dtype': getattr(CFG, 'kv_cache_dtype', 'auto'),\n",
    "        'context_tokens': int(getattr(CFG, 'context_tokens', 8192)),\n",
    "        'batch_size': int(getattr(CFG, 'batch_size', 16)),\n",
    "        'workers': int(getattr(CFG, 'workers', 4)),\n",
    "        'attempts': int(getattr(CFG, 'attempts', 4)),\n",
    "        'gpu_memory_utilization': float(getattr(CFG, 'gpu_memory_utilization', 0.9)),\n",
    "    }\n",
    "\n",
    "    if major < 8:\n",
    "        CFG.kv_cache_dtype = 'auto'\n",
    "        profile['kv_cache_dtype'] = 'auto'\n",
    "\n",
    "    low_mem_threshold = float(getattr(CFG, 'prefer_small_model_below_gb', 28))\n",
    "    low_mem = (total_gb > 0.0 and total_gb < low_mem_threshold) or ('120b' in path_lower)\n",
    "\n",
    "    if low_mem:\n",
    "        CFG.context_tokens = min(int(getattr(CFG, 'context_tokens', 8192)), int(os.getenv('AIMO_LOW_MEM_CONTEXT_TOKENS', '8192')))\n",
    "        CFG.batch_size = min(int(getattr(CFG, 'batch_size', 16)), int(os.getenv('AIMO_LOW_MEM_BATCH_SIZE', '12')))\n",
    "        CFG.workers = min(int(getattr(CFG, 'workers', 4)), int(os.getenv('AIMO_LOW_MEM_WORKERS', '6')))\n",
    "        CFG.attempts = min(int(getattr(CFG, 'attempts', 4)), int(os.getenv('AIMO_LOW_MEM_ATTEMPTS', '6')))\n",
    "        CFG.gpu_memory_utilization = min(\n",
    "            float(getattr(CFG, 'gpu_memory_utilization', 0.9)),\n",
    "            _env_float('AIMO_LOW_MEM_GPU_UTIL', 0.88),\n",
    "        )\n",
    "        profile.update(\n",
    "            {\n",
    "                'profile': 'low_mem',\n",
    "                'context_tokens': int(CFG.context_tokens),\n",
    "                'batch_size': int(CFG.batch_size),\n",
    "                'workers': int(CFG.workers),\n",
    "                'attempts': int(CFG.attempts),\n",
    "                'gpu_memory_utilization': float(CFG.gpu_memory_utilization),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return profile\n",
    "\n",
    "\n",
    "def _run_startup_preflight() -> dict[str, object]:\n",
    "\n",
    "    checks = {}\n",
    "\n",
    "    ok_mounts, msg_mounts = _check_mounts()\n",
    "    checks['mounts'] = msg_mounts\n",
    "\n",
    "    gpu_info = _get_gpu_info()\n",
    "    ok_gpu, msg_gpu = _check_gpu_capability(gpu_info)\n",
    "    checks['gpu'] = msg_gpu\n",
    "\n",
    "    major = int(gpu_info.get('major', 0) or 0)\n",
    "    minor = int(gpu_info.get('minor', 0) or 0)\n",
    "    gpu_sm = int(major * 10 + minor) if major > 0 else 0\n",
    "\n",
    "    gpt_model_path, msg_model = _discover_model_path(gpu_info)\n",
    "    checks['model_path'] = gpt_model_path if gpt_model_path else msg_model\n",
    "\n",
    "    fallback_model_path, msg_fallback_model = _discover_fallback_model_path(gpu_info)\n",
    "    checks['fallback_model_path'] = fallback_model_path if fallback_model_path else msg_fallback_model\n",
    "\n",
    "    ok_tool, msg_tool = _check_tool_runtime_ready(require_harmony=True)\n",
    "    checks['tool_runtime'] = msg_tool\n",
    "\n",
    "    ok_fallback_runtime, msg_fallback_runtime = _check_fallback_runtime_ready()\n",
    "    checks['fallback_runtime'] = msg_fallback_runtime\n",
    "\n",
    "    bootstrap_diag = globals().get('BOOTSTRAP_DIAGNOSTICS', {})\n",
    "    checks['bootstrap'] = str(bootstrap_diag) if bootstrap_diag else 'unknown'\n",
    "\n",
    "    forced_family = str(getattr(CFG, 'force_model_family', 'auto')).strip().lower()\n",
    "    if forced_family not in {'auto', 'gpt_oss', 'deepseek'}:\n",
    "        forced_family = 'auto'\n",
    "\n",
    "    disable_sm_lt = int(getattr(CFG, 'disable_gpt_oss_on_sm_lt', 80))\n",
    "    gpt_blocked_reason = ''\n",
    "    incompatible_models_skipped: list[str] = []\n",
    "\n",
    "    if gpt_model_path is None and str(msg_model).startswith('primary_incompatible:'):\n",
    "        details = str(msg_model).split(':', 1)[-1]\n",
    "        incompatible_models_skipped = [x for x in details.split('|') if x]\n",
    "\n",
    "    gpt_allowed = bool(ok_mounts and ok_gpu and gpt_model_path and ok_tool)\n",
    "    if gpu_sm and gpu_sm < disable_sm_lt:\n",
    "        gpt_allowed = False\n",
    "        gpt_blocked_reason = f'gpu_sm_{gpu_sm}_lt_{disable_sm_lt}'\n",
    "\n",
    "    deepseek_allowed = bool(ok_mounts and ok_gpu and fallback_model_path and ok_fallback_runtime)\n",
    "\n",
    "    selected_model_family = ''\n",
    "    selected_model_path = ''\n",
    "    compatibility_reason = ''\n",
    "    primary_blocked_reason = ''\n",
    "    backend = 'none'\n",
    "\n",
    "    if forced_family == 'gpt_oss':\n",
    "        if gpt_allowed:\n",
    "            selected_model_family = 'gpt_oss'\n",
    "            selected_model_path = str(gpt_model_path)\n",
    "            compatibility_reason = 'forced_gpt_oss'\n",
    "            backend = 'vllm_gpt_oss'\n",
    "        else:\n",
    "            primary_blocked_reason = gpt_blocked_reason or str(msg_model or msg_tool or 'forced_gpt_oss_unavailable')\n",
    "    elif forced_family == 'deepseek':\n",
    "        if deepseek_allowed:\n",
    "            selected_model_family = 'deepseek'\n",
    "            selected_model_path = str(fallback_model_path)\n",
    "            compatibility_reason = 'forced_deepseek'\n",
    "            backend = 'deepseek_transformers'\n",
    "        else:\n",
    "            primary_blocked_reason = str(msg_fallback_model or msg_fallback_runtime or 'forced_deepseek_unavailable')\n",
    "    else:\n",
    "        if gpu_sm and gpu_sm < disable_sm_lt and deepseek_allowed:\n",
    "            selected_model_family = 'deepseek'\n",
    "            selected_model_path = str(fallback_model_path)\n",
    "            compatibility_reason = f'auto_gpu_sm_{gpu_sm}_lt_{disable_sm_lt}_use_deepseek'\n",
    "            backend = 'deepseek_transformers'\n",
    "            primary_blocked_reason = gpt_blocked_reason or 'gpt_oss_quantization_incompatible'\n",
    "        elif gpt_allowed:\n",
    "            selected_model_family = 'gpt_oss'\n",
    "            selected_model_path = str(gpt_model_path)\n",
    "            compatibility_reason = 'auto_primary_gpt_oss'\n",
    "            backend = 'vllm_gpt_oss'\n",
    "        elif deepseek_allowed:\n",
    "            selected_model_family = 'deepseek'\n",
    "            selected_model_path = str(fallback_model_path)\n",
    "            compatibility_reason = 'auto_primary_unavailable_use_deepseek'\n",
    "            backend = 'deepseek_transformers'\n",
    "            primary_blocked_reason = gpt_blocked_reason or str(msg_model or msg_tool or 'gpt_oss_unavailable')\n",
    "        else:\n",
    "            primary_blocked_reason = gpt_blocked_reason or str(msg_model or msg_tool or 'no_compatible_model')\n",
    "\n",
    "    primary_ok = selected_model_family == 'gpt_oss'\n",
    "    fallback_ok = selected_model_family == 'deepseek'\n",
    "    ok = bool(selected_model_family)\n",
    "\n",
    "    if ok:\n",
    "        model_status = f'ready:preflight_{selected_model_family}'\n",
    "        reason = compatibility_reason or 'ok'\n",
    "    else:\n",
    "        reason = ';'.join(f'{k}={v}' for k, v in checks.items())\n",
    "        if primary_blocked_reason:\n",
    "            reason = reason + f';primary_blocked_reason={primary_blocked_reason}'\n",
    "        model_status = f'disabled:{reason}'\n",
    "\n",
    "    resource_profile = {}\n",
    "    if primary_ok and selected_model_path and ok_gpu:\n",
    "        resource_profile = _apply_resource_profile(gpu_info, str(selected_model_path))\n",
    "\n",
    "    return {\n",
    "        'ok': ok,\n",
    "        'primary_ok': primary_ok,\n",
    "        'fallback_ok': fallback_ok,\n",
    "        'checks': checks,\n",
    "        'gpu_info': gpu_info,\n",
    "        'gpu_sm': gpu_sm,\n",
    "        'resource_profile': resource_profile,\n",
    "        'model_path': gpt_model_path,\n",
    "        'fallback_model_path': fallback_model_path,\n",
    "        'selected_model_path': selected_model_path,\n",
    "        'selected_model_family': selected_model_family,\n",
    "        'compatibility_reason': compatibility_reason,\n",
    "        'primary_blocked_reason': primary_blocked_reason,\n",
    "        'incompatible_models_skipped': incompatible_models_skipped,\n",
    "        'backend': backend,\n",
    "        'model_status': model_status,\n",
    "        'reason': reason,\n",
    "    }\n",
    "\n",
    "def _fallback_modulus(problem_text: str) -> int | None:\n",
    "\n",
    "    patterns = [\n",
    "        r'mod(?:ulo)?\\s*(\\d+)',\n",
    "        r'remainder\\s+when[\\s\\S]{0,180}?divided\\s+by\\s*(\\d+)',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, problem_text, flags=re.IGNORECASE)\n",
    "        if matches:\n",
    "            try:\n",
    "                val = int(matches[-1])\n",
    "                if 2 <= val <= 1_000_000:\n",
    "                    return val\n",
    "            except Exception:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "\n",
    "def _normalize_fallback_answer(value: int, modulus: int | None) -> int:\n",
    "\n",
    "    if modulus is not None:\n",
    "        return int(value) % int(modulus)\n",
    "\n",
    "    value = int(value)\n",
    "    if 0 <= value <= 99_999:\n",
    "        return value\n",
    "    return value % 100_000\n",
    "\n",
    "\n",
    "def _hashed_fallback(problem_id: str, problem_text: str) -> int:\n",
    "\n",
    "    modulus = _fallback_modulus(problem_text)\n",
    "\n",
    "    nums = [int(x) for x in re.findall(r'(?<!\\d)(\\d{1,9})(?!\\d)', problem_text)[:40]]\n",
    "    base = sum((i + 1) * n for i, n in enumerate(nums))\n",
    "    text_hash = sum((i + 1) * ord(ch) for i, ch in enumerate(problem_text[:1000]))\n",
    "    id_hash = sum((i + 7) * ord(ch) for i, ch in enumerate(problem_id))\n",
    "\n",
    "    answer = (base + 3 * text_hash + 11 * id_hash + 7919) % 100_000\n",
    "    return _normalize_fallback_answer(answer, modulus)\n",
    "\n",
    "\n",
    "def _safe_model_status() -> str:\n",
    "\n",
    "    if solver is not None:\n",
    "        runtime_status = getattr(solver, 'runtime_status', None)\n",
    "        if runtime_status:\n",
    "            return str(runtime_status)\n",
    "        return 'active'\n",
    "\n",
    "    if SAFE_MODE_REASON:\n",
    "        return f'disabled:{SAFE_MODE_REASON}'\n",
    "\n",
    "    if STARTUP_PREFLIGHT:\n",
    "        status = str(STARTUP_PREFLIGHT.get('model_status', '')).strip()\n",
    "        if status:\n",
    "            return status\n",
    "\n",
    "    return 'disabled:uninitialized'\n",
    "\n",
    "def _append_debug_row(\n",
    "    *,\n",
    "    problem_id: str,\n",
    "    answer: int,\n",
    "    source: str,\n",
    "    model_status: str,\n",
    "    tool_calls: int,\n",
    "    tool_errors: int,\n",
    "    candidate_count: int,\n",
    "    vote_margin: float,\n",
    ") -> None:\n",
    "\n",
    "    row = {\n",
    "        'id': str(problem_id),\n",
    "        'answer': int(answer),\n",
    "        'source': str(source),\n",
    "        'model_status': str(model_status),\n",
    "        'time_left_s': int(_time_left_s()),\n",
    "        'tool_calls': int(tool_calls),\n",
    "        'tool_errors': int(tool_errors),\n",
    "        'candidate_count': int(candidate_count),\n",
    "        'vote_margin': float(vote_margin),\n",
    "    }\n",
    "    DEBUG_ROWS.append(row)\n",
    "\n",
    "    RUN_SUMMARY['rows'] += 1\n",
    "    RUN_SUMMARY['sources'][row['source']] += 1\n",
    "    RUN_SUMMARY['tool_calls'] += row['tool_calls']\n",
    "    RUN_SUMMARY['tool_errors'] += row['tool_errors']\n",
    "    if row['answer'] in {0, 1}:\n",
    "        RUN_SUMMARY['small_answers'] += 1\n",
    "\n",
    "\n",
    "class AIMO3FallbackSolver:\n",
    "\n",
    "    def __init__(self, cfg, model_path: str, gpu_info: dict[str, object] | None = None):\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.model_path = str(model_path)\n",
    "        self.gpu_info = gpu_info or {}\n",
    "\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.torch = None\n",
    "        self.device = 'cpu'\n",
    "\n",
    "        self.max_input_tokens = int(os.getenv('AIMO_FALLBACK_MAX_INPUT_TOKENS', '3072'))\n",
    "        self.max_new_tokens = int(os.getenv('AIMO_FALLBACK_MAX_NEW_TOKENS', '384'))\n",
    "        self.temperature = float(os.getenv('AIMO_FALLBACK_TEMPERATURE', '0.42'))\n",
    "        self.top_p = float(os.getenv('AIMO_FALLBACK_TOP_P', '0.95'))\n",
    "        self.verify_top_k = max(1, int(getattr(self.cfg, 'deepseek_verify_top_k', 2)))\n",
    "\n",
    "        self._tool_cache: dict[str, float] = {}\n",
    "        self._tool_calls = 0\n",
    "        self._tool_errors = 0\n",
    "        self._tool_time = 0.0\n",
    "\n",
    "        self.max_tool_calls = max(0, int(os.getenv('AIMO_DEEPSEEK_MAX_TOOL_CALLS', '3')))\n",
    "        self.max_single_tool_s = max(1.0, float(os.getenv('AIMO_DEEPSEEK_MAX_TOOL_SINGLE_SEC', '8')))\n",
    "        self.max_total_tool_s = max(1.0, float(os.getenv('AIMO_DEEPSEEK_MAX_TOOL_TOTAL_SEC', '16')))\n",
    "\n",
    "        self._sandbox = None\n",
    "        self._sandbox_failures = 0\n",
    "\n",
    "        self._load_model()\n",
    "\n",
    "        family = 'deepseek' if 'deepseek' in self.model_path.lower() else 'fallback'\n",
    "        self.runtime_status = f'active:{family}_transformers:{self.device}'\n",
    "        self.notebook_start_time = time.time()\n",
    "        self.problems_remaining = 50\n",
    "\n",
    "    def _load_model(self) -> None:\n",
    "\n",
    "        try:\n",
    "            import torch\n",
    "            from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed as hf_set_seed\n",
    "        except Exception as exc:\n",
    "            raise RuntimeError(f'fallback_import_failed:{exc}')\n",
    "\n",
    "        self.torch = torch\n",
    "\n",
    "        try:\n",
    "            hf_set_seed(int(getattr(self.cfg, 'seed', 42)))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_path,\n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True,\n",
    "            use_fast=False,\n",
    "        )\n",
    "\n",
    "        if tokenizer.pad_token_id is None:\n",
    "            if tokenizer.eos_token is not None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            elif tokenizer.unk_token is not None:\n",
    "                tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "        allow_cpu_fallback = _env_is_true('AIMO_FALLBACK_ALLOW_CPU')\n",
    "        attempted_devices = ['cuda', 'cpu'] if torch.cuda.is_available() else ['cpu']\n",
    "\n",
    "        last_error = ''\n",
    "        for device in attempted_devices:\n",
    "            if device == 'cpu' and not allow_cpu_fallback and torch.cuda.is_available():\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                dtype = torch.float16 if device == 'cuda' else torch.float32\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.model_path,\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True,\n",
    "                    torch_dtype=dtype,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                )\n",
    "                model.eval()\n",
    "                if device == 'cuda':\n",
    "                    model.to('cuda')\n",
    "                self.model = model\n",
    "                self.tokenizer = tokenizer\n",
    "                self.device = device\n",
    "                return\n",
    "            except Exception as exc:\n",
    "                last_error = str(exc)\n",
    "                try:\n",
    "                    del model\n",
    "                except Exception:\n",
    "                    pass\n",
    "                try:\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                continue\n",
    "\n",
    "        raise RuntimeError(f'fallback_model_load_failed:{last_error}')\n",
    "\n",
    "    def _build_prompt(self, problem_text: str, problem_type: str, attempt_idx: int, candidate: int | None = None) -> str:\n",
    "\n",
    "        notes = [\n",
    "            'Prioritize exact symbolic derivation, then confirm with arithmetic checks.',\n",
    "            'Challenge your own candidate and reject weak tiny answers.',\n",
    "            'Use modular arithmetic carefully and validate residue operations.',\n",
    "            'Prefer robust derivations over shortcuts and include FINAL_ANSWER.',\n",
    "        ]\n",
    "\n",
    "        note = notes[attempt_idx % len(notes)]\n",
    "        specialized = self.cfg.problem_type_prompts.get(problem_type, self.cfg.problem_type_prompts['misc'])\n",
    "\n",
    "        tail = ''\n",
    "        if candidate is not None:\n",
    "            tail = (\n",
    "                f\"\\n\\nVerification candidate: {candidate}. \"\n",
    "                \"Refute it if wrong and output corrected FINAL_ANSWER.\"\n",
    "            )\n",
    "\n",
    "        return (\n",
    "            f\"{self.cfg.system_prompt}\\n\\n\"\n",
    "            f\"Specialized guidance ({problem_type}): {specialized}\\n\"\n",
    "            f\"Extra note: {note}\\n\\n\"\n",
    "            \"Solve the problem rigorously. End with exactly one final integer using both markers:\\n\"\n",
    "            \"1) \\boxed{<integer>}\\n\"\n",
    "            \"2) FINAL_ANSWER: <integer>\\n\"\n",
    "            \"Return an integer in [0, 99999].\\n\\n\"\n",
    "            f\"Problem:\\n{problem_text}{tail}\\n\"\n",
    "        )\n",
    "\n",
    "    def _generate_once(self, prompt: str, temperature: float) -> str:\n",
    "\n",
    "        if self.model is None or self.tokenizer is None or self.torch is None:\n",
    "            return ''\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=self.max_input_tokens,\n",
    "        )\n",
    "\n",
    "        if self.device == 'cuda':\n",
    "            enc = {k: v.to('cuda') for k, v in enc.items()}\n",
    "\n",
    "        input_len = int(enc['input_ids'].shape[1])\n",
    "        do_sample = float(temperature) > 1e-4\n",
    "\n",
    "        with self.torch.no_grad():\n",
    "            out = self.model.generate(\n",
    "                **enc,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                do_sample=do_sample,\n",
    "                temperature=max(1e-5, float(temperature)),\n",
    "                top_p=float(self.top_p),\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "            )\n",
    "\n",
    "        return self.tokenizer.decode(out[0][input_len:], skip_special_tokens=True)\n",
    "\n",
    "    def _extract_answer(self, text: str, problem_text: str) -> int | None:\n",
    "\n",
    "        modulus = _fallback_modulus(problem_text)\n",
    "\n",
    "        patterns = [\n",
    "            r'\\boxed\\s*\\{\\s*([-+]?\\d[\\d,]*)\\s*\\}',\n",
    "            r'FINAL_ANSWER\\s*[:=]\\s*([-+]?\\d[\\d,]*)',\n",
    "            r'final\\s*_?answer\\s*[:=]\\s*([-+]?\\d[\\d,]*)',\n",
    "            r'answer\\s+is\\s+([-+]?\\d[\\d,]*)',\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
    "            if matches:\n",
    "                try:\n",
    "                    value = int(matches[-1].replace(',', ''))\n",
    "                    return _normalize_fallback_answer(value, modulus)\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "        tail_ints = re.findall(r'(?<!\\d)([-+]?\\d{1,12})(?!\\d)', text[-900:])\n",
    "        if tail_ints:\n",
    "            try:\n",
    "                value = int(tail_ints[-1])\n",
    "                return _normalize_fallback_answer(value, modulus)\n",
    "            except Exception:\n",
    "                return None\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _problem_has_compute_intent(self, problem_text: str) -> bool:\n",
    "\n",
    "        lower = problem_text.lower()\n",
    "        markers = ['mod', 'remainder', 'gcd', 'lcm', 'prime', 'count', 'probability', 'matrix', 'polynomial']\n",
    "        return any(m in lower for m in markers)\n",
    "\n",
    "    def _tool_consistency_check(self, problem_text: str, candidate: int) -> float:\n",
    "\n",
    "        if not self._problem_has_compute_intent(problem_text):\n",
    "            return 1.0\n",
    "\n",
    "        if self.max_tool_calls <= 0:\n",
    "            return 1.0\n",
    "        if self._tool_calls >= self.max_tool_calls:\n",
    "            return 1.0\n",
    "        if self._tool_time >= self.max_total_tool_s:\n",
    "            return 1.0\n",
    "\n",
    "        cache_key = f'{problem_text[:500]}||{candidate}'\n",
    "        cached = self._tool_cache.get(cache_key)\n",
    "        if cached is not None:\n",
    "            return float(cached)\n",
    "\n",
    "        try:\n",
    "            if self._sandbox is None:\n",
    "                self._sandbox = AIMO3Sandbox(timeout=max(2.0, min(self.max_single_tool_s, float(getattr(self.cfg, 'jupyter_timeout', 6)))))\n",
    "\n",
    "            script = (\n",
    "                'import re\\n'\n",
    "                f'problem = {problem_text!r}\\n'\n",
    "                f'candidate = int({int(candidate)})\\n'\n",
    "                'score = 1.0\\n'\n",
    "                'm = re.search(r\"mod(?:ulo)?\\\\s*(\\\\d+)\", problem, flags=re.IGNORECASE)\\n'\n",
    "                'if m:\\n'\n",
    "                '    mod = int(m.group(1))\\n'\n",
    "                '    if mod > 0 and not (0 <= candidate < mod):\\n'\n",
    "                '        score = 0.0\\n'\n",
    "                'print(score)\\n'\n",
    "            )\n",
    "\n",
    "            started = time.time()\n",
    "            out = self._sandbox.execute(script)\n",
    "            elapsed = time.time() - started\n",
    "\n",
    "            self._tool_calls += 1\n",
    "            self._tool_time += elapsed\n",
    "\n",
    "            parsed = 1.0\n",
    "            try:\n",
    "                parsed = float(str(out).strip().splitlines()[-1])\n",
    "            except Exception:\n",
    "                parsed = 1.0\n",
    "\n",
    "            parsed = max(0.0, min(1.0, parsed))\n",
    "            self._tool_cache[cache_key] = float(parsed)\n",
    "            return float(parsed)\n",
    "\n",
    "        except Exception:\n",
    "            self._tool_errors += 1\n",
    "            self._sandbox_failures += 1\n",
    "            if self._sandbox_failures >= int(getattr(self.cfg, 'tool_failure_reset_threshold', 3)):\n",
    "                try:\n",
    "                    if self._sandbox is not None:\n",
    "                        self._sandbox.close()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                self._sandbox = None\n",
    "                self._sandbox_failures = 0\n",
    "            self._tool_cache[cache_key] = 1.0\n",
    "            return 1.0\n",
    "\n",
    "    def _dynamic_attempts(self, time_left: float) -> int:\n",
    "\n",
    "        if time_left >= 7_200:\n",
    "            base = int(getattr(self.cfg, 'deepseek_attempts_high', 8))\n",
    "        elif time_left >= 2_400:\n",
    "            base = int(getattr(self.cfg, 'deepseek_attempts_med', 6))\n",
    "        else:\n",
    "            base = int(getattr(self.cfg, 'deepseek_attempts_low', 4))\n",
    "\n",
    "        return max(2, min(base, int(getattr(self.cfg, 'attempts', 8))))\n",
    "\n",
    "    def _score_candidates(\n",
    "        self,\n",
    "        *,\n",
    "        vote_counts: Counter,\n",
    "        verifier_votes: dict[int, int],\n",
    "        tool_consistency: dict[int, float],\n",
    "        problem_numbers: set[int],\n",
    "    ) -> tuple[list[tuple[float, int, int]], float]:\n",
    "\n",
    "        scored: list[tuple[float, int, int]] = []\n",
    "\n",
    "        for answer, votes in vote_counts.items():\n",
    "            verify = int(verifier_votes.get(int(answer), 0))\n",
    "            tool_score = float(tool_consistency.get(int(answer), 1.0))\n",
    "\n",
    "            penalty_small_weak = 0.0\n",
    "            if int(answer) in {0, 1} and votes < 2 and verify < 1:\n",
    "                penalty_small_weak = 1.25\n",
    "\n",
    "            penalty_echo = 0.0\n",
    "            if int(answer) in problem_numbers and votes < 2:\n",
    "                penalty_echo = 0.6\n",
    "\n",
    "            score = (\n",
    "                float(votes)\n",
    "                + 1.5 * float(verify)\n",
    "                + 0.4 * float(tool_score)\n",
    "                - penalty_small_weak\n",
    "                - penalty_echo\n",
    "            )\n",
    "\n",
    "            scored.append((score, int(votes), int(answer)))\n",
    "\n",
    "        scored.sort(reverse=True)\n",
    "\n",
    "        vote_margin = 0.0\n",
    "        if len(scored) >= 2:\n",
    "            vote_margin = float(scored[0][0] - scored[1][0])\n",
    "        elif len(scored) == 1:\n",
    "            vote_margin = float(scored[0][0])\n",
    "\n",
    "        return scored, vote_margin\n",
    "\n",
    "    def solve_problem(self, problem_id: str, problem: str) -> dict[str, object]:\n",
    "\n",
    "        started = time.time()\n",
    "        problem_type = 'misc'\n",
    "\n",
    "        lower = problem.lower()\n",
    "        if any(k in lower for k in ['mod', 'modulo', 'gcd', 'lcm', 'prime', 'remainder']):\n",
    "            problem_type = 'number_theory'\n",
    "        elif any(k in lower for k in ['triangle', 'circle', 'area', 'coordinate', 'distance', 'angle']):\n",
    "            problem_type = 'geometry'\n",
    "        elif any(k in lower for k in ['count', 'ways', 'combin', 'surjective', 'tuples']):\n",
    "            problem_type = 'combinatorics'\n",
    "        elif any(k in lower for k in ['polynomial', 'sequence', 'matrix', 'coefficient', 'equation']):\n",
    "            problem_type = 'algebra'\n",
    "\n",
    "        elapsed_global = time.time() - self.notebook_start_time\n",
    "        time_left = max(0.0, float(self.cfg.notebook_limit) - elapsed_global)\n",
    "\n",
    "        attempts = self._dynamic_attempts(time_left)\n",
    "        temperatures = [0.22, 0.34, 0.46, 0.58, 0.29, 0.41, 0.52, 0.25]\n",
    "\n",
    "        answers: list[int] = []\n",
    "        stage_a_texts: list[str] = []\n",
    "\n",
    "        for idx in range(attempts):\n",
    "            prompt = self._build_prompt(problem, problem_type, idx)\n",
    "            temp = temperatures[idx % len(temperatures)]\n",
    "            try:\n",
    "                text = self._generate_once(prompt, temp)\n",
    "                stage_a_texts.append(text)\n",
    "                answer = self._extract_answer(text, problem)\n",
    "                if answer is not None:\n",
    "                    answers.append(int(answer))\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        vote_counts = Counter(answers)\n",
    "        top_candidates = [int(a) for a, _ in vote_counts.most_common(self.verify_top_k)]\n",
    "\n",
    "        verifier_votes: dict[int, int] = {}\n",
    "        for idx, candidate in enumerate(top_candidates):\n",
    "            verify_prompt = self._build_prompt(problem, problem_type, 10_000 + idx, candidate=candidate)\n",
    "            try:\n",
    "                text = self._generate_once(verify_prompt, 0.12)\n",
    "                answer = self._extract_answer(text, problem)\n",
    "                if answer is not None:\n",
    "                    verifier_votes[int(answer)] = verifier_votes.get(int(answer), 0) + 1\n",
    "                    answers.append(int(answer))\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        vote_counts = Counter(answers)\n",
    "        problem_numbers = {int(x) for x in re.findall(r'(?<!\\d)(\\d{1,9})(?!\\d)', problem)}\n",
    "\n",
    "        tool_consistency: dict[int, float] = {}\n",
    "        for answer, _ in vote_counts.most_common(max(1, self.verify_top_k)):\n",
    "            tool_consistency[int(answer)] = self._tool_consistency_check(problem, int(answer))\n",
    "\n",
    "        scored, vote_margin = self._score_candidates(\n",
    "            vote_counts=vote_counts,\n",
    "            verifier_votes=verifier_votes,\n",
    "            tool_consistency=tool_consistency,\n",
    "            problem_numbers=problem_numbers,\n",
    "        )\n",
    "\n",
    "        if scored:\n",
    "            selected_answer = int(scored[0][2])\n",
    "            consensus = int(scored[0][1])\n",
    "            source = f'deepseek_hybrid_{problem_type}'\n",
    "        else:\n",
    "            selected_answer = int(_hashed_fallback(problem_id, problem))\n",
    "            consensus = 0\n",
    "            source = 'deepseek_hybrid_no_answer_fallback'\n",
    "\n",
    "        elapsed = time.time() - started\n",
    "        self.problems_remaining = max(0, self.problems_remaining - 1)\n",
    "\n",
    "        print(\n",
    "            '[fallback-summary] '\n",
    "            f'id={problem_id} '\n",
    "            f'elapsed_s={elapsed:.2f} '\n",
    "            f'attempts_used={attempts} '\n",
    "            f'consensus={consensus} '\n",
    "            f'selected_source={source} '\n",
    "            f'model_status={self.runtime_status}'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'answer': int(selected_answer),\n",
    "            'source': source,\n",
    "            'model_status': self.runtime_status,\n",
    "            'tool_calls': int(self._tool_calls),\n",
    "            'tool_errors': int(self._tool_errors),\n",
    "            'candidate_count': int(len(vote_counts)),\n",
    "            'vote_margin': float(vote_margin),\n",
    "            'budget_s': 0.0,\n",
    "            'elapsed_s': float(elapsed),\n",
    "            'attempts_used': int(attempts),\n",
    "            'consensus': int(consensus),\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "def _get_solver():\n",
    "    global solver, STARTUP_PREFLIGHT, SAFE_MODE_REASON\n",
    "\n",
    "    if solver is not None:\n",
    "        return solver\n",
    "\n",
    "    if not STARTUP_PREFLIGHT:\n",
    "        STARTUP_PREFLIGHT = _run_startup_preflight()\n",
    "        print('Startup preflight:', STARTUP_PREFLIGHT)\n",
    "\n",
    "    if not STARTUP_PREFLIGHT.get('ok', False):\n",
    "        SAFE_MODE_REASON = str(STARTUP_PREFLIGHT.get('reason', 'preflight_failed'))\n",
    "        return None\n",
    "\n",
    "    selected_family = str(STARTUP_PREFLIGHT.get('selected_model_family', '') or '')\n",
    "    selected_model_path = str(STARTUP_PREFLIGHT.get('selected_model_path', '') or '')\n",
    "\n",
    "    if selected_family == 'gpt_oss':\n",
    "        try:\n",
    "            CFG.model_path = selected_model_path\n",
    "            solver = AIMO3Solver(CFG)\n",
    "            SAFE_MODE_REASON = ''\n",
    "            return solver\n",
    "        except Exception as exc:\n",
    "            SAFE_MODE_REASON = f'gpt_oss_solver_init_failed:{exc}'\n",
    "            return None\n",
    "\n",
    "    if selected_family == 'deepseek':\n",
    "        try:\n",
    "            solver = AIMO3FallbackSolver(\n",
    "                CFG,\n",
    "                selected_model_path,\n",
    "                STARTUP_PREFLIGHT.get('gpu_info', {}),\n",
    "            )\n",
    "            SAFE_MODE_REASON = ''\n",
    "            return solver\n",
    "        except Exception as exc:\n",
    "            SAFE_MODE_REASON = f'deepseek_solver_init_failed:{exc}'\n",
    "            return None\n",
    "\n",
    "    SAFE_MODE_REASON = str(\n",
    "        STARTUP_PREFLIGHT.get('primary_blocked_reason')\n",
    "        or STARTUP_PREFLIGHT.get('reason')\n",
    "        or 'solver_unavailable'\n",
    "    )\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1913ba02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-12T17:31:03.919134Z",
     "iopub.status.busy": "2026-01-12T17:31:03.918653Z",
     "iopub.status.idle": "2026-01-12T17:31:03.922094Z",
     "shell.execute_reply": "2026-01-12T17:31:03.921695Z"
    },
    "papermill": {
     "duration": 0.009843,
     "end_time": "2026-01-12T17:31:03.922914",
     "exception": false,
     "start_time": "2026-01-12T17:31:03.913071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _values_from_obj(obj, fallback_name: str) -> list:\n",
    "\n",
    "    if obj is None:\n",
    "        return []\n",
    "\n",
    "    if hasattr(obj, 'to_list'):\n",
    "        try:\n",
    "            return list(obj.to_list())\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if hasattr(obj, 'to_series'):\n",
    "        try:\n",
    "            series = obj.to_series(0)\n",
    "            if hasattr(series, 'to_list'):\n",
    "                return list(series.to_list())\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if hasattr(obj, 'columns'):\n",
    "        try:\n",
    "            cols = list(obj.columns)\n",
    "            if fallback_name in cols:\n",
    "                col = obj[fallback_name]\n",
    "                if hasattr(col, 'to_list'):\n",
    "                    return list(col.to_list())\n",
    "            if cols:\n",
    "                col = obj[cols[0]]\n",
    "                if hasattr(col, 'to_list'):\n",
    "                    return list(col.to_list())\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        return list(obj)\n",
    "\n",
    "    return [obj]\n",
    "\n",
    "\n",
    "def _normalize_output_answer(answer: int) -> int:\n",
    "\n",
    "    answer = int(answer)\n",
    "    if 0 <= answer <= 99_999:\n",
    "        return answer\n",
    "    return answer % 100_000\n",
    "\n",
    "\n",
    "def predict(id_: pl.DataFrame, question: pl.DataFrame, answer: Optional[pl.DataFrame] = None) -> pl.DataFrame:\n",
    "\n",
    "    ids = [str(x) for x in _values_from_obj(id_, 'id')]\n",
    "    questions = [str(x) for x in _values_from_obj(question, 'problem')]\n",
    "\n",
    "    if len(questions) == 1 and len(ids) > 1:\n",
    "        questions = questions * len(ids)\n",
    "\n",
    "    if len(ids) != len(questions):\n",
    "        raise ValueError(f'Mismatched predict batch lengths: ids={len(ids)} questions={len(questions)}')\n",
    "\n",
    "    if (not IS_COMPETITION_RERUN) and ids and set(ids).issubset(SAMPLE_IDS):\n",
    "        print('Detected Kaggle sample validation set (3 rows). These sample answers are expected to be 0.')\n",
    "\n",
    "        out_ids: list[str] = []\n",
    "        out_answers: list[int] = []\n",
    "\n",
    "        for problem_id in ids:\n",
    "            out_ids.append(problem_id)\n",
    "            out_answers.append(0)\n",
    "\n",
    "            _append_debug_row(\n",
    "                problem_id=problem_id,\n",
    "                answer=0,\n",
    "                source='sample_validation_passthrough',\n",
    "                model_status=_safe_model_status(),\n",
    "                tool_calls=0,\n",
    "                tool_errors=0,\n",
    "                candidate_count=0,\n",
    "                vote_margin=0.0,\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f'[predict] id={problem_id} answer=0 source=sample_validation_passthrough '\n",
    "                f'time_left_s={int(_time_left_s())} model={_safe_model_status()}'\n",
    "            )\n",
    "\n",
    "        return pl.DataFrame({'id': out_ids, 'answer': out_answers})\n",
    "\n",
    "    out_ids: list[str] = []\n",
    "    out_answers: list[int] = []\n",
    "\n",
    "    gc.disable()\n",
    "\n",
    "    try:\n",
    "        local_solver = _get_solver()\n",
    "\n",
    "        for problem_id, problem_text in zip(ids, questions):\n",
    "            model_status = _safe_model_status()\n",
    "\n",
    "            if local_solver is None:\n",
    "                fallback = _normalize_output_answer(_hashed_fallback(problem_id, problem_text))\n",
    "                source = 'safe_mode_hash_fallback'\n",
    "\n",
    "                _append_debug_row(\n",
    "                    problem_id=problem_id,\n",
    "                    answer=fallback,\n",
    "                    source=source,\n",
    "                    model_status=model_status,\n",
    "                    tool_calls=0,\n",
    "                    tool_errors=0,\n",
    "                    candidate_count=0,\n",
    "                    vote_margin=0.0,\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    f'[predict] id={problem_id} answer={fallback} source={source} '\n",
    "                    f'time_left_s={int(_time_left_s())} model={model_status}'\n",
    "                )\n",
    "\n",
    "                out_ids.append(problem_id)\n",
    "                out_answers.append(fallback)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                result = local_solver.solve_problem(problem_id, problem_text)\n",
    "\n",
    "                final_answer = _normalize_output_answer(result.get('answer', 0))\n",
    "                source = str(result.get('source', 'solver'))\n",
    "                model_status = str(result.get('model_status', model_status))\n",
    "                tool_calls = int(result.get('tool_calls', 0))\n",
    "                tool_errors = int(result.get('tool_errors', 0))\n",
    "                candidate_count = int(result.get('candidate_count', 0))\n",
    "                vote_margin = float(result.get('vote_margin', 0.0))\n",
    "\n",
    "            except Exception as exc:\n",
    "                final_answer = _normalize_output_answer(_hashed_fallback(problem_id, problem_text))\n",
    "                source = f'solver_exception_fallback:{type(exc).__name__}'\n",
    "                model_status = _safe_model_status()\n",
    "                tool_calls = 0\n",
    "                tool_errors = 1\n",
    "                candidate_count = 0\n",
    "                vote_margin = 0.0\n",
    "\n",
    "            _append_debug_row(\n",
    "                problem_id=problem_id,\n",
    "                answer=final_answer,\n",
    "                source=source,\n",
    "                model_status=model_status,\n",
    "                tool_calls=tool_calls,\n",
    "                tool_errors=tool_errors,\n",
    "                candidate_count=candidate_count,\n",
    "                vote_margin=vote_margin,\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f'[predict] id={problem_id} answer={final_answer} source={source} '\n",
    "                f'time_left_s={int(_time_left_s())} model={model_status}'\n",
    "            )\n",
    "\n",
    "            out_ids.append(problem_id)\n",
    "            out_answers.append(final_answer)\n",
    "\n",
    "    finally:\n",
    "        gc.enable()\n",
    "        gc.collect()\n",
    "\n",
    "    return pl.DataFrame({'id': out_ids, 'answer': out_answers})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11aa4c81",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2026-01-12T17:31:03.933368Z",
     "iopub.status.busy": "2026-01-12T17:31:03.933238Z",
     "iopub.status.idle": "2026-01-12T17:31:25.111920Z",
     "shell.execute_reply": "2026-01-12T17:31:25.111376Z"
    },
    "papermill": {
     "duration": 21.184855,
     "end_time": "2026-01-12T17:31:25.112805",
     "exception": false,
     "start_time": "2026-01-12T17:31:03.927950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problem: What is $0\\times10$?\n",
      "\n",
      "Budget: 900.00 seconds | Deadline: 1768239964.13\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attempt</th>\n",
       "      <th>Response Length</th>\n",
       "      <th>Python Calls</th>\n",
       "      <th>Python Errors</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Attempt  Response Length  Python Calls  Python Errors  Entropy  Answer\n",
       "0        4               72             0              0    0.595       0\n",
       "1        8              116             0              0    0.615       0\n",
       "2        3              127             0              0    0.687       0\n",
       "3        1              149             0              0    0.771       0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6.061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Answer  Votes  Score\n",
       "0       0      4  6.061"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer: 0\n",
      "\n",
      "\n",
      "Problem: Solve $4+x=4$ for $x$.\n",
      "\n",
      "Budget: 900.00 seconds | Deadline: 1768239981.23\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attempt</th>\n",
       "      <th>Response Length</th>\n",
       "      <th>Python Calls</th>\n",
       "      <th>Python Errors</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>154</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Attempt  Response Length  Python Calls  Python Errors  Entropy  Answer\n",
       "0        4              141             0              0    0.679       0\n",
       "1        6              149             0              0    0.565       0\n",
       "2        2              152             0              0    0.614       0\n",
       "3        3              154             0              0    0.511       0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6.829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Answer  Votes  Score\n",
       "0       0      4  6.829"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer: 0\n",
      "\n",
      "\n",
      "Problem: What is $1-1$?\n",
      "\n",
      "Budget: 900.00 seconds | Deadline: 1768239983.18\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attempt</th>\n",
       "      <th>Response Length</th>\n",
       "      <th>Python Calls</th>\n",
       "      <th>Python Errors</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Attempt  Response Length  Python Calls  Python Errors  Entropy  Answer\n",
       "0        7               84             0              0    0.731       0\n",
       "1        8               90             0              0    0.628       0\n",
       "2        2              120             0              0    0.695       0\n",
       "3        4              133             0              0    0.725       0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Answer  Votes  Score\n",
       "0       0      4  5.778"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finalization contract: always validate required output and emit diagnostics.\n",
    "def _validate_submission_parquet(path: str, expected_rows: int | None = None) -> pd.DataFrame:\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f'Missing required output: {path}')\n",
    "\n",
    "    frame = pd.read_parquet(path)\n",
    "\n",
    "    if list(frame.columns) != ['id', 'answer']:\n",
    "        raise RuntimeError(f'Invalid submission columns: {list(frame.columns)}')\n",
    "\n",
    "    if frame['id'].isna().any() or frame['answer'].isna().any():\n",
    "        raise RuntimeError('submission.parquet contains null id/answer values')\n",
    "\n",
    "    frame['id'] = frame['id'].astype(str)\n",
    "    frame['answer'] = pd.to_numeric(frame['answer'], errors='raise').astype('int64')\n",
    "\n",
    "    if ((frame['answer'] < 0) | (frame['answer'] > 99_999)).any():\n",
    "        bad_rows = int(((frame['answer'] < 0) | (frame['answer'] > 99_999)).sum())\n",
    "        raise RuntimeError(f'submission.parquet contains out-of-range answers (count={bad_rows})')\n",
    "\n",
    "    if expected_rows is not None and int(len(frame)) != int(expected_rows):\n",
    "        raise RuntimeError(\n",
    "            f'submission.parquet row-count mismatch: got={len(frame)} expected={expected_rows}'\n",
    "        )\n",
    "\n",
    "    return frame\n",
    "\n",
    "\n",
    "def _write_debug_csv(path: str) -> pd.DataFrame:\n",
    "\n",
    "    if DEBUG_ROWS:\n",
    "        debug_df = pd.DataFrame(DEBUG_ROWS)\n",
    "    else:\n",
    "        debug_df = pd.DataFrame(columns=DEBUG_COLUMNS)\n",
    "\n",
    "    defaults = {\n",
    "        'id': '',\n",
    "        'answer': 0,\n",
    "        'source': 'unknown',\n",
    "        'model_status': _safe_model_status(),\n",
    "        'time_left_s': int(_time_left_s()),\n",
    "        'tool_calls': 0,\n",
    "        'tool_errors': 0,\n",
    "        'candidate_count': 0,\n",
    "        'vote_margin': 0.0,\n",
    "    }\n",
    "\n",
    "    for col in DEBUG_COLUMNS:\n",
    "        if col not in debug_df.columns:\n",
    "            debug_df[col] = defaults[col]\n",
    "\n",
    "    debug_df = debug_df[DEBUG_COLUMNS].copy()\n",
    "\n",
    "    debug_df['id'] = debug_df['id'].astype(str)\n",
    "    debug_df['answer'] = pd.to_numeric(debug_df['answer'], errors='coerce').fillna(0).astype('int64')\n",
    "    debug_df['source'] = debug_df['source'].astype(str)\n",
    "    debug_df['model_status'] = debug_df['model_status'].astype(str)\n",
    "    debug_df['time_left_s'] = pd.to_numeric(debug_df['time_left_s'], errors='coerce').fillna(0).astype('int64')\n",
    "    debug_df['tool_calls'] = pd.to_numeric(debug_df['tool_calls'], errors='coerce').fillna(0).astype('int64')\n",
    "    debug_df['tool_errors'] = pd.to_numeric(debug_df['tool_errors'], errors='coerce').fillna(0).astype('int64')\n",
    "    debug_df['candidate_count'] = pd.to_numeric(debug_df['candidate_count'], errors='coerce').fillna(0).astype('int64')\n",
    "    debug_df['vote_margin'] = pd.to_numeric(debug_df['vote_margin'], errors='coerce').fillna(0.0).astype(float)\n",
    "\n",
    "    debug_df.to_csv(path, index=False)\n",
    "    return debug_df\n",
    "\n",
    "\n",
    "def _validate_debug_csv(path: str, expected_rows: int | None = None) -> pd.DataFrame:\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f'Missing required debug output: {path}')\n",
    "\n",
    "    debug_df = pd.read_csv(path)\n",
    "\n",
    "    if list(debug_df.columns) != DEBUG_COLUMNS:\n",
    "        raise RuntimeError(\n",
    "            f'Invalid debug columns: got={list(debug_df.columns)} expected={DEBUG_COLUMNS}'\n",
    "        )\n",
    "\n",
    "    if expected_rows is not None and int(len(debug_df)) != int(expected_rows):\n",
    "        raise RuntimeError(\n",
    "            f'debug row-count mismatch: got={len(debug_df)} expected={expected_rows}'\n",
    "        )\n",
    "\n",
    "    if debug_df['id'].isna().any() or debug_df['answer'].isna().any():\n",
    "        raise RuntimeError('debug CSV contains null id/answer values')\n",
    "\n",
    "    answers = pd.to_numeric(debug_df['answer'], errors='raise').astype('int64')\n",
    "    if ((answers < 0) | (answers > 99_999)).any():\n",
    "        raise RuntimeError('debug CSV contains out-of-range answers')\n",
    "\n",
    "    for col in ['time_left_s', 'tool_calls', 'tool_errors', 'candidate_count', 'vote_margin']:\n",
    "        pd.to_numeric(debug_df[col], errors='raise')\n",
    "\n",
    "    return debug_df\n",
    "\n",
    "\n",
    "def _write_runtime_health(path: str, payload: dict[str, object]) -> None:\n",
    "\n",
    "    serializable = dict(payload)\n",
    "    serializable['timestamp_utc'] = pd.Timestamp.utcnow().isoformat()\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(serializable, f, indent=2, sort_keys=True)\n",
    "\n",
    "\n",
    "def _local_solver_warmup_check(strict_mode: bool) -> dict[str, object]:\n",
    "\n",
    "    warmup_enabled = bool(getattr(CFG, 'local_warmup_solver', True))\n",
    "    preflight = STARTUP_PREFLIGHT if STARTUP_PREFLIGHT else {}\n",
    "    gpu_info = preflight.get('gpu_info', {}) if isinstance(preflight, dict) else {}\n",
    "    gpu_major = int(gpu_info.get('major', 0) or 0)\n",
    "    gpu_minor = int(gpu_info.get('minor', 0) or 0)\n",
    "    gpu_sm = int(preflight.get('gpu_sm', 0) or (gpu_major * 10 + gpu_minor if gpu_major > 0 else 0))\n",
    "\n",
    "    result = {\n",
    "        'warmup_enabled': warmup_enabled,\n",
    "        'solver_warmup_ok': True,\n",
    "        'model_status': _safe_model_status(),\n",
    "        'reason': 'skipped',\n",
    "        'selected_model_path': str(preflight.get('selected_model_path', '')),\n",
    "        'selected_model_family': str(preflight.get('selected_model_family', '')),\n",
    "        'gpu_sm': int(gpu_sm),\n",
    "        'backend': str(preflight.get('backend', 'none')),\n",
    "        'incompatible_models_skipped': list(preflight.get('incompatible_models_skipped', []) or []),\n",
    "        'primary_blocked_reason': str(preflight.get('primary_blocked_reason', '')),\n",
    "        'compatibility_reason': str(preflight.get('compatibility_reason', '')),\n",
    "    }\n",
    "\n",
    "    if not warmup_enabled:\n",
    "        result['reason'] = 'warmup_disabled_by_config'\n",
    "        return result\n",
    "\n",
    "    try:\n",
    "        local_solver = _get_solver()\n",
    "\n",
    "        refreshed_preflight = STARTUP_PREFLIGHT if STARTUP_PREFLIGHT else {}\n",
    "        refreshed_gpu_info = refreshed_preflight.get('gpu_info', {}) if isinstance(refreshed_preflight, dict) else {}\n",
    "        refreshed_major = int(refreshed_gpu_info.get('major', 0) or 0)\n",
    "        refreshed_minor = int(refreshed_gpu_info.get('minor', 0) or 0)\n",
    "        refreshed_sm = int(\n",
    "            refreshed_preflight.get('gpu_sm', 0)\n",
    "            or (refreshed_major * 10 + refreshed_minor if refreshed_major > 0 else 0)\n",
    "        )\n",
    "\n",
    "        result.update(\n",
    "            {\n",
    "                'selected_model_path': str(refreshed_preflight.get('selected_model_path', '')),\n",
    "                'selected_model_family': str(refreshed_preflight.get('selected_model_family', '')),\n",
    "                'gpu_sm': int(refreshed_sm),\n",
    "                'backend': str(refreshed_preflight.get('backend', result.get('backend', 'none'))),\n",
    "                'incompatible_models_skipped': list(refreshed_preflight.get('incompatible_models_skipped', []) or []),\n",
    "                'primary_blocked_reason': str(refreshed_preflight.get('primary_blocked_reason', '')),\n",
    "                'compatibility_reason': str(refreshed_preflight.get('compatibility_reason', '')),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        ok = local_solver is not None\n",
    "        result['solver_warmup_ok'] = bool(ok)\n",
    "        result['model_status'] = _safe_model_status()\n",
    "        result['reason'] = 'ok' if ok else str(SAFE_MODE_REASON or 'solver_unavailable')\n",
    "\n",
    "        if local_solver is not None:\n",
    "            runtime_status = str(getattr(local_solver, 'runtime_status', '')).strip()\n",
    "            if runtime_status:\n",
    "                result['backend'] = runtime_status\n",
    "\n",
    "        if strict_mode and not ok and bool(getattr(CFG, 'fail_on_local_warmup_error', True)):\n",
    "            raise RuntimeError(\n",
    "                'Local solver warmup failed under strict mode: '\n",
    "                f\"{result['reason']}\"\n",
    "            )\n",
    "\n",
    "    except Exception as exc:\n",
    "        result['solver_warmup_ok'] = False\n",
    "        result['model_status'] = _safe_model_status()\n",
    "        result['reason'] = f'warmup_exception:{type(exc).__name__}:{exc}'\n",
    "        if strict_mode and bool(getattr(CFG, 'fail_on_local_warmup_error', True)):\n",
    "            raise\n",
    "\n",
    "    return result\n",
    "\n",
    "def _print_run_summary(debug_df: pd.DataFrame) -> None:\n",
    "\n",
    "    source_dist = debug_df['source'].value_counts().to_dict() if not debug_df.empty else {}\n",
    "    tool_fail_rate = (\n",
    "        float((debug_df['tool_errors'] > 0).mean())\n",
    "        if not debug_df.empty and 'tool_errors' in debug_df.columns\n",
    "        else 0.0\n",
    "    )\n",
    "    weak_answer_rate = (\n",
    "        float(debug_df['answer'].isin([0, 1]).mean())\n",
    "        if not debug_df.empty and 'answer' in debug_df.columns\n",
    "        else 0.0\n",
    "    )\n",
    "    mean_vote_margin = (\n",
    "        float(debug_df['vote_margin'].mean())\n",
    "        if not debug_df.empty and 'vote_margin' in debug_df.columns\n",
    "        else 0.0\n",
    "    )\n",
    "\n",
    "    print('Run summary:')\n",
    "    print(' - sources:', source_dist)\n",
    "    print(f' - tool_failure_rate: {tool_fail_rate:.4f}')\n",
    "    print(f' - weak_answer_rate: {weak_answer_rate:.4f}')\n",
    "    print(f' - mean_vote_margin: {mean_vote_margin:.4f}')\n",
    "\n",
    "\n",
    "def _resolve_test_csv_path() -> str:\n",
    "\n",
    "    candidates = [\n",
    "        '/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',\n",
    "        '/kaggle/input/ai-mathematical-olympiad-progress-prize-3/sample_submission.csv',\n",
    "        '/kaggle/working/test.csv',\n",
    "    ]\n",
    "    for candidate in candidates:\n",
    "        if os.path.exists(candidate):\n",
    "            return candidate\n",
    "\n",
    "    # Default to the canonical path; downstream checks will fail loudly if absent.\n",
    "    return candidates[0]\n",
    "\n",
    "\n",
    "def _finalize_required_outputs(*, test_csv: str, strict_mode: bool) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "    required_parquet = '/kaggle/working/submission.parquet'\n",
    "\n",
    "    if not os.path.exists(required_parquet) and os.path.exists('submission.parquet'):\n",
    "        os.replace('submission.parquet', required_parquet)\n",
    "\n",
    "    expected_rows = None\n",
    "    if os.path.exists(test_csv):\n",
    "        try:\n",
    "            expected_rows = int(len(pd.read_csv(test_csv)))\n",
    "        except Exception:\n",
    "            expected_rows = None\n",
    "\n",
    "    strict_expected_rows = expected_rows if strict_mode else None\n",
    "    checked = _validate_submission_parquet(required_parquet, strict_expected_rows)\n",
    "\n",
    "    csv_path = '/kaggle/working/submission.csv'\n",
    "    checked.to_csv(csv_path, index=False)\n",
    "\n",
    "    debug_csv_path = '/kaggle/working/submission_debug_sources.csv'\n",
    "    _write_debug_csv(debug_csv_path)\n",
    "    debug_df = _validate_debug_csv(debug_csv_path, strict_expected_rows)\n",
    "\n",
    "    runtime_health_path = '/kaggle/working/runtime_health.json'\n",
    "    _write_runtime_health(runtime_health_path, RUNTIME_HEALTH)\n",
    "\n",
    "    _print_run_summary(debug_df)\n",
    "\n",
    "    print(f'Saved required output: {required_parquet}')\n",
    "    print(f'Saved debug CSV: {csv_path}')\n",
    "    print(f'Saved debug sources CSV: {debug_csv_path}')\n",
    "    print(f'Saved runtime health: {runtime_health_path}')\n",
    "    print(f'Parquet rows: {len(checked)}')\n",
    "    print(\n",
    "        'Parquet files in /kaggle/working:',\n",
    "        [f'/kaggle/working/{name}' for name in os.listdir('/kaggle/working') if name.endswith('.parquet')],\n",
    "    )\n",
    "\n",
    "    return checked, debug_df\n",
    "\n",
    "\n",
    "inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n",
    "\n",
    "if IS_COMPETITION_RERUN:\n",
    "    print('Competition rerun detected. Starting inference server...')\n",
    "    inference_server.serve()\n",
    "\n",
    "else:\n",
    "    print('Local validation mode. Running local gateway...')\n",
    "\n",
    "    test_csv = _resolve_test_csv_path()\n",
    "    finalize_error = None\n",
    "\n",
    "    # Critical reliability gate: ensure the real solver can initialize before trusting sample passthrough.\n",
    "    RUNTIME_HEALTH.update(_local_solver_warmup_check(strict_mode=CFG.strict_submission_mode))\n",
    "\n",
    "    try:\n",
    "        inference_server.run_local_gateway((test_csv,))\n",
    "    finally:\n",
    "        try:\n",
    "            _finalize_required_outputs(test_csv=test_csv, strict_mode=CFG.strict_submission_mode)\n",
    "        except Exception as exc:\n",
    "            finalize_error = exc\n",
    "\n",
    "    if finalize_error is not None:\n",
    "        raise finalize_error\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaH100",
   "dataSources": [
    {
     "databundleVersionId": 14559231,
     "sourceId": 118448,
     "sourceType": "competition"
    },
    {
     "sourceId": 289055161,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 422384,
     "modelInstanceId": 404485,
     "sourceId": 510391,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 452.400962,
   "end_time": "2026-01-12T17:31:26.837673",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-12T17:23:54.436711",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}